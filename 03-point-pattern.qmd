# Point Pattern Analysis
This week, we will be focusing on Point Pattern Analysis, which aims to detect clusters or patterns within a set of points. Through this analysis, we can measure density, dispersion, and homogeneity in point structures. Various methods exist for calculating and identifying these clusters, and today we will explore several of these techniques using our [bike theft dataset from last week](02-operations.html#crime-data).

## Lecture slides
The slides for this week's lecture can be downloaded here: [[Link]]({{< var slides.week03 >}})

## Reading list
#### Essential readings {.unnumbered}
- Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. *Journal of Urban Economics* 125: 103217. [[Link]](https://doi.org/10.1016/j.jue.2019.103217)
- Cheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. *International Journal of Geographical Information Science* 26(2), pp.309-325. [[Link]](https://doi.org/10.1080/13658816.2011.591291)
- Longley, P. *et al.* 2015. Geographic Information Science & systems, **Chapter 12**: *Geovisualization*. [[Link]](https://ucl.rl.talis.com/link?url=https%3A%2F%2Fapp.knovel.com%2Fhotlink%2Ftoc%2Fid%3AkpGISSE001%2Fgeographic-information-science%3Fkpromoter%3Dmarc&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a)

#### Suggested readings {.unnumbered}
- Van Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. *Journal of Maps* 16, pp.58-76. [[Link]](https://doi.org/10.1080/17445647.2020.1746418)
- Yin, P. 2020. *Kernels and density estimation*. The Geographic Information Science & Technology Body of Knowledge. [[Link]](https://doi.org/10.22224/gistbok/2020.1.12)

## Bike theft in London II
This week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the [bicycle theft dataset](02-operations.html#crime-data) that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.

| File                                        | Type   | Link |
| :------                                     | :------| :------ |
| London Bicycle Theft 2023                   | [Download](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/spatial/London-BicycleTheft-2023.gpkg) |
| London MSOA 2021 Spatial Boundaries         | `GeoPackage` | [Download](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/spatial/London-MSOA-2021.gpkg) |

To get started, let us create our first script. **File** -> **New File** -> **R Script**. Save your script as `w03-bike-theft.r`. 

We will start by loading the libraries that we will need:

```{r}
#| label: 03-load-libraries
#| classes: styled-output
#| echo: True
#| eval: True
#| output: False
#| tidy: True
#| filename: 'R code'
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(spatstat)
library(dbscan)
```

::: {.callout-warning}
You may have to install some of these libraries if you have not used these before.
:::

As always, we will start by loading loading our files into memory:
```{r}
#| label: 01-load-gpkg-csv
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: 'R code'
# load msoa dataset
msoa21 <- st_read('data/spatial/London-MSOA-2021.gpkg')

# load bicycle theft dataset
theft_bike <- st_read('data/spatial/London-BicycleTheft-2023.gpkg')

# inspect
head(msoa21)

# inspect
head(theft_bike)
``` 

::: {.callout-note}
You can further inspect both objects using the `View()` function. 
:::

### Point in polygon
Point data are particularly useful because they are scale-free, meaning they can be aggregated to any geographic level that suits the analysis. While we will work directly with point location data further on in this tutorial, we will begin by creating a simple map that shows the distribution of bicycle thefts by counting the number of incidents within each MSOA.

```{r}
#| label: 03-intersect-point-in-poly
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| message: False
#| filename: "R code"
# point in polygon
msoa21 <- msoa21 |>
  mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike)))
```

::: {.callout-tip}
To create a point-in-polygon count within `sf`, we use the `st_intersects()` function and keep its default `sparse = TRUE` output, which produces a list of intersecting points by index for each polygon (e.g., MSOA). We then apply the `lengths()` function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.
:::

We can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.

```{r}
#| label: 03-thefts-per-km2
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| message: False
#| filename: "R code"
# msoa area size
msoa21 <- msoa21 |>
  mutate(km_sq = as.numeric(st_area(msoa21)) / 1e6)

# theft density
msoa21  <- msoa21 |>
  mutate(thef_km_sq = theft_bike_n/km_sq)
```

Let's put this onto a map:

```{r tidy='styler'} 
#| label: fig-03-choro-3
#| fig-cap: Number of reported bicycle thefts by square kilometre.
#| echo: True
#| eval: True
#| cache: True
#| filename: "R code"
# shape, polygons
tm_shape(msoa21) +

  # specify column, classes, labels, title
  tm_polygons(
    col= 'thef_km_sq', n = 5, style = 'jenks',
    border.col = '#ffffff',
    border.alpha = 0.3,
    palette = c('#fee5d9','#fcae91','#fb6a4a','#de2d26','#a50f15'), 
    title = 'Thefts / Square kilometre',
  ) +
  
  # set layout
  tm_layout(
    frame = FALSE,
    legend.position = c('left', 'bottom'),
  )

``` 
### Density-based methods

### Distance-based methods

## Before you leave
As geographers we are keen to understand our point data's distribution and understand whether are our points clustered, randomly distributed, uniform or dispersed. We have looked at various techniques that enable us to statistically and visually assess our data's distribution and understand whether our data is randomly distributed or clustered in space. And that is [us done for this week](https://www.youtube.com/watch?v=-zxtbwGogyY). Reading list anyone?