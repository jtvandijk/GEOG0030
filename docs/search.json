[
  {
    "objectID": "07-geodemographics.html",
    "href": "07-geodemographics.html",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "This week we will turn to geodemographic classification. Geodemographic classification is a method used to categorise geographic areas and the people living in them based on demographic, socioeconomic, and sometimes lifestyle characteristics. This approach combines geographic information with demographic data to create profiles of different neighborhoods.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nSingleton, A. and Longley, P. A. 2024. Classifying and mapping residential structure through the London Output Area Classification. Environment and Planning B: Urban Analytics and City Science 51(5): 1153-1164. [Link]\nWyszomierski, J., Longley, P. A., and Singleton, A. et al. 2024. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. 190(2): e12550. [Link]\n\n\n\n\n\nDalton, C. M. and Thatcher. J. 2015. Inflated granularity: Spatial “Big Data” and geodemographics. Big Data & Society 2(2): 1-15. [Link]\nFränti, P. and Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]\n\n\n\n\n\nToday, we will create our own geodemographic classification to examine demographic clusters across London, drawing inspiration from London Output Area Classification. Specifically, we will try to identify clusters based on age group, self-identified ethnicity, country of birth, and first or preferred language.\nThe data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. These datasets have been extracted using the Custom Dataset Tool, and you can download each file via the links provided below. A copy of the 2021 London LSOAs spatial boundaries is also available. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Country of Birth\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Main Language\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w07-geodemographic-classification.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggcorrplot)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the individual csv files that we downloaded into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/attributes/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load country of birth data\nlsoa_cob &lt;- read_csv(\"data/attributes/London-LSOA-Country-of-Birth.csv\")\n\nRows: 39952 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Country of birth (8 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load language data\nlsoa_lan &lt;- read_csv(\"data/attributes/London-LSOA-MainLanguage.csv\")\n\nRows: 54934 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Main language (11 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect age data\nhead(lsoa_age)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect country of birth data\nhead(lsoa_cob)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Country of birth (8 …³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Country of birth (8 categories) Code`\n# ℹ 2 more variables: `Country of birth (8 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect ethnicity data\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect language data\nhead(lsoa_lan)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Main language (11 ca…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Main language (11 categories) Code`\n# ℹ 2 more variables: `Main language (11 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\nTo identify geodemographic clusters in our dataset, we will use a technique called \\(k\\)-means. \\(k\\)-means aims to partition a set of standardised observations into a specified number of clusters (\\(k\\)). To do this we first need to prepare the individual datasets, as well as transform and standardise the input variables.\n\n\n\n\n\n\n\\(k\\)-means clustering is an unsupervised machine learning algorithm used to group data into a predefined number of clusters, based on similarities between data points. It works by initially assigning \\(k\\) random centroids, then iteratively updating them by assigning each data point to the nearest centroid and recalculating the centroid’s position based on the mean of the points in each cluster. The process continues until the centroids stabilise, meaning they no longer change significantly. \\(k\\)-means is often used for tasks such as data segmentation, image compression, or anomaly detection. It is simple but may not work well with non-spherical or overlapping clusters.\n\n\n\nBecause all the data are stored in long format, with each London LSOA appearing on multiple rows for each category — such as separate rows for different age groups, ethnicities, countries of birth, and first or preferred languages - we need to transform it into a wide format. For example, instead of having multiple rows for an LSOA showing counts for different age groups all the information for each LSOA will be consolidated into a single row. Additionally, we will clean up the column names to follow standard R naming conventions and make the data easier to work with. We can automate this process using the janitor package.\nWe will begin with the age dataframe:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"age_5_categories\",\n        values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions and only retain those columns that are likely to be meaningful in the context of the classification:\n\n\n\nR code\n\n# total observations\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(age_pop = sum(across(2:6)))\n\n# total proportions, select columns\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(2:6, ~./age_pop)) |&gt;\n    select(1:6)\n\n# inspect\nhead(lsoa_age)\n\n\n# A tibble: 6 × 6\n# Rowwise: \n  lower_layer_super_output_areas_code aged_15_years_and_un…¹ aged_16_to_24_years\n  &lt;chr&gt;                                                &lt;dbl&gt;               &lt;dbl&gt;\n1 E01000001                                           0.0846              0.0744\n2 E01000002                                           0.0621              0.0889\n3 E01000003                                           0.0682              0.0706\n4 E01000005                                           0.127               0.178 \n5 E01000006                                           0.224               0.120 \n6 E01000007                                           0.257               0.103 \n# ℹ abbreviated name: ¹​aged_15_years_and_under\n# ℹ 3 more variables: aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;\n\n\nThis looks much better. We can do the same for the country of birth data:\n\n\n\nR code\n\n# prepare country of birth data\nlsoa_cob &lt;- lsoa_cob |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"country_of_birth_8_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_cob &lt;- lsoa_cob |&gt;\n    rowwise() |&gt;\n    mutate(cob_pop = sum(across(2:9))) |&gt;\n    mutate(across(2:9, ~./cob_pop)) |&gt;\n    select(-2, -10)\n\n\nAnd we can do the same for the ethnicity and language datasets:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2, -22)\n\n# prepare language data\nlsoa_lan &lt;- lsoa_lan |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"main_language_11_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_lan &lt;- lsoa_lan |&gt;\n    rowwise() |&gt;\n    mutate(lan_pop = sum(across(2:12))) |&gt;\n    mutate(across(2:12, ~./lan_pop)) |&gt;\n    select(-2, -11, -13)\n\n\nWe now have four separate datasets, each containing the proportions of usual residents classified into different groups based on age, country of birth, ethnicity, and language.\n\n\n\nWhere we initially selected variables from different demographic domains, not all variables may be suitable for inclusion. Firstly, the variables need to exhibit sufficient heterogeneity to ensure they capture meaningful differences between observations. Secondly, variables should not be highly correlated with one another, as this redundancy can skew the clustering results. Ensuring acceptable correlation between variables helps maintain the diversity of information and improves the robustness of the clustering outcome.\n\n\n\n\n\n\nVariable selection is often a time-consuming process that requires a combination of domain knowledge and more extensive exploratory analysis than is covered in this practical.\n\n\n\nA straightforward yet effective method to examine the distribution of our variables is to create boxplots for each variable. This can be efficiently achieved by using facet_wrap() to generate a matrix of panels, allowing us to visualise all variables in a single view.\n\n\n\n\n\n\nFor more details on facet_wrap(), you can refer to the ggplot2 documentation.\n\n\n\n\n\n\nR code\n\n# wide to long\nlsoa_age_wd &lt;- lsoa_age |&gt;\n    pivot_longer(cols = c(2:5), names_to = \"agegroup\", values_to = \"count\")\n\n# facet age\nggplot(lsoa_age_wd, aes(y = count)) + geom_boxplot() + facet_wrap(~agegroup, ncol = 2) +\n    theme_minimal() + ylab(\"\")\n\n\n\n\n\nFigure 1: Boxplots of the distribution of the age dataset.\n\n\n\n\nWhen repeating this process for the birth, ethnicity, and language variables, you will notice that some variables have a very limited distribution. Specifically, some variables may have a value of 0 for the majority of London LSOAs. As a rule of thumb, we will retain only those variables where at least 75% of the LSOAs have values different from 0.\n\n\n\n\n\n\nThis threshold of 75% is arbitrary, and in practice, more thorough consideration should be given when deciding whether to include or exclude a variable.\n\n\n\n\n\n\nR code\n\n# join\nlsoa_df &lt;- lsoa_age |&gt;\n    left_join(lsoa_cob, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_eth, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_lan, by = \"lower_layer_super_output_areas_code\")\n\n# calculate proportion of zeroes\nzero_prop &lt;- sapply(lsoa_df[2:41], function(x) {\n    mean(x == 0)\n})\n\n# extract variables with high proportion zeroes\nidx &lt;- which(zero_prop &gt; 0.25)\n\n# inspect\nidx\n\n\n   white_gypsy_or_irish_traveller            any_other_uk_languages \n                               27                                33 \n  oceanic_or_australian_languages north_or_south_american_languages \n                               37                                38 \n\n# remove variables with high proportion zeroes\nlsoa_df &lt;- lsoa_df |&gt;\n    select(-white_gypsy_or_irish_traveller, -any_other_uk_languages, -oceanic_or_australian_languages,\n        -north_or_south_american_languages)\n\n\n\n\n\n\n\nThe code above makes use of Boolean logic to calculate the proportion of zeroes within each variable. The x == 0 part checks each value in column x to see if it is equal to 0, returning TRUE or FALSE for each element. The mean() function is then used to calculate the average of the TRUE values in the column. Since TRUE is treated as 1 and FALSE as 0, this gives the proportion of values in the column that are equal to zero.\n\n\n\nWe can subsequently check for multicollinearity of the remaining variables. The easiest way to check the correlations between all variables is probably by visualising a correlation matrix:\n\n\n\nR code\n\n# inspect variable names\nnames(lsoa_df)\n\n\n [1] \"lower_layer_super_output_areas_code\"                                  \n [2] \"aged_15_years_and_under\"                                              \n [3] \"aged_16_to_24_years\"                                                  \n [4] \"aged_25_to_34_years\"                                                  \n [5] \"aged_35_to_49_years\"                                                  \n [6] \"aged_50_years_and_over\"                                               \n [7] \"europe_united_kingdom\"                                                \n [8] \"europe_ireland\"                                                       \n [9] \"europe_other_europe\"                                                  \n[10] \"africa\"                                                               \n[11] \"middle_east_and_asia\"                                                 \n[12] \"the_americas_and_the_caribbean\"                                       \n[13] \"antarctica_and_oceania_including_australasia_and_other\"               \n[14] \"asian_asian_british_or_asian_welsh_bangladeshi\"                       \n[15] \"asian_asian_british_or_asian_welsh_chinese\"                           \n[16] \"asian_asian_british_or_asian_welsh_indian\"                            \n[17] \"asian_asian_british_or_asian_welsh_pakistani\"                         \n[18] \"asian_asian_british_or_asian_welsh_other_asian\"                       \n[19] \"black_black_british_black_welsh_caribbean_or_african_african\"         \n[20] \"black_black_british_black_welsh_caribbean_or_african_caribbean\"       \n[21] \"black_black_british_black_welsh_caribbean_or_african_other_black\"     \n[22] \"mixed_or_multiple_ethnic_groups_white_and_asian\"                      \n[23] \"mixed_or_multiple_ethnic_groups_white_and_black_african\"              \n[24] \"mixed_or_multiple_ethnic_groups_white_and_black_caribbean\"            \n[25] \"mixed_or_multiple_ethnic_groups_other_mixed_or_multiple_ethnic_groups\"\n[26] \"white_english_welsh_scottish_northern_irish_or_british\"               \n[27] \"white_irish\"                                                          \n[28] \"white_roma\"                                                           \n[29] \"white_other_white\"                                                    \n[30] \"other_ethnic_group_arab\"                                              \n[31] \"other_ethnic_group_any_other_ethnic_group\"                            \n[32] \"english_or_welsh\"                                                     \n[33] \"european_languages_eu\"                                                \n[34] \"other_european_languages_non_eu\"                                      \n[35] \"asian_languages\"                                                      \n[36] \"african_languages\"                                                    \n[37] \"any_other_languages\"                                                  \n\n# change variable names to index to improve visualisation\nlsoa_df_vis &lt;- lsoa_df\nnames(lsoa_df_vis)[2:37] &lt;- paste0(\"v\", sprintf(\"%02d\", 1:36))\n\n# correlation matrix\ncor_mat &lt;- cor(lsoa_df_vis[, -1])\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\nFigure 2: Correlation plot of classification variables.\n\n\n\n\nFollowing the approach from Wyszomierski et al. (2024), we can define a weak correlation as lying between 0 and 0.40, moderate as between 0.41 and 0.65, strong as between 0.66 and 0.80, and very strong as between 0.81 and 1.\nA few strong and very strong correlations can be observed that potentially could be removed; however, to maintain representation, here we decide to retain all variables.\n\n\n\nIf the input data are heavily skewed or contain outliers, \\(k\\)-means may produce less meaningful clusters. While normality is not required per se, it has been common to do this nonetheless. More important is to standardise the input variables, especially when they are measured on different scales. This ensures that each variable contributes equally to the clustering process.\n\n\n\nR code\n\n# inverse hyperbolic sine\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], asinh)\n\n# range standardise\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], function(x) {\n    (x - min(x))/(max(x) - min(x))\n})\n\n\n\n\n\nNow our data are prepared we will start by creating an elbow plot. The elbow method is a visual tool that helps determine the optimal number of clusters in a dataset. This is important because with \\(k\\)-means clustering you need to specify the numbers of clusters a priori. The elbow method involves running the clustering algorithm with varying numbers of clusters (\\(k\\)) and plotting the total explained variation (known as the Within Sum of Squares) against the number of clusters. The goal is to identify the ‘elbow’ point on the curve, where the rate of decrease in explained variation starts to slow. This point suggests that adding more clusters yields diminishing returns in terms of explained variation.\n\n\n\nR code\n\n# elbow plot\nfviz_nbclust(lsoa_df_vis[, -1], kmeans, nstart = 100, iter.max = 100, method = \"wss\")\n\n\n\n\n\nFigure 3: Elbow plot with ‘Within Sum of Squares’ against number of clusters.\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 6 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective, and multiple options for the optimal number of clusters might be justified; for instance, 4, 5, or even 7 clusters could be reasonable choices. In addition to the elbow method, other techniques can aid in determining the optimal number of clusters, such as silhouette scores and the gap statistic. An alternative and helful approach is to use a clustergram, which is a two-dimensional plot that visualises the flows of observations between clusters as more clusters are added. This method illustrates how your data reshuffles with each additional cluster and provides insights into the quality of the splits. This method can be done in R, but currently easier to implement in Python.\n\n\n\n\n\n\nNow we have decided on the number of clusters, we can run our \\(k\\)-means analysis.\n\n\n\nR code\n\n# set seed for reproducibility\nset.seed(999)\n\n# k-means\nlsoa_clus &lt;- kmeans(lsoa_df_vis[, -1], centers = 6, nstart = 100, iter.max = 100)\n\n\nWe can inspect the object to get some information about our clusters:\n\n\n\nR code\n\n# inspect\nlsoa_clus\n\n\nK-means clustering with 6 clusters of sizes 796, 1097, 771, 1011, 851, 468\n\nCluster means:\n        v01       v02       v03       v04       v05       v06       v07\n1 0.4816225 0.1632210 0.2425566 0.4838983 0.4169123 0.5410477 0.1337158\n        v08       v09       v10        v11        v12        v13        v14\n1 0.3007540 0.2480613 0.2859754 0.08913663 0.05177222 0.14603013 0.06993627\n         v15        v16        v17        v18        v19        v20        v21\n1 0.12176548 0.10935022 0.20109979 0.18150482 0.11605092 0.12757934 0.09288236\n         v22        v23       v24       v25       v26        v27       v28\n1 0.07473711 0.14662903 0.1842217 0.3522638 0.1490577 0.02997040 0.2423784\n         v29       v30       v31       v32        v33        v34        v35\n1 0.07148504 0.2193009 0.5870244 0.2411272 0.07541661 0.23467242 0.10174187\n         v36\n1 0.10216507\n [ reached getOption(\"max.print\") -- omitted 5 rows ]\n\nClustering vector:\n [1] 4 4 4 1 1 5 5 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 2 1 1 2 2 2 1 1 1\n[39] 1 1 1 1 5 1 5 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4944 entries ]\n\nWithin cluster sum of squares by cluster:\n[1] 259.0272 177.7951 288.8625 232.7770 298.9145 160.1702\n (between_SS / total_SS =  48.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\nWe now need to perform some post-processing to extract useful summary data for each cluster. To characterise the clusters, we can compare the global mean values of each variable with the mean values specific to each cluster.\n\n\n\nR code\n\n# global means\nglob_means &lt;- colMeans(lsoa_df_vis[, -1])\n\n# add clusters to input data\nlsoa_df_vis &lt;- cbind(lsoa_df_vis, cluster = lsoa_clus$cluster)\n\n# cluster means\ncluster_means &lt;- lsoa_df_vis |&gt;\n    group_by(cluster) |&gt;\n    summarise(across(2:37, mean))\n\n# difference\ncluster_diffs &lt;- cluster_means |&gt;\n    mutate(across(2:37, ~. - glob_means[cur_column()]))\n\n\nThese comparisons can then be visualised using, for instance, a radial bar plot:\n\n\n\nR code\n\n# to long format\ncluster_diffs_long &lt;- cluster_diffs |&gt;\n    pivot_longer(!cluster, names_to = \"vars\", values_to = \"score\")\n\n# facet clusters\nggplot(cluster_diffs_long, aes(x = factor(vars), y = score)) + geom_bar(stat = \"identity\") +\n    coord_radial(rotate.angle = TRUE, expand = FALSE) + facet_wrap(~cluster, ncol = 3) +\n    theme_minimal() + theme(axis.text.x = element_text(size = 7)) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\nFigure 4: Radial barplots of cluster means for each input variable.\n\n\n\n\nThese plots can serve as a foundation for creating pen portraits by closely examining which variables drive each cluster.\n\n\n\n\n\n\nFor easier interpretation, these values can be transformed into index scores, allowing us to assess which variables are under- or overrepresented within each cluster group.\n\n\n\nOf course, we can also map the results:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join\nlsoa21 &lt;- cbind(lsoa21, cluster = lsoa_clus$cluster)\n\n# shape, polygon\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"cluster\",\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Cluster number\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 5: Classification of London LSOAs based on several demographic variables.\n\n\n\n\n\n\n\n\nThe creation of a geodemographic classification is an iterative process. This typically includes adding or removing variables, adjusting the number of clusters, and grouping data in different ways to achieve the most meaningful segmentation. Try to do the following:\n\nDownload the two datasets provided below and save them to your data folder. The datasets include:\n\nA csv file containing the number of people aged 16 years and older by occupational category, as defined by the Standard Occupational Classification 2020, aggregated by 2021 LSOAs.\nA csv file containing the number of people aged 16 years and older by their highest level of qualification, also aggregated to the 2021 LSOA level.\n\nPrepare these two datasets and retain only those variables that are potentially meaningful. Filter out any variables with a high proportion of zero values.\nMerge the education and occupation dataset with the dataset used to generate the initial geodemographic classification. Check for multicollinearity and consider removing any variables that are highly correlated.\nPerform \\(k\\)-means clustering on your extended dataset. Make sure to select an appropriate number of clusters for your analysis.\nInterpret the individual clusters in terms of the variables that are under- and overrepresented.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Occupation\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Education\ncsv\nDownload\n\n\n\n\n\n\nHaving finished this tutorial, you should now understand the basics of a geodemographic classification. That is all for this week!"
  },
  {
    "objectID": "07-geodemographics.html#lecture-w07",
    "href": "07-geodemographics.html#lecture-w07",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "07-geodemographics.html#reading-w07",
    "href": "07-geodemographics.html#reading-w07",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Longley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nSingleton, A. and Longley, P. A. 2024. Classifying and mapping residential structure through the London Output Area Classification. Environment and Planning B: Urban Analytics and City Science 51(5): 1153-1164. [Link]\nWyszomierski, J., Longley, P. A., and Singleton, A. et al. 2024. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. 190(2): e12550. [Link]\n\n\n\n\n\nDalton, C. M. and Thatcher. J. 2015. Inflated granularity: Spatial “Big Data” and geodemographics. Big Data & Society 2(2): 1-15. [Link]\nFränti, P. and Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]"
  },
  {
    "objectID": "07-geodemographics.html#classifying-london",
    "href": "07-geodemographics.html#classifying-london",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Today, we will create our own geodemographic classification to examine demographic clusters across London, drawing inspiration from London Output Area Classification. Specifically, we will try to identify clusters based on age group, self-identified ethnicity, country of birth, and first or preferred language.\nThe data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. These datasets have been extracted using the Custom Dataset Tool, and you can download each file via the links provided below. A copy of the 2021 London LSOAs spatial boundaries is also available. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Country of Birth\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Main Language\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w07-geodemographic-classification.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggcorrplot)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the individual csv files that we downloaded into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/attributes/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load country of birth data\nlsoa_cob &lt;- read_csv(\"data/attributes/London-LSOA-Country-of-Birth.csv\")\n\nRows: 39952 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Country of birth (8 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load language data\nlsoa_lan &lt;- read_csv(\"data/attributes/London-LSOA-MainLanguage.csv\")\n\nRows: 54934 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Main language (11 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect age data\nhead(lsoa_age)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect country of birth data\nhead(lsoa_cob)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Country of birth (8 …³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Country of birth (8 categories) Code`\n# ℹ 2 more variables: `Country of birth (8 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect ethnicity data\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect language data\nhead(lsoa_lan)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Main language (11 ca…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Main language (11 categories) Code`\n# ℹ 2 more variables: `Main language (11 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\nTo identify geodemographic clusters in our dataset, we will use a technique called \\(k\\)-means. \\(k\\)-means aims to partition a set of standardised observations into a specified number of clusters (\\(k\\)). To do this we first need to prepare the individual datasets, as well as transform and standardise the input variables.\n\n\n\n\n\n\n\\(k\\)-means clustering is an unsupervised machine learning algorithm used to group data into a predefined number of clusters, based on similarities between data points. It works by initially assigning \\(k\\) random centroids, then iteratively updating them by assigning each data point to the nearest centroid and recalculating the centroid’s position based on the mean of the points in each cluster. The process continues until the centroids stabilise, meaning they no longer change significantly. \\(k\\)-means is often used for tasks such as data segmentation, image compression, or anomaly detection. It is simple but may not work well with non-spherical or overlapping clusters.\n\n\n\nBecause all the data are stored in long format, with each London LSOA appearing on multiple rows for each category — such as separate rows for different age groups, ethnicities, countries of birth, and first or preferred languages - we need to transform it into a wide format. For example, instead of having multiple rows for an LSOA showing counts for different age groups all the information for each LSOA will be consolidated into a single row. Additionally, we will clean up the column names to follow standard R naming conventions and make the data easier to work with. We can automate this process using the janitor package.\nWe will begin with the age dataframe:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"age_5_categories\",\n        values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions and only retain those columns that are likely to be meaningful in the context of the classification:\n\n\n\nR code\n\n# total observations\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(age_pop = sum(across(2:6)))\n\n# total proportions, select columns\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(2:6, ~./age_pop)) |&gt;\n    select(1:6)\n\n# inspect\nhead(lsoa_age)\n\n\n# A tibble: 6 × 6\n# Rowwise: \n  lower_layer_super_output_areas_code aged_15_years_and_un…¹ aged_16_to_24_years\n  &lt;chr&gt;                                                &lt;dbl&gt;               &lt;dbl&gt;\n1 E01000001                                           0.0846              0.0744\n2 E01000002                                           0.0621              0.0889\n3 E01000003                                           0.0682              0.0706\n4 E01000005                                           0.127               0.178 \n5 E01000006                                           0.224               0.120 \n6 E01000007                                           0.257               0.103 \n# ℹ abbreviated name: ¹​aged_15_years_and_under\n# ℹ 3 more variables: aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;\n\n\nThis looks much better. We can do the same for the country of birth data:\n\n\n\nR code\n\n# prepare country of birth data\nlsoa_cob &lt;- lsoa_cob |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"country_of_birth_8_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_cob &lt;- lsoa_cob |&gt;\n    rowwise() |&gt;\n    mutate(cob_pop = sum(across(2:9))) |&gt;\n    mutate(across(2:9, ~./cob_pop)) |&gt;\n    select(-2, -10)\n\n\nAnd we can do the same for the ethnicity and language datasets:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2, -22)\n\n# prepare language data\nlsoa_lan &lt;- lsoa_lan |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"main_language_11_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_lan &lt;- lsoa_lan |&gt;\n    rowwise() |&gt;\n    mutate(lan_pop = sum(across(2:12))) |&gt;\n    mutate(across(2:12, ~./lan_pop)) |&gt;\n    select(-2, -11, -13)\n\n\nWe now have four separate datasets, each containing the proportions of usual residents classified into different groups based on age, country of birth, ethnicity, and language.\n\n\n\nWhere we initially selected variables from different demographic domains, not all variables may be suitable for inclusion. Firstly, the variables need to exhibit sufficient heterogeneity to ensure they capture meaningful differences between observations. Secondly, variables should not be highly correlated with one another, as this redundancy can skew the clustering results. Ensuring acceptable correlation between variables helps maintain the diversity of information and improves the robustness of the clustering outcome.\n\n\n\n\n\n\nVariable selection is often a time-consuming process that requires a combination of domain knowledge and more extensive exploratory analysis than is covered in this practical.\n\n\n\nA straightforward yet effective method to examine the distribution of our variables is to create boxplots for each variable. This can be efficiently achieved by using facet_wrap() to generate a matrix of panels, allowing us to visualise all variables in a single view.\n\n\n\n\n\n\nFor more details on facet_wrap(), you can refer to the ggplot2 documentation.\n\n\n\n\n\n\nR code\n\n# wide to long\nlsoa_age_wd &lt;- lsoa_age |&gt;\n    pivot_longer(cols = c(2:5), names_to = \"agegroup\", values_to = \"count\")\n\n# facet age\nggplot(lsoa_age_wd, aes(y = count)) + geom_boxplot() + facet_wrap(~agegroup, ncol = 2) +\n    theme_minimal() + ylab(\"\")\n\n\n\n\n\nFigure 1: Boxplots of the distribution of the age dataset.\n\n\n\n\nWhen repeating this process for the birth, ethnicity, and language variables, you will notice that some variables have a very limited distribution. Specifically, some variables may have a value of 0 for the majority of London LSOAs. As a rule of thumb, we will retain only those variables where at least 75% of the LSOAs have values different from 0.\n\n\n\n\n\n\nThis threshold of 75% is arbitrary, and in practice, more thorough consideration should be given when deciding whether to include or exclude a variable.\n\n\n\n\n\n\nR code\n\n# join\nlsoa_df &lt;- lsoa_age |&gt;\n    left_join(lsoa_cob, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_eth, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_lan, by = \"lower_layer_super_output_areas_code\")\n\n# calculate proportion of zeroes\nzero_prop &lt;- sapply(lsoa_df[2:41], function(x) {\n    mean(x == 0)\n})\n\n# extract variables with high proportion zeroes\nidx &lt;- which(zero_prop &gt; 0.25)\n\n# inspect\nidx\n\n\n   white_gypsy_or_irish_traveller            any_other_uk_languages \n                               27                                33 \n  oceanic_or_australian_languages north_or_south_american_languages \n                               37                                38 \n\n# remove variables with high proportion zeroes\nlsoa_df &lt;- lsoa_df |&gt;\n    select(-white_gypsy_or_irish_traveller, -any_other_uk_languages, -oceanic_or_australian_languages,\n        -north_or_south_american_languages)\n\n\n\n\n\n\n\nThe code above makes use of Boolean logic to calculate the proportion of zeroes within each variable. The x == 0 part checks each value in column x to see if it is equal to 0, returning TRUE or FALSE for each element. The mean() function is then used to calculate the average of the TRUE values in the column. Since TRUE is treated as 1 and FALSE as 0, this gives the proportion of values in the column that are equal to zero.\n\n\n\nWe can subsequently check for multicollinearity of the remaining variables. The easiest way to check the correlations between all variables is probably by visualising a correlation matrix:\n\n\n\nR code\n\n# inspect variable names\nnames(lsoa_df)\n\n\n [1] \"lower_layer_super_output_areas_code\"                                  \n [2] \"aged_15_years_and_under\"                                              \n [3] \"aged_16_to_24_years\"                                                  \n [4] \"aged_25_to_34_years\"                                                  \n [5] \"aged_35_to_49_years\"                                                  \n [6] \"aged_50_years_and_over\"                                               \n [7] \"europe_united_kingdom\"                                                \n [8] \"europe_ireland\"                                                       \n [9] \"europe_other_europe\"                                                  \n[10] \"africa\"                                                               \n[11] \"middle_east_and_asia\"                                                 \n[12] \"the_americas_and_the_caribbean\"                                       \n[13] \"antarctica_and_oceania_including_australasia_and_other\"               \n[14] \"asian_asian_british_or_asian_welsh_bangladeshi\"                       \n[15] \"asian_asian_british_or_asian_welsh_chinese\"                           \n[16] \"asian_asian_british_or_asian_welsh_indian\"                            \n[17] \"asian_asian_british_or_asian_welsh_pakistani\"                         \n[18] \"asian_asian_british_or_asian_welsh_other_asian\"                       \n[19] \"black_black_british_black_welsh_caribbean_or_african_african\"         \n[20] \"black_black_british_black_welsh_caribbean_or_african_caribbean\"       \n[21] \"black_black_british_black_welsh_caribbean_or_african_other_black\"     \n[22] \"mixed_or_multiple_ethnic_groups_white_and_asian\"                      \n[23] \"mixed_or_multiple_ethnic_groups_white_and_black_african\"              \n[24] \"mixed_or_multiple_ethnic_groups_white_and_black_caribbean\"            \n[25] \"mixed_or_multiple_ethnic_groups_other_mixed_or_multiple_ethnic_groups\"\n[26] \"white_english_welsh_scottish_northern_irish_or_british\"               \n[27] \"white_irish\"                                                          \n[28] \"white_roma\"                                                           \n[29] \"white_other_white\"                                                    \n[30] \"other_ethnic_group_arab\"                                              \n[31] \"other_ethnic_group_any_other_ethnic_group\"                            \n[32] \"english_or_welsh\"                                                     \n[33] \"european_languages_eu\"                                                \n[34] \"other_european_languages_non_eu\"                                      \n[35] \"asian_languages\"                                                      \n[36] \"african_languages\"                                                    \n[37] \"any_other_languages\"                                                  \n\n# change variable names to index to improve visualisation\nlsoa_df_vis &lt;- lsoa_df\nnames(lsoa_df_vis)[2:37] &lt;- paste0(\"v\", sprintf(\"%02d\", 1:36))\n\n# correlation matrix\ncor_mat &lt;- cor(lsoa_df_vis[, -1])\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\nFigure 2: Correlation plot of classification variables.\n\n\n\n\nFollowing the approach from Wyszomierski et al. (2024), we can define a weak correlation as lying between 0 and 0.40, moderate as between 0.41 and 0.65, strong as between 0.66 and 0.80, and very strong as between 0.81 and 1.\nA few strong and very strong correlations can be observed that potentially could be removed; however, to maintain representation, here we decide to retain all variables.\n\n\n\nIf the input data are heavily skewed or contain outliers, \\(k\\)-means may produce less meaningful clusters. While normality is not required per se, it has been common to do this nonetheless. More important is to standardise the input variables, especially when they are measured on different scales. This ensures that each variable contributes equally to the clustering process.\n\n\n\nR code\n\n# inverse hyperbolic sine\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], asinh)\n\n# range standardise\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], function(x) {\n    (x - min(x))/(max(x) - min(x))\n})\n\n\n\n\n\nNow our data are prepared we will start by creating an elbow plot. The elbow method is a visual tool that helps determine the optimal number of clusters in a dataset. This is important because with \\(k\\)-means clustering you need to specify the numbers of clusters a priori. The elbow method involves running the clustering algorithm with varying numbers of clusters (\\(k\\)) and plotting the total explained variation (known as the Within Sum of Squares) against the number of clusters. The goal is to identify the ‘elbow’ point on the curve, where the rate of decrease in explained variation starts to slow. This point suggests that adding more clusters yields diminishing returns in terms of explained variation.\n\n\n\nR code\n\n# elbow plot\nfviz_nbclust(lsoa_df_vis[, -1], kmeans, nstart = 100, iter.max = 100, method = \"wss\")\n\n\n\n\n\nFigure 3: Elbow plot with ‘Within Sum of Squares’ against number of clusters.\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 6 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective, and multiple options for the optimal number of clusters might be justified; for instance, 4, 5, or even 7 clusters could be reasonable choices. In addition to the elbow method, other techniques can aid in determining the optimal number of clusters, such as silhouette scores and the gap statistic. An alternative and helful approach is to use a clustergram, which is a two-dimensional plot that visualises the flows of observations between clusters as more clusters are added. This method illustrates how your data reshuffles with each additional cluster and provides insights into the quality of the splits. This method can be done in R, but currently easier to implement in Python.\n\n\n\n\n\n\nNow we have decided on the number of clusters, we can run our \\(k\\)-means analysis.\n\n\n\nR code\n\n# set seed for reproducibility\nset.seed(999)\n\n# k-means\nlsoa_clus &lt;- kmeans(lsoa_df_vis[, -1], centers = 6, nstart = 100, iter.max = 100)\n\n\nWe can inspect the object to get some information about our clusters:\n\n\n\nR code\n\n# inspect\nlsoa_clus\n\n\nK-means clustering with 6 clusters of sizes 796, 1097, 771, 1011, 851, 468\n\nCluster means:\n        v01       v02       v03       v04       v05       v06       v07\n1 0.4816225 0.1632210 0.2425566 0.4838983 0.4169123 0.5410477 0.1337158\n        v08       v09       v10        v11        v12        v13        v14\n1 0.3007540 0.2480613 0.2859754 0.08913663 0.05177222 0.14603013 0.06993627\n         v15        v16        v17        v18        v19        v20        v21\n1 0.12176548 0.10935022 0.20109979 0.18150482 0.11605092 0.12757934 0.09288236\n         v22        v23       v24       v25       v26        v27       v28\n1 0.07473711 0.14662903 0.1842217 0.3522638 0.1490577 0.02997040 0.2423784\n         v29       v30       v31       v32        v33        v34        v35\n1 0.07148504 0.2193009 0.5870244 0.2411272 0.07541661 0.23467242 0.10174187\n         v36\n1 0.10216507\n [ reached getOption(\"max.print\") -- omitted 5 rows ]\n\nClustering vector:\n [1] 4 4 4 1 1 5 5 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 2 1 1 2 2 2 1 1 1\n[39] 1 1 1 1 5 1 5 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4944 entries ]\n\nWithin cluster sum of squares by cluster:\n[1] 259.0272 177.7951 288.8625 232.7770 298.9145 160.1702\n (between_SS / total_SS =  48.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\nWe now need to perform some post-processing to extract useful summary data for each cluster. To characterise the clusters, we can compare the global mean values of each variable with the mean values specific to each cluster.\n\n\n\nR code\n\n# global means\nglob_means &lt;- colMeans(lsoa_df_vis[, -1])\n\n# add clusters to input data\nlsoa_df_vis &lt;- cbind(lsoa_df_vis, cluster = lsoa_clus$cluster)\n\n# cluster means\ncluster_means &lt;- lsoa_df_vis |&gt;\n    group_by(cluster) |&gt;\n    summarise(across(2:37, mean))\n\n# difference\ncluster_diffs &lt;- cluster_means |&gt;\n    mutate(across(2:37, ~. - glob_means[cur_column()]))\n\n\nThese comparisons can then be visualised using, for instance, a radial bar plot:\n\n\n\nR code\n\n# to long format\ncluster_diffs_long &lt;- cluster_diffs |&gt;\n    pivot_longer(!cluster, names_to = \"vars\", values_to = \"score\")\n\n# facet clusters\nggplot(cluster_diffs_long, aes(x = factor(vars), y = score)) + geom_bar(stat = \"identity\") +\n    coord_radial(rotate.angle = TRUE, expand = FALSE) + facet_wrap(~cluster, ncol = 3) +\n    theme_minimal() + theme(axis.text.x = element_text(size = 7)) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\nFigure 4: Radial barplots of cluster means for each input variable.\n\n\n\n\nThese plots can serve as a foundation for creating pen portraits by closely examining which variables drive each cluster.\n\n\n\n\n\n\nFor easier interpretation, these values can be transformed into index scores, allowing us to assess which variables are under- or overrepresented within each cluster group.\n\n\n\nOf course, we can also map the results:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join\nlsoa21 &lt;- cbind(lsoa21, cluster = lsoa_clus$cluster)\n\n# shape, polygon\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"cluster\",\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Cluster number\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 5: Classification of London LSOAs based on several demographic variables."
  },
  {
    "objectID": "07-geodemographics.html#assignment",
    "href": "07-geodemographics.html#assignment",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "The creation of a geodemographic classification is an iterative process. This typically includes adding or removing variables, adjusting the number of clusters, and grouping data in different ways to achieve the most meaningful segmentation. Try to do the following:\n\nDownload the two datasets provided below and save them to your data folder. The datasets include:\n\nA csv file containing the number of people aged 16 years and older by occupational category, as defined by the Standard Occupational Classification 2020, aggregated by 2021 LSOAs.\nA csv file containing the number of people aged 16 years and older by their highest level of qualification, also aggregated to the 2021 LSOA level.\n\nPrepare these two datasets and retain only those variables that are potentially meaningful. Filter out any variables with a high proportion of zero values.\nMerge the education and occupation dataset with the dataset used to generate the initial geodemographic classification. Check for multicollinearity and consider removing any variables that are highly correlated.\nPerform \\(k\\)-means clustering on your extended dataset. Make sure to select an appropriate number of clusters for your analysis.\nInterpret the individual clusters in terms of the variables that are under- and overrepresented.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Occupation\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Education\ncsv\nDownload"
  },
  {
    "objectID": "07-geodemographics.html#before-you-leave",
    "href": "07-geodemographics.html#before-you-leave",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Having finished this tutorial, you should now understand the basics of a geodemographic classification. That is all for this week!"
  },
  {
    "objectID": "10-maps.html",
    "href": "10-maps.html",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "1 Beyond the Choropleth\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis while providing a thorough introduction to programming. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, and cleaning spatial, demographic, and socioeconomic datasets, and apply core spatial analysis techniques to interpret them.\n\n\n\nMoodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial.\n\n\n\nThe topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nProgramming for Spatial Analysis\n\n\n2\nCore Spatial Analysis\nAnalysing Point Data\n\n\n3\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nComplex Visualisations\n\n\n10\nData Visualisation\nBeyond the Choropleth\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2024-2025 academic year. The content for 2023-2024 has been archived and can be found here: [Link]\n\n\n\n\n\n\nFor specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hour.\nBook into the Academic Support and Feedback hours.\n\n\n\n\n\n\n\n\n\n\nThis year’s version features the following major updates:\n\nSecond full rewrite of the workbook using Quarto.\nImproved alignment with GEOG0018 Methods in Human Geography.\nNew material on geographically weighted statistics, geodemographic classification, and data visualisation.\nFully updated material on point data analysis, spatial queries and geometric operations, spatial autocorrelation, and accessibility analysis.\nIntroduction of package management using renv.\n\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis while providing a thorough introduction to programming. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, and cleaning spatial, demographic, and socioeconomic datasets, and apply core spatial analysis techniques to interpret them."
  },
  {
    "objectID": "00-index.html#moodle",
    "href": "00-index.html#moodle",
    "title": "Geocomputation",
    "section": "",
    "text": "Moodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial."
  },
  {
    "objectID": "00-index.html#module-overview",
    "href": "00-index.html#module-overview",
    "title": "Geocomputation",
    "section": "",
    "text": "The topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nProgramming for Spatial Analysis\n\n\n2\nCore Spatial Analysis\nAnalysing Point Data\n\n\n3\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nComplex Visualisations\n\n\n10\nData Visualisation\nBeyond the Choropleth\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2024-2025 academic year. The content for 2023-2024 has been archived and can be found here: [Link]"
  },
  {
    "objectID": "00-index.html#troubleshooting",
    "href": "00-index.html#troubleshooting",
    "title": "Geocomputation",
    "section": "",
    "text": "For specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hour.\nBook into the Academic Support and Feedback hours."
  },
  {
    "objectID": "00-index.html#major-updates",
    "href": "00-index.html#major-updates",
    "title": "Geocomputation",
    "section": "",
    "text": "This year’s version features the following major updates:\n\nSecond full rewrite of the workbook using Quarto.\nImproved alignment with GEOG0018 Methods in Human Geography.\nNew material on geographically weighted statistics, geodemographic classification, and data visualisation.\nFully updated material on point data analysis, spatial queries and geometric operations, spatial autocorrelation, and accessibility analysis.\nIntroduction of package management using renv."
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "Geocomputation",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "01-spatial.html",
    "href": "01-spatial.html",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "1 Programming for Spatial Analysis\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, this installation of some of these libraries may fail since additional (non-R) libraries are required (which are automatically installed for Windows users). If this is the case, please refer to the information pages of the sf library for instructions on how to install these additional libraries."
  },
  {
    "objectID": "02-point-pattern.html",
    "href": "02-point-pattern.html",
    "title": "1 Analysing Point Data",
    "section": "",
    "text": "1 Analysing Point Data\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "05-models.html",
    "href": "05-models.html",
    "title": "1 Spatial Models",
    "section": "",
    "text": "1 Spatial Models\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "08-network.html",
    "href": "08-network.html",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Accessibility is often described as the ease with which individuals can reach places and opportunities, such as employment, public services, and cultural activities. We can utilise transport network data to quantify accessibility and characterise areas based on their accessibility levels. This week, we will use the dodgr R library to measure accessibility between different points of interest by calculating the network distances between them.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGeurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\n\n\n\n\n\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]\n\n\n\n\n\nThis week, we will analyse the accessibility of fast-food outlets in the London Borough of Lambeth. Specifically, we will examine how closely these outlets are located within walking distance of primary and secondary schools, and explore any potential relationships between their proximity and the relative levels of deprivation in the area.\nWe will extract the points of interest that we will use for this analysis from the Point of Interest (POI) data for the United Kingdom, obtained from the Overture Maps Foundation and pre-processed by the Consumer Data Research Centre to provide users with easy access.\nYou can download a subset of the POI dataset via the link provided below. A copy of the 2011 London LSOAs spatial boundaries, the boundaries of the London Boroughs, and the 2019 English Index of Multiple Deprivation. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLambeth Overture Points of Interest 2024\nGeoPackage\nDownload\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\nEngland 2019 Index of Multiple Deprivation\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo extract the Lambeth Overture Points of Interest data, a 2-kilometre buffer was applied around the boundaries of Lambeth. This approach ensures that points just outside the study area are included, as locations beyond the borough boundary may still be accessible to residents and could represent the nearest available options.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w08-accessibility-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the spatial data into R.\n\n\n\nR code\n\n# read poi data\npoi24 &lt;- st_read(\"data/spatial/Lambeth-POI-2024.gpkg\")\n\n\nReading layer `Lambeth-POI-2024' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/Lambeth-POI-2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 65060 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526556.6 ymin: 167827 xmax: 535640.4 ymax: 182673.8\nProjected CRS: OSGB36 / British National Grid\n\n# read lsoa dataset\nlsoa11 &lt;- st_read(\"data/spatial/London-LSOA-2011.gpkg\")\n\nReading layer `London-LSOA-2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4835 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# read borough dataset\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\")\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect poi data\nhead(poi24)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526913.4 ymin: 169695.2 xmax: 526945.5 ymax: 169970.8\nProjected CRS: OSGB36 / British National Grid\n                                id    primary_name       main_category\n1 08f194ada9716b86030eab41bbd4207e \"Gorgeous Grub\" \"burger_restaurant\"\n2 08f194ada9715a1903d73f4aef170602    \"TLC Direct\"   \"wholesale_store\"\n3 08f194ada944cba203fa613de4f5e6d5     \"JD Sports\"       \"sports_wear\"\n4 08f194ada9449a8a0345a466a0a6ece9       \"Lidl GB\"       \"supermarket\"\n                    alternate_category                                 address\n1   eat_and_drink|fast_food_restaurant                 \"1 Prince Georges Road\"\n2 professional_services|lighting_store                      \"280 Western Road\"\n3            sporting_goods|shoe_store \"Unit 2 Tandem Centre Top Of Church Rd\"\n4          retail|fast_food_restaurant                         \"Colliers Wood\"\n         locality   postcode region country source   source_record_id\n1        \"London\"   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\"  \"232538816864698\"\n2        \"London\" \"SW19 2QA\"  \"ENG\"    \"GB\" \"meta\" \"1959707454355017\"\n3 \"Colliers Wood\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"644899945690935\"\n4        \"London\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"111430837210163\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526921.1 16969...\n3 MULTIPOINT ((526915.7 16997...\n4 MULTIPOINT ((526922.2 16988...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect lsoa dataset\nhead(lsoa11)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa11cd            lsoa11nm           lsoa11nmw  bng_e  bng_n     long\n1 E01000001 City of London 001A City of London 001A 532129 181625 -0.09706\n2 E01000002 City of London 001B City of London 001B 532480 181699 -0.09197\n3 E01000003 City of London 001C City of London 001C 532245 182036 -0.09523\n4 E01000005 City of London 001E City of London 001E 533581 181265 -0.07628\n       lat                               globalid          lsoa11_name pop2011\n1 51.51810 {283B0EAD-F8FC-40B6-9A79-1DDD7E5C0758}  City of London 001A    1465\n2 51.51868 {DDCE266B-7825-428C-9E0A-DF66B0179A55}  City of London 001B    1436\n3 51.52176 {C45E358E-A794-485A-BF76-D96E5D458EA4}  City of London 001C    1346\n4 51.51452 {4DDAF5E4-E47F-4312-89A0-923FFEC028A6}  City of London 001E     985\n                            geom\n1 MULTIPOLYGON (((532105.1 18...\n2 MULTIPOLYGON (((532634.5 18...\n3 MULTIPOLYGON (((532135.1 18...\n4 MULTIPOLYGON (((533808 1807...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect borough dataset\nhead(borough)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 507007.4 ymin: 155850.8 xmax: 561957.5 ymax: 194889.3\nProjected CRS: OSGB36 / British National Grid\n  objectid                 name  gss_code  hectares nonld_area ons_inner\n1        1 Kingston upon Thames E09000021  3726.117      0.000         F\n2        2              Croydon E09000008  8649.441      0.000         F\n3        3              Bromley E09000006 15013.487      0.000         F\n4        4             Hounslow E09000018  5658.541     60.755         F\n5        5               Ealing E09000009  5554.428      0.000         F\n6        6             Havering E09000016 11445.735    210.763         F\n  sub_2011                           geom\n1    South POLYGON ((516401.6 160201.8...\n2    South POLYGON ((535009.2 159504.7...\n3    South POLYGON ((540373.6 157530.4...\n4     West POLYGON ((509703.4 175356.6...\n5     West POLYGON ((515647.2 178787.8...\n6     East POLYGON ((553564 179127.1, ...\n\n\n\n\nThe inspection shows that the POI dataset contains a wide variety of location types, with each point tagged under a main and alternative category, as provided by the Overture Maps Foundation via Meta and Microsoft. However, these tags may not be consistent across the dataset, so we will need to identify specific keywords to filter the main_category and alternate_category columns.\nWe will start by filtering out all POIs where the word school features in the main_category column:\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"school\"))\n\n# inspect\nhead(unique(poi_schools$main_category), n = 50)\n\n\n [1] \"\\\"day_care_preschool\\\"\"              \"\\\"driving_school\\\"\"                 \n [3] \"\\\"elementary_school\\\"\"               \"\\\"school\\\"\"                         \n [5] \"\\\"language_school\\\"\"                 \"\\\"music_school\\\"\"                   \n [7] \"\\\"specialty_school\\\"\"                \"\\\"preschool\\\"\"                      \n [9] \"\\\"dance_school\\\"\"                    \"\\\"high_school\\\"\"                    \n[11] \"\\\"drama_school\\\"\"                    \"\\\"cooking_school\\\"\"                 \n[13] \"\\\"middle_school\\\"\"                   \"\\\"vocational_and_technical_school\\\"\"\n[15] \"\\\"art_school\\\"\"                      \"\\\"private_school\\\"\"                 \n[17] \"\\\"religious_school\\\"\"                \"\\\"nursing_school\\\"\"                 \n[19] \"\\\"montessori_school\\\"\"               \"\\\"public_school\\\"\"                  \n[21] \"\\\"cosmetology_school\\\"\"              \"\\\"medical_school\\\"\"                 \n[23] \"\\\"engineering_schools\\\"\"             \"\\\"massage_school\\\"\"                 \n[25] \"\\\"business_schools\\\"\"                \"\\\"law_schools\\\"\"                    \n[27] \"\\\"medical_sciences_schools\\\"\"        \"\\\"sports_school\\\"\"                  \n[29] \"\\\"flight_school\\\"\"                  \n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nThis is still a very large list, and looking at the categories not all POIs containing the string school should be included. However, this initial selection has given us a more manageable list from which we can choose the relevant tags. We can now further filter the dataset as well as clip the dataset to the administrative boundaries of Lambeth.\n\n\n\nR code\n\n# remove quotes for easier processing\npoi_schools &lt;- poi_schools |&gt;\n    mutate(main_category = str_replace_all(main_category, \"\\\"\", \"\"))\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(main_category == \"elementary_school\" | main_category == \"high_school\" |\n        main_category == \"middle_school\" | main_category == \"private_school\" | main_category ==\n        \"public_school\" | main_category == \"school\")\n\n# filter school poi data\nlambeth &lt;- borough |&gt;\n    filter(name == \"Lambeth\")\n\npoi_schools &lt;- poi_schools |&gt;\n    st_intersection(lambeth) |&gt;\n    select(1:11)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 141 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 169846.4 xmax: 533065.9 ymax: 180398\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                 id\n6  08f194ad1a394235035f3ab7c2e4721d\n7  08f194ad1a8da734035945d69c357ddd\n8  08f194ad1abb648603defd9d76b4c314\n27 08f194ad130f0cd303c1c9f9b42438f8\n                                               primary_name     main_category\n6                         \"Woodmansterne Children's Centre\" elementary_school\n7   \"Immanuel & St Andrew Church of England Primary School\"            school\n8  \"Monkey Puzzle Day Nursery & Preschool Streatham Common\"            school\n27                                     \"Campsbourne School\"            school\n                        alternate_category                   address locality\n6                         school|education            \"Stockport Rd\"     &lt;NA&gt;\n7              elementary_school|education           \"Northanger Rd\"     &lt;NA&gt;\n8  education|public_service_and_government \"496 Streatham High Road\" \"London\"\n27                               education                      &lt;NA&gt; \"London\"\n     postcode region country source   source_record_id\n6  \"SW16 5XE\"   &lt;NA&gt;    \"GB\" \"meta\"  \"114577088601307\"\n7  \"SW16 5SL\"   &lt;NA&gt;    \"GB\" \"meta\"  \"128479257200832\"\n8  \"SW16 3QB\"  \"ENG\"    \"GB\" \"meta\" \"1092187950854118\"\n27       &lt;NA&gt;   &lt;NA&gt;    \"GB\" \"meta\"  \"114411542481619\"\n                        geom\n6  POINT (529701.5 169846.4)\n7  POINT (530016.4 170574.1)\n8  POINT (530208.6 170587.9)\n27 POINT (528819.8 174228.7)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nThis is still a rather long list and likely inaccurate. According to Lambeth Council Education Statistics, there should be 80 primary and secondary schools across the borough. We can use the alternate_category column to further narrow down our results.\n\n\n\n\n\n\nYou can inspect the different tags and their frequencies easily by creating a frequency table: table(poi_schools$alternate_category).\n\n\n\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(str_detect(alternate_category, \"elementary_school\") | str_detect(alternate_category,\n        \"high_school\") | str_detect(alternate_category, \"middle_school\") | str_detect(alternate_category,\n        \"private_school\") | str_detect(alternate_category, \"public_school\"))\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 58 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id\n1 08f194ad1a8da734035945d69c357ddd\n2 08f194ad1a70460d037da737c256001b\n3 08f194ad1c2dc81c032e9e0aa296a8d1\n4 08f194ad1e4cec5903fafb7496a2d2f3\n                                             primary_name     main_category\n1 \"Immanuel & St Andrew Church of England Primary School\"            school\n2                                \"Granton Primary School\" elementary_school\n3                 \"Kingswood Primary School (Upper Site)\" elementary_school\n4                              \"Battersea Grammar School\"            school\n           alternate_category          address locality   postcode region\n1 elementary_school|education  \"Northanger Rd\"     &lt;NA&gt; \"SW16 5SL\"   &lt;NA&gt;\n2        school|public_school     \"Granton Rd\"     &lt;NA&gt; \"SW16 5AN\"   &lt;NA&gt;\n3          school|high_school \"193 Gipsy Road\" \"London\"   \"SE27 9\"  \"ENG\"\n4       high_school|education             &lt;NA&gt; \"London\"       &lt;NA&gt;   &lt;NA&gt;\n  country source  source_record_id                      geom\n1    \"GB\" \"meta\" \"128479257200832\" POINT (530016.4 170574.1)\n2    \"GB\" \"meta\" \"235737420093504\" POINT (529299.7 170025.3)\n3    \"GB\" \"meta\" \"110066125723254\" POINT (532897.2 171498.4)\n4    \"GB\" \"meta\" \"103107239728950\" POINT (529523.9 172310.9)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nSince the POI dataset is compiled from various open sources, the data quality is not guaranteed. Some schools may be missing, while others could be duplicated, perhaps under slightly different names or because different buildings have been assigned separate point locations. However, it is unlikely that more than one school would share the same postcode. Therefore, we will use postcode information (where available) to finalise our school selection and remove any likely duplicates.\n\n\n\nR code\n\n# identify duplicate postcodes\npoi_schools &lt;- poi_schools |&gt;\n    group_by(postcode) |&gt;\n    mutate(rank = rank(primary_name)) |&gt;\n    ungroup()\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(is.na(postcode) | rank == 1) |&gt;\n    select(-rank)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 54 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 54 × 12\n   id    primary_name main_category alternate_category address locality postcode\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1 08f1… \"\\\"Immanuel… school        elementary_school… \"\\\"Nor…  &lt;NA&gt;    \"\\\"SW16…\n 2 08f1… \"\\\"Granton … elementary_s… school|public_sch… \"\\\"Gra…  &lt;NA&gt;    \"\\\"SW16…\n 3 08f1… \"\\\"Kingswoo… elementary_s… school|high_school \"\\\"193… \"\\\"Lond… \"\\\"SE27…\n 4 08f1… \"\\\"Batterse… school        high_school|educa…  &lt;NA&gt;   \"\\\"Lond…  &lt;NA&gt;   \n 5 08f1… \"\\\"St Bede'… school        elementary_school… \"\\\"St …  &lt;NA&gt;    \"\\\"SW12…\n 6 08f1… \"\\\"St Leona… school        elementary_school… \"\\\"42 … \"\\\"Lond… \"\\\"SW16…\n 7 08f1… \"\\\"Richard … elementary_s… college_universit… \"\\\"New…  &lt;NA&gt;    \"\\\"SW2 …\n 8 08f1… \"\\\"Henry Ca… school        high_school|eleme… \"\\\"Hyd…  &lt;NA&gt;    \"\\\"SW12…\n 9 08f1… \"\\\"South Ba… school        high_school|b2b_s… \"\\\"56 … \"\\\"Lond… \"\\\"SW2 …\n10 08f1… \"\\\"Glenbroo… elementary_s… school|public_sch… \"\\\"Cla…  &lt;NA&gt;    \"\\\"SW4 …\n# ℹ 44 more rows\n# ℹ 5 more variables: region &lt;chr&gt;, country &lt;chr&gt;, source &lt;chr&gt;,\n#   source_record_id &lt;chr&gt;, geom &lt;POINT [m]&gt;\n\n\nAlthough we now have fewer schools than we had expected, either due to overly restrictive filtering of tags or because some school locations are not recorded in the dataset, we will proceed with the current data.\n\n\n\n\n\n\nVariable preparation can be a time-consuming process that often necessitates a more extensive exploratory analysis to ensure sufficient data quality. This may involve sourcing additional data to supplement your existing dataset.\n\n\n\nWe can use a similar approach to approximate the locations of fast food outlets in the Borough.\n\n\n\nR code\n\n# filter fast food poi data\npoi_fastfood &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"fast_food_restaurant\") | str_detect(alternate_category,\n        \"fast_food_restaurant\") | str_detect(alternate_category, \"chicken_restaurant\") |\n        str_detect(alternate_category, \"burger_restaurant\"))\n\n# inspect\npoi_fastfood\n\n\nSimple feature collection with 1444 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526666.3 ymin: 168272.9 xmax: 535546.9 ymax: 182554\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id     primary_name          main_category\n1 08f194ada9716b86030eab41bbd4207e  \"Gorgeous Grub\"    \"burger_restaurant\"\n2 08f194ada9449a8a0345a466a0a6ece9        \"Lidl GB\"          \"supermarket\"\n3 08f194ada944daa80328c6604dab3503     \"Moss Bros.\" \"men's_clothing_store\"\n4 08f194ada932ad8603db11bbb7f953a7 \"Livi's Cuisine\"   \"african_restaurant\"\n                  alternate_category                          address  locality\n1 eat_and_drink|fast_food_restaurant          \"1 Prince Georges Road\"  \"London\"\n2        retail|fast_food_restaurant                  \"Colliers Wood\"  \"London\"\n3               fast_food_restaurant \"Unit 5, Tandem Shopping Centre\"  \"London\"\n4       caterer|fast_food_restaurant                   \"1 Locks Lane\" \"Mitcham\"\n    postcode region country source  source_record_id\n1   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\" \"232538816864698\"\n2 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"111430837210163\"\n3 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"478090646011341\"\n4    \"CR4 2\"  \"ENG\"    \"GB\" \"meta\" \"231745500530140\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526922.2 16988...\n3 MULTIPOINT ((526945.5 16992...\n4 MULTIPOINT ((527970.3 16955...\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nLet’s map both datasets to get an idea of how the data look like:\n\n\n\nR code\n\n# combine for mapping\npoi_schools &lt;- poi_schools |&gt;\n  mutate(type = \"School\")\npoi_fastfood &lt;- poi_fastfood |&gt;\n  mutate(type = \"Fast food\")\npoi_lambeth &lt;- rbind(poi_schools, poi_fastfood)\n\n# shape, polygon\ntm_shape(lambeth) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(poi_lambeth) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"type\",\n    size = 0.05,\n    palette = c(\"#beaed4\", \"#fdc086\"),\n    title = \"\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Extracted school and fast food locations for Lambeth.\n\n\n\n\n\n\n\nIn addition to the locations of interest, we need network data to assess the accessibility of schools in relation to fast food outlets. We will use OpenStreetMap to extract road segment data. Similar to the POI dataset, OSM uses key and value tags to categorise the features within its dataset.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world, but its coverage is uneven globally. However, the accuracy and quality of the data can at times be questionable, with details such as road types and speed limits missing. The OpenStreetMap Wiki provides more details on the tagging system.\n\n\n\nTo download the Lambeth road network dataset, we first need to define our bounding box coordinates. We will then use these coordinates in our OSM query to extract specific types of road segments within the defined search area. Our focus will be on selecting all OSM features with the highway tag that are likely to be used by pedestrians (e.g. excluding motorways).\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_lambeth &lt;- poi24 |&gt;\n    st_transform(4326) |&gt;\n    st_bbox()\n\n# osm query\nosm_network &lt;- opq(bbox = bbox_lambeth) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download].\nYou can load this copy into R through load('data/London-OSM-Roads.RData')\n\n\n\nThe returned osm_network object contains a variety of elements with the specified tags. Our next step is to extract the spatial data from this object to create our road network dataset. Specifically, we will extract the edges of the network, which represent the lines of the roads, as well as the nodes, which represent the points where the roads start, end, or intersect.\n\n\n\nR code\n\n# extract the nodes, with their osm_id\nosm_network_nodes &lt;- osm_network$osm_points[, \"osm_id\"]\n\n# extract the edges, with their osm_id and relevant columns\nosm_network_edges &lt;- osm_network$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n# inspect\nhead(osm_network_nodes)\n\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1541499 ymin: 51.52434 xmax: -0.1457924 ymax: 51.52698\nGeodetic CRS:  WGS 84\n      osm_id                    geometry\n78112  78112 POINT (-0.1457924 51.52698)\n99878  99878 POINT (-0.1529787 51.52434)\n99879  99879 POINT (-0.1532934 51.52482)\n99880  99880 POINT (-0.1535802 51.52508)\n99882  99882 POINT (-0.1541499 51.52567)\n99883  99883 POINT (-0.1541362 51.52598)\n\n# inspect\nhead(osm_network_edges)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -0.1398347 ymin: 51.50608 xmax: -0.0821093 ymax: 51.5246\nGeodetic CRS:  WGS 84\n         osm_id                 name     highway maxspeed oneway\n31030     31030          Grafton Way     primary   20 mph    yes\n31039     31039 Tottenham Court Road     primary   20 mph   &lt;NA&gt;\n31959     31959     Cleveland Street residential   20 mph    yes\n554369   554369  King William Street    tertiary   20 mph    yes\n554526   554526     Fenchurch Street    tertiary   20 mph   &lt;NA&gt;\n1530592 1530592  Borough High Street     primary   30 mph    yes\n                              geometry\n31030   LINESTRING (-0.1349153 51.5...\n31039   LINESTRING (-0.1303693 51.5...\n31959   LINESTRING (-0.139512 51.52...\n554369  LINESTRING (-0.08745 51.511...\n554526  LINESTRING (-0.085135 51.51...\n1530592 LINESTRING (-0.0882957 51.5...\n\n\nWe can quickly map the network edges to see how the road network looks like:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(osm_network_edges) +\n\n  # specify column, classes\n  tm_lines(\n    col = \"#bdbdbd\",\n    lwd = 0.2,\n  ) +\n\n  # shape, polygon\n  tm_shape(lambeth) +\n\n  # specify column, classes\n  tm_borders(\n    col = \"#252525\",\n    lwd = 2\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Road segments\",\n    col = \"#bdbdbd\"\n  ) +\n\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Outline Lambeth\",\n    col = \"#252525\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1\n  )\n\n\n\n\n\nFigure 2: Extracted OpenStreetMap road network data for Lambeth.\n\n\n\n\n\n\n\nSince our focus is on schoolchildren and walking distances, we will overwrite the oneway variable to assume that none of the road segments are restricted to one-way traffic. This adjustment will ensure our analysis is not skewed by such restrictions and will help maintain a more accurate representation of the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nosm_network_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics with our prepared point of interest data.\nIn any network analysis, the primary data structure is a graph composed of nodes and edges. The dodgr library utilises weighting profiles to assign weights based on road types, tailored to the mode of transport that each profile is designed to model. In this instance, we will use the foot weighting profile, as our focus is on modelling walking accessibility. To prevent errors related to the weighting profile, we will replace any NA values in the highway tag with the value unclassified.\n\n\n\nR code\n\n# replace missing highway tags with unclassified\nosm_network_edges &lt;- osm_network_edges |&gt;\n    mutate(highway = if_else(is.na(highway), \"unclassified\", highway))\n\n# create network graph\nosm_network_graph &lt;- weight_streetnet(osm_network_edges, wt_profile = \"foot\")\n\n\nOnce we have constructed our graph, we can use it to calculate network distances between our points of interest. One important consideration is that not all individual components in the extracted network may be connected. This can occur, for example, if the bounding box cuts off access to the road of a cul-de-sac. To ensure that our entire extracted network is connected, we will therefore extract the largest connected component of the graph.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always representing the largest component. It is essential to inspect the resulting subgraph to ensure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\nnetx_connected &lt;- osm_network_graph[osm_network_graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(netx_connected)\n\n\n[1] 417750\n\n\n\n\n\n\n\n\nOpenStreetMap is a dynamic dataset, meaning that changes are made on a continuous basis. As a result, it is quite possible that the number of remaining road segments, as shown above, may differ slightly when you run this analysis.\n\n\n\n\n\n\nNow that we have our connected subgraph, we can use the dodgr_distances() function to calculate the network distances between every possible origin (i.e. school) and destination (i.e. fast food outlet). For all combinations, the function will map the point of interest locations to the nearest point on the network and return the corresponding shortest-path distances.\n\n\n\n\n\n\nThe dodgr package requires data to be projected in WGS84, so we need to reproject our point of interest data accordingly.\n\n\n\n\n\n\nR code\n\n# reproject\npoi_schools &lt;- poi_schools |&gt;\n    st_transform(4326)\npoi_fastfood &lt;- poi_fastfood |&gt;\n    st_transform(4326)\n\n# distance matrix\ndistance_matrix &lt;- dodgr_distances(netx_connected, from = st_coordinates(poi_schools),\n    to = st_coordinates(poi_fastfood), shortest = FALSE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance matrix that contains the network distances between all origins (i.e. schools) and all destinations (i.e. fast-food outlets):\n\n\n\nR code\n\n# inspect\ndistance_matrix[1:5, 1:5]\n\n\n            6807494201 7110321980 7110321980 11371586827 33148215\n8796433764    4660.831   4661.009   4661.009    3127.402 3085.486\n8820889464    3620.812   3762.437   3762.437    1962.068 1920.151\n11479633279   8497.581   8497.760   8497.760    6938.918 6897.002\n292521291     4917.554   4917.732   4917.732    4222.538 4287.953\n6625498885    6279.569   6279.747   6279.747    5584.552 5649.968\n\n\n\n\n\n\n\n\nThe above output displays the distance (in metres) between the first five schools and the first five fast-food outlets. The row and column IDs refer to the nearest nodes on the OSM network to which the schools and fast-food outlets were mapped.\n\n\n\nNow that we have the distance matrix, we can aggregate the data and perform accessibility analysis. For example, we can count the number of fast-food outlets within 500 or 1,000 metres walking distance from each school:\n\n\n\nR code\n\n# fast-food outlets within 500m\npoi_schools$fastfood_500m &lt;- rowSums(distance_matrix &lt;= 500)\n\n# fast-food outlets within 1000m\npoi_schools$fastfood_1000m &lt;- rowSums(distance_matrix &lt;= 1000)\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nIn the final step, we can investigate whether there is a relationship between the proximity of fast-food outlets and the relative levels of deprivation in the area. One approach is to calculate the average number of fast-food outlets within 1,000 metres of a school for each LSOA, and then compare these figures to their corresponding IMD deciles.\n\n\n\nR code\n\n# read imd dataset\nimd19 &lt;- read_csv(\"data/attributes/England-IMD-2019.csv\")\n\n\nRows: 32844 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lsoa11cd\ndbl (2): imd_rank, imd_dec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# join imd\nlsoa11 &lt;- lsoa11 |&gt;\n  left_join(imd19, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n# join schools to their parent lsoa\npoi_schools &lt;- poi_schools |&gt;\n  st_transform(27700) |&gt;\n  st_join(lsoa11)\n\nWe can use this approach to derive the average number of fast-food by IMD decile:\n\n\n\nR code\n\n# average counts by imd decile\nfastfood_imd &lt;- poi_schools |&gt;\n    group_by(imd_dec) |&gt;\n    mutate(avg_cnt = mean(fastfood_1000m)) |&gt;\n    distinct(imd_dec, avg_cnt) |&gt;\n    arrange(imd_dec)\n\n# inspect\nfastfood_imd\n\n\n# A tibble: 7 × 2\n# Groups:   imd_dec [7]\n  imd_dec avg_cnt\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       2   20.1 \n2       3   14.3 \n3       4   17.5 \n4       5    9.83\n5       6    3   \n6       7    8.2 \n7       8   23.5 \n\n\nThere appears to be a weak relationship, with schools in more deprived areas having, on average, a higher number of fast-food outlets within a 1,000-metre walking distance. However, this trend is not consistent, as schools in the least deprived areas of Lambeth show the highest accessibility on average.\n\n\n\n\nAccessibility analysis involves evaluating how easily people can reach essential services, destinations, or opportunities, such as schools, healthcare facilities, or workplaces, from a given location. It considers factors like distance, travel time, and transport networks, providing valuable insights for urban planning, transport policies, and social equity. The CDRC Access to Healthy Assets & Hazards (AHAH) dataset, for instance, uses accessibility analysis to quantify how easy it is to reach ‘unhealthy’ places, such as pubs and gambling outlets, for each neighbourhood in Great Britain.\nHaving run through all the steps during the tutorial, we can recreate this analysis ourselves. Using Lambeth as a case study, try to complete the following tasks:\n\nExtract all pubs from the Point of Interest dataset.\nFor each LSOA within Lambeth, calculate the average walking distance to the nearest pub.\nCreate a map of the results.\n\n\n\n\n\n\n\nUnlike before, LSOAs are now the unit of analysis. This means you will need to input the LSOA centroids into your distance matrix.\n\n\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R.\n\n\n\n\n\n\nThis brings us to the end of the tutorial. You should now have a basic understanding of the concepts behind accessibility analysis, how it can be executed in R, and some of the challenges you may encounter when conducting your own research. With this being said, you have now reached the end of this week’s content. Onwards and upwards!"
  },
  {
    "objectID": "08-network.html#lecture-w08",
    "href": "08-network.html#lecture-w08",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "08-network.html#reading-w08",
    "href": "08-network.html#reading-w08",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Geurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\n\n\n\n\n\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]"
  },
  {
    "objectID": "08-network.html#accessibility-in-lambeth",
    "href": "08-network.html#accessibility-in-lambeth",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "This week, we will analyse the accessibility of fast-food outlets in the London Borough of Lambeth. Specifically, we will examine how closely these outlets are located within walking distance of primary and secondary schools, and explore any potential relationships between their proximity and the relative levels of deprivation in the area.\nWe will extract the points of interest that we will use for this analysis from the Point of Interest (POI) data for the United Kingdom, obtained from the Overture Maps Foundation and pre-processed by the Consumer Data Research Centre to provide users with easy access.\nYou can download a subset of the POI dataset via the link provided below. A copy of the 2011 London LSOAs spatial boundaries, the boundaries of the London Boroughs, and the 2019 English Index of Multiple Deprivation. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLambeth Overture Points of Interest 2024\nGeoPackage\nDownload\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\nEngland 2019 Index of Multiple Deprivation\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo extract the Lambeth Overture Points of Interest data, a 2-kilometre buffer was applied around the boundaries of Lambeth. This approach ensures that points just outside the study area are included, as locations beyond the borough boundary may still be accessible to residents and could represent the nearest available options.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w08-accessibility-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the spatial data into R.\n\n\n\nR code\n\n# read poi data\npoi24 &lt;- st_read(\"data/spatial/Lambeth-POI-2024.gpkg\")\n\n\nReading layer `Lambeth-POI-2024' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/Lambeth-POI-2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 65060 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526556.6 ymin: 167827 xmax: 535640.4 ymax: 182673.8\nProjected CRS: OSGB36 / British National Grid\n\n# read lsoa dataset\nlsoa11 &lt;- st_read(\"data/spatial/London-LSOA-2011.gpkg\")\n\nReading layer `London-LSOA-2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4835 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# read borough dataset\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\")\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect poi data\nhead(poi24)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526913.4 ymin: 169695.2 xmax: 526945.5 ymax: 169970.8\nProjected CRS: OSGB36 / British National Grid\n                                id    primary_name       main_category\n1 08f194ada9716b86030eab41bbd4207e \"Gorgeous Grub\" \"burger_restaurant\"\n2 08f194ada9715a1903d73f4aef170602    \"TLC Direct\"   \"wholesale_store\"\n3 08f194ada944cba203fa613de4f5e6d5     \"JD Sports\"       \"sports_wear\"\n4 08f194ada9449a8a0345a466a0a6ece9       \"Lidl GB\"       \"supermarket\"\n                    alternate_category                                 address\n1   eat_and_drink|fast_food_restaurant                 \"1 Prince Georges Road\"\n2 professional_services|lighting_store                      \"280 Western Road\"\n3            sporting_goods|shoe_store \"Unit 2 Tandem Centre Top Of Church Rd\"\n4          retail|fast_food_restaurant                         \"Colliers Wood\"\n         locality   postcode region country source   source_record_id\n1        \"London\"   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\"  \"232538816864698\"\n2        \"London\" \"SW19 2QA\"  \"ENG\"    \"GB\" \"meta\" \"1959707454355017\"\n3 \"Colliers Wood\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"644899945690935\"\n4        \"London\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"111430837210163\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526921.1 16969...\n3 MULTIPOINT ((526915.7 16997...\n4 MULTIPOINT ((526922.2 16988...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect lsoa dataset\nhead(lsoa11)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa11cd            lsoa11nm           lsoa11nmw  bng_e  bng_n     long\n1 E01000001 City of London 001A City of London 001A 532129 181625 -0.09706\n2 E01000002 City of London 001B City of London 001B 532480 181699 -0.09197\n3 E01000003 City of London 001C City of London 001C 532245 182036 -0.09523\n4 E01000005 City of London 001E City of London 001E 533581 181265 -0.07628\n       lat                               globalid          lsoa11_name pop2011\n1 51.51810 {283B0EAD-F8FC-40B6-9A79-1DDD7E5C0758}  City of London 001A    1465\n2 51.51868 {DDCE266B-7825-428C-9E0A-DF66B0179A55}  City of London 001B    1436\n3 51.52176 {C45E358E-A794-485A-BF76-D96E5D458EA4}  City of London 001C    1346\n4 51.51452 {4DDAF5E4-E47F-4312-89A0-923FFEC028A6}  City of London 001E     985\n                            geom\n1 MULTIPOLYGON (((532105.1 18...\n2 MULTIPOLYGON (((532634.5 18...\n3 MULTIPOLYGON (((532135.1 18...\n4 MULTIPOLYGON (((533808 1807...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect borough dataset\nhead(borough)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 507007.4 ymin: 155850.8 xmax: 561957.5 ymax: 194889.3\nProjected CRS: OSGB36 / British National Grid\n  objectid                 name  gss_code  hectares nonld_area ons_inner\n1        1 Kingston upon Thames E09000021  3726.117      0.000         F\n2        2              Croydon E09000008  8649.441      0.000         F\n3        3              Bromley E09000006 15013.487      0.000         F\n4        4             Hounslow E09000018  5658.541     60.755         F\n5        5               Ealing E09000009  5554.428      0.000         F\n6        6             Havering E09000016 11445.735    210.763         F\n  sub_2011                           geom\n1    South POLYGON ((516401.6 160201.8...\n2    South POLYGON ((535009.2 159504.7...\n3    South POLYGON ((540373.6 157530.4...\n4     West POLYGON ((509703.4 175356.6...\n5     West POLYGON ((515647.2 178787.8...\n6     East POLYGON ((553564 179127.1, ...\n\n\n\n\nThe inspection shows that the POI dataset contains a wide variety of location types, with each point tagged under a main and alternative category, as provided by the Overture Maps Foundation via Meta and Microsoft. However, these tags may not be consistent across the dataset, so we will need to identify specific keywords to filter the main_category and alternate_category columns.\nWe will start by filtering out all POIs where the word school features in the main_category column:\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"school\"))\n\n# inspect\nhead(unique(poi_schools$main_category), n = 50)\n\n\n [1] \"\\\"day_care_preschool\\\"\"              \"\\\"driving_school\\\"\"                 \n [3] \"\\\"elementary_school\\\"\"               \"\\\"school\\\"\"                         \n [5] \"\\\"language_school\\\"\"                 \"\\\"music_school\\\"\"                   \n [7] \"\\\"specialty_school\\\"\"                \"\\\"preschool\\\"\"                      \n [9] \"\\\"dance_school\\\"\"                    \"\\\"high_school\\\"\"                    \n[11] \"\\\"drama_school\\\"\"                    \"\\\"cooking_school\\\"\"                 \n[13] \"\\\"middle_school\\\"\"                   \"\\\"vocational_and_technical_school\\\"\"\n[15] \"\\\"art_school\\\"\"                      \"\\\"private_school\\\"\"                 \n[17] \"\\\"religious_school\\\"\"                \"\\\"nursing_school\\\"\"                 \n[19] \"\\\"montessori_school\\\"\"               \"\\\"public_school\\\"\"                  \n[21] \"\\\"cosmetology_school\\\"\"              \"\\\"medical_school\\\"\"                 \n[23] \"\\\"engineering_schools\\\"\"             \"\\\"massage_school\\\"\"                 \n[25] \"\\\"business_schools\\\"\"                \"\\\"law_schools\\\"\"                    \n[27] \"\\\"medical_sciences_schools\\\"\"        \"\\\"sports_school\\\"\"                  \n[29] \"\\\"flight_school\\\"\"                  \n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nThis is still a very large list, and looking at the categories not all POIs containing the string school should be included. However, this initial selection has given us a more manageable list from which we can choose the relevant tags. We can now further filter the dataset as well as clip the dataset to the administrative boundaries of Lambeth.\n\n\n\nR code\n\n# remove quotes for easier processing\npoi_schools &lt;- poi_schools |&gt;\n    mutate(main_category = str_replace_all(main_category, \"\\\"\", \"\"))\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(main_category == \"elementary_school\" | main_category == \"high_school\" |\n        main_category == \"middle_school\" | main_category == \"private_school\" | main_category ==\n        \"public_school\" | main_category == \"school\")\n\n# filter school poi data\nlambeth &lt;- borough |&gt;\n    filter(name == \"Lambeth\")\n\npoi_schools &lt;- poi_schools |&gt;\n    st_intersection(lambeth) |&gt;\n    select(1:11)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 141 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 169846.4 xmax: 533065.9 ymax: 180398\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                 id\n6  08f194ad1a394235035f3ab7c2e4721d\n7  08f194ad1a8da734035945d69c357ddd\n8  08f194ad1abb648603defd9d76b4c314\n27 08f194ad130f0cd303c1c9f9b42438f8\n                                               primary_name     main_category\n6                         \"Woodmansterne Children's Centre\" elementary_school\n7   \"Immanuel & St Andrew Church of England Primary School\"            school\n8  \"Monkey Puzzle Day Nursery & Preschool Streatham Common\"            school\n27                                     \"Campsbourne School\"            school\n                        alternate_category                   address locality\n6                         school|education            \"Stockport Rd\"     &lt;NA&gt;\n7              elementary_school|education           \"Northanger Rd\"     &lt;NA&gt;\n8  education|public_service_and_government \"496 Streatham High Road\" \"London\"\n27                               education                      &lt;NA&gt; \"London\"\n     postcode region country source   source_record_id\n6  \"SW16 5XE\"   &lt;NA&gt;    \"GB\" \"meta\"  \"114577088601307\"\n7  \"SW16 5SL\"   &lt;NA&gt;    \"GB\" \"meta\"  \"128479257200832\"\n8  \"SW16 3QB\"  \"ENG\"    \"GB\" \"meta\" \"1092187950854118\"\n27       &lt;NA&gt;   &lt;NA&gt;    \"GB\" \"meta\"  \"114411542481619\"\n                        geom\n6  POINT (529701.5 169846.4)\n7  POINT (530016.4 170574.1)\n8  POINT (530208.6 170587.9)\n27 POINT (528819.8 174228.7)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nThis is still a rather long list and likely inaccurate. According to Lambeth Council Education Statistics, there should be 80 primary and secondary schools across the borough. We can use the alternate_category column to further narrow down our results.\n\n\n\n\n\n\nYou can inspect the different tags and their frequencies easily by creating a frequency table: table(poi_schools$alternate_category).\n\n\n\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(str_detect(alternate_category, \"elementary_school\") | str_detect(alternate_category,\n        \"high_school\") | str_detect(alternate_category, \"middle_school\") | str_detect(alternate_category,\n        \"private_school\") | str_detect(alternate_category, \"public_school\"))\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 58 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id\n1 08f194ad1a8da734035945d69c357ddd\n2 08f194ad1a70460d037da737c256001b\n3 08f194ad1c2dc81c032e9e0aa296a8d1\n4 08f194ad1e4cec5903fafb7496a2d2f3\n                                             primary_name     main_category\n1 \"Immanuel & St Andrew Church of England Primary School\"            school\n2                                \"Granton Primary School\" elementary_school\n3                 \"Kingswood Primary School (Upper Site)\" elementary_school\n4                              \"Battersea Grammar School\"            school\n           alternate_category          address locality   postcode region\n1 elementary_school|education  \"Northanger Rd\"     &lt;NA&gt; \"SW16 5SL\"   &lt;NA&gt;\n2        school|public_school     \"Granton Rd\"     &lt;NA&gt; \"SW16 5AN\"   &lt;NA&gt;\n3          school|high_school \"193 Gipsy Road\" \"London\"   \"SE27 9\"  \"ENG\"\n4       high_school|education             &lt;NA&gt; \"London\"       &lt;NA&gt;   &lt;NA&gt;\n  country source  source_record_id                      geom\n1    \"GB\" \"meta\" \"128479257200832\" POINT (530016.4 170574.1)\n2    \"GB\" \"meta\" \"235737420093504\" POINT (529299.7 170025.3)\n3    \"GB\" \"meta\" \"110066125723254\" POINT (532897.2 171498.4)\n4    \"GB\" \"meta\" \"103107239728950\" POINT (529523.9 172310.9)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nSince the POI dataset is compiled from various open sources, the data quality is not guaranteed. Some schools may be missing, while others could be duplicated, perhaps under slightly different names or because different buildings have been assigned separate point locations. However, it is unlikely that more than one school would share the same postcode. Therefore, we will use postcode information (where available) to finalise our school selection and remove any likely duplicates.\n\n\n\nR code\n\n# identify duplicate postcodes\npoi_schools &lt;- poi_schools |&gt;\n    group_by(postcode) |&gt;\n    mutate(rank = rank(primary_name)) |&gt;\n    ungroup()\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(is.na(postcode) | rank == 1) |&gt;\n    select(-rank)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 54 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 54 × 12\n   id    primary_name main_category alternate_category address locality postcode\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1 08f1… \"\\\"Immanuel… school        elementary_school… \"\\\"Nor…  &lt;NA&gt;    \"\\\"SW16…\n 2 08f1… \"\\\"Granton … elementary_s… school|public_sch… \"\\\"Gra…  &lt;NA&gt;    \"\\\"SW16…\n 3 08f1… \"\\\"Kingswoo… elementary_s… school|high_school \"\\\"193… \"\\\"Lond… \"\\\"SE27…\n 4 08f1… \"\\\"Batterse… school        high_school|educa…  &lt;NA&gt;   \"\\\"Lond…  &lt;NA&gt;   \n 5 08f1… \"\\\"St Bede'… school        elementary_school… \"\\\"St …  &lt;NA&gt;    \"\\\"SW12…\n 6 08f1… \"\\\"St Leona… school        elementary_school… \"\\\"42 … \"\\\"Lond… \"\\\"SW16…\n 7 08f1… \"\\\"Richard … elementary_s… college_universit… \"\\\"New…  &lt;NA&gt;    \"\\\"SW2 …\n 8 08f1… \"\\\"Henry Ca… school        high_school|eleme… \"\\\"Hyd…  &lt;NA&gt;    \"\\\"SW12…\n 9 08f1… \"\\\"South Ba… school        high_school|b2b_s… \"\\\"56 … \"\\\"Lond… \"\\\"SW2 …\n10 08f1… \"\\\"Glenbroo… elementary_s… school|public_sch… \"\\\"Cla…  &lt;NA&gt;    \"\\\"SW4 …\n# ℹ 44 more rows\n# ℹ 5 more variables: region &lt;chr&gt;, country &lt;chr&gt;, source &lt;chr&gt;,\n#   source_record_id &lt;chr&gt;, geom &lt;POINT [m]&gt;\n\n\nAlthough we now have fewer schools than we had expected, either due to overly restrictive filtering of tags or because some school locations are not recorded in the dataset, we will proceed with the current data.\n\n\n\n\n\n\nVariable preparation can be a time-consuming process that often necessitates a more extensive exploratory analysis to ensure sufficient data quality. This may involve sourcing additional data to supplement your existing dataset.\n\n\n\nWe can use a similar approach to approximate the locations of fast food outlets in the Borough.\n\n\n\nR code\n\n# filter fast food poi data\npoi_fastfood &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"fast_food_restaurant\") | str_detect(alternate_category,\n        \"fast_food_restaurant\") | str_detect(alternate_category, \"chicken_restaurant\") |\n        str_detect(alternate_category, \"burger_restaurant\"))\n\n# inspect\npoi_fastfood\n\n\nSimple feature collection with 1444 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526666.3 ymin: 168272.9 xmax: 535546.9 ymax: 182554\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id     primary_name          main_category\n1 08f194ada9716b86030eab41bbd4207e  \"Gorgeous Grub\"    \"burger_restaurant\"\n2 08f194ada9449a8a0345a466a0a6ece9        \"Lidl GB\"          \"supermarket\"\n3 08f194ada944daa80328c6604dab3503     \"Moss Bros.\" \"men's_clothing_store\"\n4 08f194ada932ad8603db11bbb7f953a7 \"Livi's Cuisine\"   \"african_restaurant\"\n                  alternate_category                          address  locality\n1 eat_and_drink|fast_food_restaurant          \"1 Prince Georges Road\"  \"London\"\n2        retail|fast_food_restaurant                  \"Colliers Wood\"  \"London\"\n3               fast_food_restaurant \"Unit 5, Tandem Shopping Centre\"  \"London\"\n4       caterer|fast_food_restaurant                   \"1 Locks Lane\" \"Mitcham\"\n    postcode region country source  source_record_id\n1   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\" \"232538816864698\"\n2 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"111430837210163\"\n3 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"478090646011341\"\n4    \"CR4 2\"  \"ENG\"    \"GB\" \"meta\" \"231745500530140\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526922.2 16988...\n3 MULTIPOINT ((526945.5 16992...\n4 MULTIPOINT ((527970.3 16955...\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nLet’s map both datasets to get an idea of how the data look like:\n\n\n\nR code\n\n# combine for mapping\npoi_schools &lt;- poi_schools |&gt;\n  mutate(type = \"School\")\npoi_fastfood &lt;- poi_fastfood |&gt;\n  mutate(type = \"Fast food\")\npoi_lambeth &lt;- rbind(poi_schools, poi_fastfood)\n\n# shape, polygon\ntm_shape(lambeth) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(poi_lambeth) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"type\",\n    size = 0.05,\n    palette = c(\"#beaed4\", \"#fdc086\"),\n    title = \"\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Extracted school and fast food locations for Lambeth.\n\n\n\n\n\n\n\nIn addition to the locations of interest, we need network data to assess the accessibility of schools in relation to fast food outlets. We will use OpenStreetMap to extract road segment data. Similar to the POI dataset, OSM uses key and value tags to categorise the features within its dataset.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world, but its coverage is uneven globally. However, the accuracy and quality of the data can at times be questionable, with details such as road types and speed limits missing. The OpenStreetMap Wiki provides more details on the tagging system.\n\n\n\nTo download the Lambeth road network dataset, we first need to define our bounding box coordinates. We will then use these coordinates in our OSM query to extract specific types of road segments within the defined search area. Our focus will be on selecting all OSM features with the highway tag that are likely to be used by pedestrians (e.g. excluding motorways).\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_lambeth &lt;- poi24 |&gt;\n    st_transform(4326) |&gt;\n    st_bbox()\n\n# osm query\nosm_network &lt;- opq(bbox = bbox_lambeth) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download].\nYou can load this copy into R through load('data/London-OSM-Roads.RData')\n\n\n\nThe returned osm_network object contains a variety of elements with the specified tags. Our next step is to extract the spatial data from this object to create our road network dataset. Specifically, we will extract the edges of the network, which represent the lines of the roads, as well as the nodes, which represent the points where the roads start, end, or intersect.\n\n\n\nR code\n\n# extract the nodes, with their osm_id\nosm_network_nodes &lt;- osm_network$osm_points[, \"osm_id\"]\n\n# extract the edges, with their osm_id and relevant columns\nosm_network_edges &lt;- osm_network$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n# inspect\nhead(osm_network_nodes)\n\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1541499 ymin: 51.52434 xmax: -0.1457924 ymax: 51.52698\nGeodetic CRS:  WGS 84\n      osm_id                    geometry\n78112  78112 POINT (-0.1457924 51.52698)\n99878  99878 POINT (-0.1529787 51.52434)\n99879  99879 POINT (-0.1532934 51.52482)\n99880  99880 POINT (-0.1535802 51.52508)\n99882  99882 POINT (-0.1541499 51.52567)\n99883  99883 POINT (-0.1541362 51.52598)\n\n# inspect\nhead(osm_network_edges)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -0.1398347 ymin: 51.50608 xmax: -0.0821093 ymax: 51.5246\nGeodetic CRS:  WGS 84\n         osm_id                 name     highway maxspeed oneway\n31030     31030          Grafton Way     primary   20 mph    yes\n31039     31039 Tottenham Court Road     primary   20 mph   &lt;NA&gt;\n31959     31959     Cleveland Street residential   20 mph    yes\n554369   554369  King William Street    tertiary   20 mph    yes\n554526   554526     Fenchurch Street    tertiary   20 mph   &lt;NA&gt;\n1530592 1530592  Borough High Street     primary   30 mph    yes\n                              geometry\n31030   LINESTRING (-0.1349153 51.5...\n31039   LINESTRING (-0.1303693 51.5...\n31959   LINESTRING (-0.139512 51.52...\n554369  LINESTRING (-0.08745 51.511...\n554526  LINESTRING (-0.085135 51.51...\n1530592 LINESTRING (-0.0882957 51.5...\n\n\nWe can quickly map the network edges to see how the road network looks like:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(osm_network_edges) +\n\n  # specify column, classes\n  tm_lines(\n    col = \"#bdbdbd\",\n    lwd = 0.2,\n  ) +\n\n  # shape, polygon\n  tm_shape(lambeth) +\n\n  # specify column, classes\n  tm_borders(\n    col = \"#252525\",\n    lwd = 2\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Road segments\",\n    col = \"#bdbdbd\"\n  ) +\n\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Outline Lambeth\",\n    col = \"#252525\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1\n  )\n\n\n\n\n\nFigure 2: Extracted OpenStreetMap road network data for Lambeth.\n\n\n\n\n\n\n\nSince our focus is on schoolchildren and walking distances, we will overwrite the oneway variable to assume that none of the road segments are restricted to one-way traffic. This adjustment will ensure our analysis is not skewed by such restrictions and will help maintain a more accurate representation of the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nosm_network_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics with our prepared point of interest data.\nIn any network analysis, the primary data structure is a graph composed of nodes and edges. The dodgr library utilises weighting profiles to assign weights based on road types, tailored to the mode of transport that each profile is designed to model. In this instance, we will use the foot weighting profile, as our focus is on modelling walking accessibility. To prevent errors related to the weighting profile, we will replace any NA values in the highway tag with the value unclassified.\n\n\n\nR code\n\n# replace missing highway tags with unclassified\nosm_network_edges &lt;- osm_network_edges |&gt;\n    mutate(highway = if_else(is.na(highway), \"unclassified\", highway))\n\n# create network graph\nosm_network_graph &lt;- weight_streetnet(osm_network_edges, wt_profile = \"foot\")\n\n\nOnce we have constructed our graph, we can use it to calculate network distances between our points of interest. One important consideration is that not all individual components in the extracted network may be connected. This can occur, for example, if the bounding box cuts off access to the road of a cul-de-sac. To ensure that our entire extracted network is connected, we will therefore extract the largest connected component of the graph.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always representing the largest component. It is essential to inspect the resulting subgraph to ensure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\nnetx_connected &lt;- osm_network_graph[osm_network_graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(netx_connected)\n\n\n[1] 417750\n\n\n\n\n\n\n\n\nOpenStreetMap is a dynamic dataset, meaning that changes are made on a continuous basis. As a result, it is quite possible that the number of remaining road segments, as shown above, may differ slightly when you run this analysis.\n\n\n\n\n\n\nNow that we have our connected subgraph, we can use the dodgr_distances() function to calculate the network distances between every possible origin (i.e. school) and destination (i.e. fast food outlet). For all combinations, the function will map the point of interest locations to the nearest point on the network and return the corresponding shortest-path distances.\n\n\n\n\n\n\nThe dodgr package requires data to be projected in WGS84, so we need to reproject our point of interest data accordingly.\n\n\n\n\n\n\nR code\n\n# reproject\npoi_schools &lt;- poi_schools |&gt;\n    st_transform(4326)\npoi_fastfood &lt;- poi_fastfood |&gt;\n    st_transform(4326)\n\n# distance matrix\ndistance_matrix &lt;- dodgr_distances(netx_connected, from = st_coordinates(poi_schools),\n    to = st_coordinates(poi_fastfood), shortest = FALSE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance matrix that contains the network distances between all origins (i.e. schools) and all destinations (i.e. fast-food outlets):\n\n\n\nR code\n\n# inspect\ndistance_matrix[1:5, 1:5]\n\n\n            6807494201 7110321980 7110321980 11371586827 33148215\n8796433764    4660.831   4661.009   4661.009    3127.402 3085.486\n8820889464    3620.812   3762.437   3762.437    1962.068 1920.151\n11479633279   8497.581   8497.760   8497.760    6938.918 6897.002\n292521291     4917.554   4917.732   4917.732    4222.538 4287.953\n6625498885    6279.569   6279.747   6279.747    5584.552 5649.968\n\n\n\n\n\n\n\n\nThe above output displays the distance (in metres) between the first five schools and the first five fast-food outlets. The row and column IDs refer to the nearest nodes on the OSM network to which the schools and fast-food outlets were mapped.\n\n\n\nNow that we have the distance matrix, we can aggregate the data and perform accessibility analysis. For example, we can count the number of fast-food outlets within 500 or 1,000 metres walking distance from each school:\n\n\n\nR code\n\n# fast-food outlets within 500m\npoi_schools$fastfood_500m &lt;- rowSums(distance_matrix &lt;= 500)\n\n# fast-food outlets within 1000m\npoi_schools$fastfood_1000m &lt;- rowSums(distance_matrix &lt;= 1000)\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nIn the final step, we can investigate whether there is a relationship between the proximity of fast-food outlets and the relative levels of deprivation in the area. One approach is to calculate the average number of fast-food outlets within 1,000 metres of a school for each LSOA, and then compare these figures to their corresponding IMD deciles.\n\n\n\nR code\n\n# read imd dataset\nimd19 &lt;- read_csv(\"data/attributes/England-IMD-2019.csv\")\n\n\nRows: 32844 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lsoa11cd\ndbl (2): imd_rank, imd_dec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# join imd\nlsoa11 &lt;- lsoa11 |&gt;\n  left_join(imd19, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n# join schools to their parent lsoa\npoi_schools &lt;- poi_schools |&gt;\n  st_transform(27700) |&gt;\n  st_join(lsoa11)\n\nWe can use this approach to derive the average number of fast-food by IMD decile:\n\n\n\nR code\n\n# average counts by imd decile\nfastfood_imd &lt;- poi_schools |&gt;\n    group_by(imd_dec) |&gt;\n    mutate(avg_cnt = mean(fastfood_1000m)) |&gt;\n    distinct(imd_dec, avg_cnt) |&gt;\n    arrange(imd_dec)\n\n# inspect\nfastfood_imd\n\n\n# A tibble: 7 × 2\n# Groups:   imd_dec [7]\n  imd_dec avg_cnt\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       2   20.1 \n2       3   14.3 \n3       4   17.5 \n4       5    9.83\n5       6    3   \n6       7    8.2 \n7       8   23.5 \n\n\nThere appears to be a weak relationship, with schools in more deprived areas having, on average, a higher number of fast-food outlets within a 1,000-metre walking distance. However, this trend is not consistent, as schools in the least deprived areas of Lambeth show the highest accessibility on average."
  },
  {
    "objectID": "08-network.html#assignment",
    "href": "08-network.html#assignment",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Accessibility analysis involves evaluating how easily people can reach essential services, destinations, or opportunities, such as schools, healthcare facilities, or workplaces, from a given location. It considers factors like distance, travel time, and transport networks, providing valuable insights for urban planning, transport policies, and social equity. The CDRC Access to Healthy Assets & Hazards (AHAH) dataset, for instance, uses accessibility analysis to quantify how easy it is to reach ‘unhealthy’ places, such as pubs and gambling outlets, for each neighbourhood in Great Britain.\nHaving run through all the steps during the tutorial, we can recreate this analysis ourselves. Using Lambeth as a case study, try to complete the following tasks:\n\nExtract all pubs from the Point of Interest dataset.\nFor each LSOA within Lambeth, calculate the average walking distance to the nearest pub.\nCreate a map of the results.\n\n\n\n\n\n\n\nUnlike before, LSOAs are now the unit of analysis. This means you will need to input the LSOA centroids into your distance matrix.\n\n\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R."
  },
  {
    "objectID": "08-network.html#before-you-leave",
    "href": "08-network.html#before-you-leave",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "This brings us to the end of the tutorial. You should now have a basic understanding of the concepts behind accessibility analysis, how it can be executed in R, and some of the challenges you may encounter when conducting your own research. With this being said, you have now reached the end of this week’s content. Onwards and upwards!"
  },
  {
    "objectID": "09-datavis.html",
    "href": "09-datavis.html",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "1 Complex Visualisations\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "04-autocorrelation.html",
    "href": "04-autocorrelation.html",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "1 Spatial Autocorrelation\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "03-operations.html",
    "href": "03-operations.html",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "1 Spatial Queries and Geometric Operations\n\n\n\n\n\n\nThe Geocomputation content for the 2024-2025 academic year is currently undergoing a major update. The materials from 2023-2024 have been archived and can be accessed here: [Link]"
  },
  {
    "objectID": "06-raster.html",
    "href": "06-raster.html",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "So far, we have exclusively focused on the use of vector and tabular data. However, depending on the nature of your research problem, you may also encounter raster data. This week’s content introduces you to raster data, map algebra, and interpolation.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: A good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]\n\n\n\n\n\nFor the first part of this week’s practical material we will be using raster datasets from WorldPop. These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These datasets can be used to explore, for example, changes in the demographic profiles or area deprivation at small spatial scales.\n\n\n\n\n\n\nThe key difference between vector and raster models lies in their structure. Vectors are made up of points, lines, and polygons. In contrast, raster data consists of pixels (or grid cells), similar to an image. Each cell holds a single value representing a geographic phenomenon, such as population density at that location. Common raster data types include remote sensing imagery, such as satellite or LIDAR data.\n\n\n\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files to your computer in your data folder.\n\n\n\n\n\n\n\nA GeoTIFF is a type of raster file format that embeds geographic information, enabling the image to be georeferenced to specific real-world coordinates. It includes metadata like projection, coordinate system, and geographic extent, making it compatible with GIS software for spatial analysis.\n\n\n\nTo focus the analysis on London, we need to clip our dataset to the boundaries of the city. For this, we will use the London Borough boundaries, which can be downloaded from the link below. Be sure to save the files in the data folder within your data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w06-raster-data-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. We can load the individual GeoTiff files that we downloaded into R and reproject them into British National Grid using the terra library.\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/spatial/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/spatial/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"EPSG:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"EPSG:27700\")\n\n\nCarefully examine each dataframe to understand its structure and the information it contains:\n\n\n\nR code\n\n# inspect 2010 data\nhead(pop2010)\n\n\n  gbr_ppp_2010_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n# inspect 2020 data\nhead(pop2020)\n\n  gbr_ppp_2020_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n\n\n\n\n\n\n\nA raster file is always rectangular, with areas lacking data stored as NA. For our population data, this means any pixels outside the land borders of Great Britain will have by definition an NA value.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can also plot the raster files for visual inspection:\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 1: WorldPop 2010 population estimates for the United Kingdom.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 2: WorldPop 2020 population estimates for the United Kingdom.\n\n\n\n\nYou will notice that while the maps appear similar, the legend indicates a significant increase in values over the decade from 2010 to 2021, with the maximum rising from approximately 12,000 people per cell to over 14,000.\nNow that we have our raster data loaded, we will focus on reducing it to display only the extent of London. We will use the London borough GeoPackage\n\n\n\n\n\n\nThe terra package does not accept sf objects, so after loading the London borough boundaries, we need to convert the file into a SpatRaster or SpatVector.\n\n\n\n\n\n\nR code\n\n# load data, to spatvector\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n    vect()\n\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# crop to extent\npop2010_london &lt;- crop(pop2010, borough)\npop2020_london &lt;- crop(pop2020, borough)\n\n# mask to boundaries\npop2010_london &lt;- mask(pop2010_london, borough)\npop2020_london &lt;- mask(pop2020_london, borough)\n\nWe should now have the raster cells that fall within the boundaries of London:\n\n\n\nR code\n\n# inspect\nplot(pop2010_london)\n\n\n\n\n\nFigure 3: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# inspect\nplot(pop2020_london)\n\n\n\n\n\nFigure 4: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 5: Population change in London 2010-2020.\n\n\n\n\n\n\n\nTo further analyse our population change raster, we can create a smoothed version of the lonpop_change raster using the focal() function. This function generates a raster that calculates the average (mean) value of the nearest neighbours for each cell.\n\n\n\nR code\n\n# smooth\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 6: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences may not be immediately apparent, but if you subtract the smoothed raster from the original raster, you will clearly see that changes have occurred.\n\n\n\nR code\n\n# substract\nlonpop_chang_smooth &lt;- lonpop_change - lonpop_smooth\n\n# inspect\nplot(lonpop_chang_smooth)\n\n\n\n\n\nFigure 7: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also use zonal functions to better represent population change by aggregating the data to coarser resolutions. For example, resizing the raster’s spatial resolution to contain larger grid cells simplifies the data, making broader trends more visible. However,it may also end up obfuscating more local patterns.\n\n\n\n\n\n\nWe can resize a raster using the aggregate() function, setting the factor parameter to the scale of resampling desired (e.g. doubling both the width and height of a cell). The function parameter determines how to aggregate the data.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# inspect\nplot(lonpop_agg)\n\n\n\n\n\nFigure 8: Aggregated cell values.\n\n\n\n\nWe can also aggregate raster cells to vector geographies. For example, we can aggregate the WorldPop gridded population estimates to the London borough boundaries:\n\n# aggregate\nlondon_borough_pop &lt;- extract(lonpop_change, borough, fun = sum)\n\n# bind to spatial boundaries\nborough &lt;- borough |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_borough_pop$gbr_ppp_2020_1km_Aggregated)\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"pop_change\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#0570b0\"),\n    title = \"\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 9: Absolute population change in London boroughs 2010-2020.\n\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have a vector dataset, which allows us to perform many of the analyses we have explored in previous weeks.\n\n\n\n\n\n\nCalculating population change, particularly over decades as we have done, can be challenging due to changes in administrative boundaries. Using raster data offers a helpful workaround, provided the rasters are of consistent size and extent.\n\n\n\n\n\n\n\nIn the second part of this week’s practical, we will explore various methods of spatial data interpolation, focusing on air pollution in London using data from Londonair. We will specifically look at Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nLondonair is the website of the London Air Quality Network (LAQN), which provides air pollution data for London and southeast England through the Environmental Research Group at Imperial College This data is publicly available and can be accessed directly using the openair R package, without needing to download files.\n\n\n\n\n\n\n\n\n\nSpatial interpolation predicts a phenomenon at unmeasured locations. It is often used when we want to estimate a variable across space, particularly in areas with sparse or no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2023:2023)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2023:2023, pollutant = \"no2\",\n    meta = TRUE)\n\n\n\n\n\n\n\n\nNot all measurements sites collect data on NO2 so it is normal to get some 404 Not Found warnings.\n\n\n\n\n\n\n\n\n\nThis code may take some time to run, as it will attempt to download data from all air measurement sites for an entire year, with many measurements taken hourly. If you experience too many errors or if it is taking too long, you can download a copy of the data here: [Download].\nOnce downloaded, place the zip file in your data folder. The file is large, so you can leave it unzipped.\n\n\n\nLet us start by loading and inspecting the data:\n\n\n\nR code\n\n# load from zip if downloaded through the link\npollution &lt;- read_csv(\"data/attributes/London-NO2-2023.zip\")\n\n\nMultiple files in zip: reading 'London-Pollution-2023.csv'\nRows: 1615976 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site       code  source latitude longitude site_type\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 2023-01-01 00:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n2 2023-01-01 01:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n3 2023-01-01 02:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n4 2023-01-01 03:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n5 2023-01-01 04:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n6 2023-01-01 05:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n\n\nIn the first five rows, we can see data from the same site, with the date field showing an observation for every hour. Given there are 24 hours in a day, 365 days in a year, and data from hundreds of sites, it is no surprise that the dataset is so large. To make the dataset more manageable, let us summarise the values by site.\n\n\n\nR code\n\n# mean site values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(longitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56  15.7 \n2 BG1       51.6     0.178 16.4 \n3 BG2       51.5     0.133 18.4 \n4 BH0       50.8    -0.148 10.8 \n5 BK0       53.8    -3.01   4.80\n6 BL0       51.5    -0.126 24.0 \n\n\nWe now have 177 measurement sites with their corresponding latitudes, longitudes, and average NO2 values. Let us have a look at the spatial distribution of these measurement sites.\n\n# load boroughs for background\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n  st_union()\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# clip measurement sites to london boundaries\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column, colours\n  tm_symbols(\n    col = \"#fc9272\",\n    size = 0.3,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Measurement site\",\n    col = \"#fc9272\",\n    size = 0.5\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 10: KCL NO2 measurement sites in London.\n\n\n\n\nWe can also use proportional symbols to visualise the values, helping us observe how measurements vary across London.\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column\n  tm_bubbles(\n    size = \"no2\",\n    title.size = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 11: Proportional symbol map of average KCL NO2 measurement in London.\n\n\n\n\nFigure 11 shows heterogeneity in average NO2 measurements across London, both in terms of coverage and NO2 levels. To make reasonable assumptions about NO2 levels in areas without measurements, we can interpolate the missing values.\n\n\nA straightforward method for interpolating values across space is to create a Voronoi tessellation polygons. These polygons define the boundaries of areas closest to each unique point, meaning that each point in the dataset has a corresponding polygon.\n\n\n\n\n\n\nIn addition to Voronoi tessellation, you may encounter the term Thiessen polygons. These terms are often used interchangeably to describe the geometry created from point data.\n\n\n\n\n\n\nR code\n\n# function\nst_voronoi_points &lt;- function(points) {\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to voronoi\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# voronoi tessellation\nmeasurement_sites_voronoi &lt;- st_voronoi_points(measurement_sites)\n\n# replace point geometry with polygon geometry\nmeasurement_sites_tesselation &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_voronoi) |&gt;\n    st_intersection(borough)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nmeasurement_sites_tesselation\n\nSimple feature collection with 98 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 98 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.4 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    18.4 POLYGON ((545264.9 181815.5, 545402.2 184801.2, 547703.4 186697.…\n 3 BL0    24.0 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    13.9 POLYGON ((546306.3 181181, 549837.9 181565.4, 548349.8 175998.9,…\n 5 BT4    38.9 POLYGON ((519912.6 183741.2, 516082 187365.6, 520958.9 189405.3,…\n 6 BT5    24.9 POLYGON ((523877.3 190896.9, 524273.2 190424, 524781.6 189259.6,…\n 7 BT6    24.6 POLYGON ((521236.1 184358.2, 522982.8 184472, 522330 181976.4, 5…\n 8 BT8    23.3 POLYGON ((522982.8 184472, 524217.5 185711.1, 525595.2 182818.7,…\n 9 BX1    15.7 MULTIPOLYGON (((550125.4 169173.1, 550132.2 169161.6, 550144.3 1…\n10 BX2    15.4 POLYGON ((548349.8 175998.9, 549837.9 181565.4, 550403.4 181822.…\n# ℹ 88 more rows\n\n\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function; just know that it takes a point spatial data frame as input and produces a tessellated spatial data frame as output.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now visualise the results of the interpolation:\n\n# shape, polygon\ntm_shape(measurement_sites_tesselation) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"no2\",\n    palette = c(\"#ffffcc\", \"#c2e699\", \"#78c679\", \"#0570b0\"),\n    title = \"Average reading\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 12: Interpolation of average NO2 measurements in London using a Voronoi tessellation.\n\n\n\n\n\n\n\nA more sophisticated method for interpolating point data is Inverse Distance Weighting (IDW). IDW converts numerical point data into a continuous surface, allowing for visualisation of how the data is distributed across space. This technique estimates values at each location by calculating a weighted average from nearby points, with the weights inversely related to their distances.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before running the IDW.\n\n\n\nR code\n\n# create regular output grid\noutput_grid &lt;- borough |&gt;\n    st_make_grid(cellsize = c(1000, 1000))\n\n# execute\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    idp = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\nThe IDW interpolation may take some time to run because it involves calculating the weighted average of nearby points for each location on the grid. In this case, idp = 2 specifies a quadratic decay, meaning the influence of a point decreases with the square of the distance.\n\n\n\nAgain, we can map the results for visual inspection.\n\n\n\n\n\n\nThe values of the IDW output are stored in the raster grid as var1.pred.\n\n\n\n\n# shape, polygon\ntm_shape(measurement_sites_idw) +\n\n  # specify column, classes\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"cont\",\n    palette = \"Oranges\",\n    title = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 13: Interpolation of average NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have set the output cell size to 1000x1000 metres. While a smaller cell size can yield a smoother IDW output, it may introduce uncertainty due to the limited number of data points available for interpolation. Moreover, reducing the cell size will exponentially increase processing time.\n\n\n\n\n\n\n\nHaving run through all the steps during the tutorial, we can conduct some more granular analysis of the NO2 measurements. For example, instead of examining the annual average measurements, we could compare data across different months. Please try the following tasks:\n\nCreate monthly averages for the pollution data.\nFor both June and December, generate a dataframe containing the London monitoring sites along with their average NO₂ readings for these months.\nPerform Inverse Distance Weighting (IDW) interpolation for the data from both months.\nCombine the results to assess the differences between these months.\n\n\n\n\nThis week, we have explored raster datasets and how to manage and process them using the terra library. While you will typically encounter vector data, particularly in relation to government statistics and administrative boundaries, there are also many use cases where raster data may be encountered. With that being said: that is it for this week!"
  },
  {
    "objectID": "06-raster.html#slides-w06",
    "href": "06-raster.html#slides-w06",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "06-raster.html#reading-w06",
    "href": "06-raster.html#reading-w06",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "Gimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: A good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]"
  },
  {
    "objectID": "06-raster.html#population-change-in-london",
    "href": "06-raster.html#population-change-in-london",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "For the first part of this week’s practical material we will be using raster datasets from WorldPop. These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These datasets can be used to explore, for example, changes in the demographic profiles or area deprivation at small spatial scales.\n\n\n\n\n\n\nThe key difference between vector and raster models lies in their structure. Vectors are made up of points, lines, and polygons. In contrast, raster data consists of pixels (or grid cells), similar to an image. Each cell holds a single value representing a geographic phenomenon, such as population density at that location. Common raster data types include remote sensing imagery, such as satellite or LIDAR data.\n\n\n\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files to your computer in your data folder.\n\n\n\n\n\n\n\nA GeoTIFF is a type of raster file format that embeds geographic information, enabling the image to be georeferenced to specific real-world coordinates. It includes metadata like projection, coordinate system, and geographic extent, making it compatible with GIS software for spatial analysis.\n\n\n\nTo focus the analysis on London, we need to clip our dataset to the boundaries of the city. For this, we will use the London Borough boundaries, which can be downloaded from the link below. Be sure to save the files in the data folder within your data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w06-raster-data-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. We can load the individual GeoTiff files that we downloaded into R and reproject them into British National Grid using the terra library.\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/spatial/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/spatial/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"EPSG:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"EPSG:27700\")\n\n\nCarefully examine each dataframe to understand its structure and the information it contains:\n\n\n\nR code\n\n# inspect 2010 data\nhead(pop2010)\n\n\n  gbr_ppp_2010_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n# inspect 2020 data\nhead(pop2020)\n\n  gbr_ppp_2020_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n\n\n\n\n\n\n\nA raster file is always rectangular, with areas lacking data stored as NA. For our population data, this means any pixels outside the land borders of Great Britain will have by definition an NA value.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can also plot the raster files for visual inspection:\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 1: WorldPop 2010 population estimates for the United Kingdom.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 2: WorldPop 2020 population estimates for the United Kingdom.\n\n\n\n\nYou will notice that while the maps appear similar, the legend indicates a significant increase in values over the decade from 2010 to 2021, with the maximum rising from approximately 12,000 people per cell to over 14,000.\nNow that we have our raster data loaded, we will focus on reducing it to display only the extent of London. We will use the London borough GeoPackage\n\n\n\n\n\n\nThe terra package does not accept sf objects, so after loading the London borough boundaries, we need to convert the file into a SpatRaster or SpatVector.\n\n\n\n\n\n\nR code\n\n# load data, to spatvector\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n    vect()\n\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# crop to extent\npop2010_london &lt;- crop(pop2010, borough)\npop2020_london &lt;- crop(pop2020, borough)\n\n# mask to boundaries\npop2010_london &lt;- mask(pop2010_london, borough)\npop2020_london &lt;- mask(pop2020_london, borough)\n\nWe should now have the raster cells that fall within the boundaries of London:\n\n\n\nR code\n\n# inspect\nplot(pop2010_london)\n\n\n\n\n\nFigure 3: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# inspect\nplot(pop2020_london)\n\n\n\n\n\nFigure 4: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 5: Population change in London 2010-2020.\n\n\n\n\n\n\n\nTo further analyse our population change raster, we can create a smoothed version of the lonpop_change raster using the focal() function. This function generates a raster that calculates the average (mean) value of the nearest neighbours for each cell.\n\n\n\nR code\n\n# smooth\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 6: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences may not be immediately apparent, but if you subtract the smoothed raster from the original raster, you will clearly see that changes have occurred.\n\n\n\nR code\n\n# substract\nlonpop_chang_smooth &lt;- lonpop_change - lonpop_smooth\n\n# inspect\nplot(lonpop_chang_smooth)\n\n\n\n\n\nFigure 7: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also use zonal functions to better represent population change by aggregating the data to coarser resolutions. For example, resizing the raster’s spatial resolution to contain larger grid cells simplifies the data, making broader trends more visible. However,it may also end up obfuscating more local patterns.\n\n\n\n\n\n\nWe can resize a raster using the aggregate() function, setting the factor parameter to the scale of resampling desired (e.g. doubling both the width and height of a cell). The function parameter determines how to aggregate the data.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# inspect\nplot(lonpop_agg)\n\n\n\n\n\nFigure 8: Aggregated cell values.\n\n\n\n\nWe can also aggregate raster cells to vector geographies. For example, we can aggregate the WorldPop gridded population estimates to the London borough boundaries:\n\n# aggregate\nlondon_borough_pop &lt;- extract(lonpop_change, borough, fun = sum)\n\n# bind to spatial boundaries\nborough &lt;- borough |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_borough_pop$gbr_ppp_2020_1km_Aggregated)\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"pop_change\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#0570b0\"),\n    title = \"\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 9: Absolute population change in London boroughs 2010-2020.\n\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have a vector dataset, which allows us to perform many of the analyses we have explored in previous weeks.\n\n\n\n\n\n\nCalculating population change, particularly over decades as we have done, can be challenging due to changes in administrative boundaries. Using raster data offers a helpful workaround, provided the rasters are of consistent size and extent."
  },
  {
    "objectID": "06-raster.html#air-pollution-in-london",
    "href": "06-raster.html#air-pollution-in-london",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "In the second part of this week’s practical, we will explore various methods of spatial data interpolation, focusing on air pollution in London using data from Londonair. We will specifically look at Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nLondonair is the website of the London Air Quality Network (LAQN), which provides air pollution data for London and southeast England through the Environmental Research Group at Imperial College This data is publicly available and can be accessed directly using the openair R package, without needing to download files.\n\n\n\n\n\n\n\n\n\nSpatial interpolation predicts a phenomenon at unmeasured locations. It is often used when we want to estimate a variable across space, particularly in areas with sparse or no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2023:2023)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2023:2023, pollutant = \"no2\",\n    meta = TRUE)\n\n\n\n\n\n\n\n\nNot all measurements sites collect data on NO2 so it is normal to get some 404 Not Found warnings.\n\n\n\n\n\n\n\n\n\nThis code may take some time to run, as it will attempt to download data from all air measurement sites for an entire year, with many measurements taken hourly. If you experience too many errors or if it is taking too long, you can download a copy of the data here: [Download].\nOnce downloaded, place the zip file in your data folder. The file is large, so you can leave it unzipped.\n\n\n\nLet us start by loading and inspecting the data:\n\n\n\nR code\n\n# load from zip if downloaded through the link\npollution &lt;- read_csv(\"data/attributes/London-NO2-2023.zip\")\n\n\nMultiple files in zip: reading 'London-Pollution-2023.csv'\nRows: 1615976 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site       code  source latitude longitude site_type\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 2023-01-01 00:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n2 2023-01-01 01:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n3 2023-01-01 02:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n4 2023-01-01 03:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n5 2023-01-01 04:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n6 2023-01-01 05:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n\n\nIn the first five rows, we can see data from the same site, with the date field showing an observation for every hour. Given there are 24 hours in a day, 365 days in a year, and data from hundreds of sites, it is no surprise that the dataset is so large. To make the dataset more manageable, let us summarise the values by site.\n\n\n\nR code\n\n# mean site values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(longitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56  15.7 \n2 BG1       51.6     0.178 16.4 \n3 BG2       51.5     0.133 18.4 \n4 BH0       50.8    -0.148 10.8 \n5 BK0       53.8    -3.01   4.80\n6 BL0       51.5    -0.126 24.0 \n\n\nWe now have 177 measurement sites with their corresponding latitudes, longitudes, and average NO2 values. Let us have a look at the spatial distribution of these measurement sites.\n\n# load boroughs for background\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n  st_union()\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# clip measurement sites to london boundaries\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column, colours\n  tm_symbols(\n    col = \"#fc9272\",\n    size = 0.3,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Measurement site\",\n    col = \"#fc9272\",\n    size = 0.5\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 10: KCL NO2 measurement sites in London.\n\n\n\n\nWe can also use proportional symbols to visualise the values, helping us observe how measurements vary across London.\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column\n  tm_bubbles(\n    size = \"no2\",\n    title.size = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 11: Proportional symbol map of average KCL NO2 measurement in London.\n\n\n\n\nFigure 11 shows heterogeneity in average NO2 measurements across London, both in terms of coverage and NO2 levels. To make reasonable assumptions about NO2 levels in areas without measurements, we can interpolate the missing values.\n\n\nA straightforward method for interpolating values across space is to create a Voronoi tessellation polygons. These polygons define the boundaries of areas closest to each unique point, meaning that each point in the dataset has a corresponding polygon.\n\n\n\n\n\n\nIn addition to Voronoi tessellation, you may encounter the term Thiessen polygons. These terms are often used interchangeably to describe the geometry created from point data.\n\n\n\n\n\n\nR code\n\n# function\nst_voronoi_points &lt;- function(points) {\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to voronoi\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# voronoi tessellation\nmeasurement_sites_voronoi &lt;- st_voronoi_points(measurement_sites)\n\n# replace point geometry with polygon geometry\nmeasurement_sites_tesselation &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_voronoi) |&gt;\n    st_intersection(borough)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nmeasurement_sites_tesselation\n\nSimple feature collection with 98 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 98 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.4 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    18.4 POLYGON ((545264.9 181815.5, 545402.2 184801.2, 547703.4 186697.…\n 3 BL0    24.0 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    13.9 POLYGON ((546306.3 181181, 549837.9 181565.4, 548349.8 175998.9,…\n 5 BT4    38.9 POLYGON ((519912.6 183741.2, 516082 187365.6, 520958.9 189405.3,…\n 6 BT5    24.9 POLYGON ((523877.3 190896.9, 524273.2 190424, 524781.6 189259.6,…\n 7 BT6    24.6 POLYGON ((521236.1 184358.2, 522982.8 184472, 522330 181976.4, 5…\n 8 BT8    23.3 POLYGON ((522982.8 184472, 524217.5 185711.1, 525595.2 182818.7,…\n 9 BX1    15.7 MULTIPOLYGON (((550125.4 169173.1, 550132.2 169161.6, 550144.3 1…\n10 BX2    15.4 POLYGON ((548349.8 175998.9, 549837.9 181565.4, 550403.4 181822.…\n# ℹ 88 more rows\n\n\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function; just know that it takes a point spatial data frame as input and produces a tessellated spatial data frame as output.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now visualise the results of the interpolation:\n\n# shape, polygon\ntm_shape(measurement_sites_tesselation) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"no2\",\n    palette = c(\"#ffffcc\", \"#c2e699\", \"#78c679\", \"#0570b0\"),\n    title = \"Average reading\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 12: Interpolation of average NO2 measurements in London using a Voronoi tessellation.\n\n\n\n\n\n\n\nA more sophisticated method for interpolating point data is Inverse Distance Weighting (IDW). IDW converts numerical point data into a continuous surface, allowing for visualisation of how the data is distributed across space. This technique estimates values at each location by calculating a weighted average from nearby points, with the weights inversely related to their distances.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before running the IDW.\n\n\n\nR code\n\n# create regular output grid\noutput_grid &lt;- borough |&gt;\n    st_make_grid(cellsize = c(1000, 1000))\n\n# execute\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    idp = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\nThe IDW interpolation may take some time to run because it involves calculating the weighted average of nearby points for each location on the grid. In this case, idp = 2 specifies a quadratic decay, meaning the influence of a point decreases with the square of the distance.\n\n\n\nAgain, we can map the results for visual inspection.\n\n\n\n\n\n\nThe values of the IDW output are stored in the raster grid as var1.pred.\n\n\n\n\n# shape, polygon\ntm_shape(measurement_sites_idw) +\n\n  # specify column, classes\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"cont\",\n    palette = \"Oranges\",\n    title = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 13: Interpolation of average NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have set the output cell size to 1000x1000 metres. While a smaller cell size can yield a smoother IDW output, it may introduce uncertainty due to the limited number of data points available for interpolation. Moreover, reducing the cell size will exponentially increase processing time."
  },
  {
    "objectID": "06-raster.html#assignment",
    "href": "06-raster.html#assignment",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "Having run through all the steps during the tutorial, we can conduct some more granular analysis of the NO2 measurements. For example, instead of examining the annual average measurements, we could compare data across different months. Please try the following tasks:\n\nCreate monthly averages for the pollution data.\nFor both June and December, generate a dataframe containing the London monitoring sites along with their average NO₂ readings for these months.\nPerform Inverse Distance Weighting (IDW) interpolation for the data from both months.\nCombine the results to assess the differences between these months."
  },
  {
    "objectID": "06-raster.html#before-you-leave",
    "href": "06-raster.html#before-you-leave",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "This week, we have explored raster datasets and how to manage and process them using the terra library. While you will typically encounter vector data, particularly in relation to government statistics and administrative boundaries, there are also many use cases where raster data may be encountered. With that being said: that is it for this week!"
  },
  {
    "objectID": "11-data.html",
    "href": "11-data.html",
    "title": "1 Data Sources",
    "section": "",
    "text": "Below is a list of resources that you may find helpful when sourcing data for your coursework or dissertation. This list is not exhaustive but includes some recommended websites to get you started.\n\n\nThe following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop\n\n\n\n\nUndergraduate students can also apply for Safeguarded datasets held by the Consumer Data Research Centre. Accessing these datasets requires following a specific process, which is outlined on the CDRC website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nSince the application process for Safeguarded CDRC datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  },
  {
    "objectID": "11-data.html#open-data",
    "href": "11-data.html#open-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "The following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop"
  },
  {
    "objectID": "11-data.html#safeguarded-data",
    "href": "11-data.html#safeguarded-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "Undergraduate students can also apply for Safeguarded datasets held by the Consumer Data Research Centre. Accessing these datasets requires following a specific process, which is outlined on the CDRC website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nSince the application process for Safeguarded CDRC datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  }
]