[
  {
    "objectID": "07-geodemographics.html",
    "href": "07-geodemographics.html",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "This week we will turn to geodemographic classification. Geodemographic classification is a method used to categorise geographic areas and the people living in them based on demographic, socioeconomic, and sometimes lifestyle characteristics. This approach combines geographic information with demographic data to create profiles of different neighborhoods.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nSingleton, A. and Longley, P. A. 2024. Classifying and mapping residential structure through the London Output Area Classification. Environment and Planning B: Urban Analytics and City Science 51(5): 1153-1164. [Link]\nWyszomierski, J., Longley, P. A., and Singleton, A. et al. 2024. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. 190(2): e12550. [Link]\n\n\n\n\n\nDalton, C. M. and Thatcher. J. 2015. Inflated granularity: Spatial “Big Data” and geodemographics. Big Data & Society 2(2): 1-15. [Link]\nFränti, P. and Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]\n\n\n\n\n\nToday, we will create our own geodemographic classification to examine demographic clusters across London, drawing inspiration from London Output Area Classification. Specifically, we will try to identify clusters based on age group, self-identified ethnicity, country of birth, and first or preferred language.\nThe data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. These datasets have been extracted using the Custom Dataset Tool, and you can download each file via the links provided below. A copy of the 2021 London LSOAs spatial boundaries is also available. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Country of Birth\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Main Language\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have already downloaded some of these datasets in previous weeks, but for completeness, they are all provided here. Only download the datasets you do not already have or did not save.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w07-geodemographic-classification.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggcorrplot)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the individual csv files that we downloaded into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/attributes/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load country of birth data\nlsoa_cob &lt;- read_csv(\"data/attributes/London-LSOA-Country-of-Birth.csv\")\n\nRows: 39952 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Country of birth (8 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load language data\nlsoa_lan &lt;- read_csv(\"data/attributes/London-LSOA-MainLanguage.csv\")\n\nRows: 54934 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Main language (11 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect age data\nhead(lsoa_age)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect country of birth data\nhead(lsoa_cob)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Country of birth (8 …³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Country of birth (8 categories) Code`\n# ℹ 2 more variables: `Country of birth (8 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect ethnicity data\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect language data\nhead(lsoa_lan)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Main language (11 ca…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Main language (11 categories) Code`\n# ℹ 2 more variables: `Main language (11 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\nTo identify geodemographic clusters in our dataset, we will use a technique called \\(k\\)-means. \\(k\\)-means aims to partition a set of standardised observations into a specified number of clusters (\\(k\\)). To do this we first need to prepare the individual datasets, as well as transform and standardise the input variables.\n\n\n\n\n\n\n\\(k\\)-means clustering is an unsupervised machine learning algorithm used to group data into a predefined number of clusters, based on similarities between data points. It works by initially assigning \\(k\\) random centroids, then iteratively updating them by assigning each data point to the nearest centroid and recalculating the centroid’s position based on the mean of the points in each cluster. The process continues until the centroids stabilise, meaning they no longer change significantly. \\(k\\)-means is often used for tasks such as data segmentation, image compression, or anomaly detection. It is simple but may not work well with non-spherical or overlapping clusters.\n\n\n\nBecause all the data are stored in long format, with each London LSOA appearing on multiple rows for each category — such as separate rows for different age groups, ethnicities, countries of birth, and first or preferred languages - we need to transform it into a wide format. For example, instead of having multiple rows for an LSOA showing counts for different age groups all the information for each LSOA will be consolidated into a single row. Additionally, we will clean up the column names to follow standard R naming conventions and make the data easier to work with. Like we have done previously, we can automate this process using the janitor package.\nWe will begin with the age dataframe:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"age_5_categories\",\n        values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions and only retain those columns that are likely to be meaningful in the context of the classification:\n\n\n\nR code\n\n# total observations\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(age_pop = sum(across(2:6)))\n\n# total proportions, select columns\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(2:6, ~./age_pop)) |&gt;\n    select(1:6)\n\n# inspect\nhead(lsoa_age)\n\n\n# A tibble: 6 × 6\n# Rowwise: \n  lower_layer_super_output_areas_code aged_15_years_and_un…¹ aged_16_to_24_years\n  &lt;chr&gt;                                                &lt;dbl&gt;               &lt;dbl&gt;\n1 E01000001                                           0.0846              0.0744\n2 E01000002                                           0.0621              0.0889\n3 E01000003                                           0.0682              0.0706\n4 E01000005                                           0.127               0.178 \n5 E01000006                                           0.224               0.120 \n6 E01000007                                           0.257               0.103 \n# ℹ abbreviated name: ¹​aged_15_years_and_under\n# ℹ 3 more variables: aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;\n\n\nThis looks much better. We can do the same for the country of birth data:\n\n\n\nR code\n\n# prepare country of birth data\nlsoa_cob &lt;- lsoa_cob |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"country_of_birth_8_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_cob &lt;- lsoa_cob |&gt;\n    rowwise() |&gt;\n    mutate(cob_pop = sum(across(2:9))) |&gt;\n    mutate(across(2:9, ~./cob_pop)) |&gt;\n    select(-2, -10)\n\n\nAnd we can do the same for the ethnicity and language datasets:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2, -22)\n\n# prepare language data\nlsoa_lan &lt;- lsoa_lan |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"main_language_11_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_lan &lt;- lsoa_lan |&gt;\n    rowwise() |&gt;\n    mutate(lan_pop = sum(across(2:12))) |&gt;\n    mutate(across(2:12, ~./lan_pop)) |&gt;\n    select(-2, -11, -13)\n\n\nWe now have four separate datasets, each containing the proportions of usual residents classified into different groups based on age, country of birth, ethnicity, and language.\n\n\n\nWhere we initially selected variables from different demographic domains, not all variables may be suitable for inclusion. Firstly, the variables need to exhibit sufficient heterogeneity to ensure they capture meaningful differences between observations. Secondly, variables should not be highly correlated with one another, as this redundancy can skew the clustering results. Ensuring acceptable correlation between variables helps maintain the diversity of information and improves the robustness of the clustering outcome.\n\n\n\n\n\n\nVariable selection is often a time-consuming process that requires a combination of domain knowledge and more extensive exploratory analysis than is covered in this practical.\n\n\n\nA straightforward yet effective method to examine the distribution of our variables is to create boxplots for each variable. This can be efficiently achieved by using facet_wrap() from the ggplot2 library to generate a matrix of panels, allowing us to visualise all variables in a single view.\n\n\n\n\n\n\nggplot2 is a popular data visualisation package in R, designed for creating complex plots. It uses the Grammar of Graphics to build layered, customisable graphics by mapping data to visual elements like colour, size, and shape. We will explore the ggplot2 library further in Weeks 9 and 10. In the meantime, you can refer to the ggplot2 documentation for more details on facet_wrap().\n\n\n\n\n\n\nR code\n\n# wide to long\nlsoa_age_wd &lt;- lsoa_age |&gt;\n    pivot_longer(cols = c(2:5), names_to = \"agegroup\", values_to = \"count\")\n\n# facet age\nggplot(lsoa_age_wd, aes(y = count)) + geom_boxplot() + facet_wrap(~agegroup, ncol = 2) +\n    theme_minimal() + ylab(\"\")\n\n\n\n\n\nFigure 1: Boxplots of the distribution of the age dataset.\n\n\n\n\nWhen repeating this process for the birth, ethnicity, and language variables, you will notice that some variables have a very limited distribution. Specifically, some variables may have a value of 0 for the majority of London LSOAs. As a rule of thumb, we will retain only those variables where at least 75% of the LSOAs have values different from 0.\n\n\n\n\n\n\nThis threshold of 75% is arbitrary, and in practice, more thorough consideration should be given when deciding whether to include or exclude a variable.\n\n\n\n\n\n\nR code\n\n# join\nlsoa_df &lt;- lsoa_age |&gt;\n    left_join(lsoa_cob, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_eth, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_lan, by = \"lower_layer_super_output_areas_code\")\n\n# calculate proportion of zeroes\nzero_prop &lt;- sapply(lsoa_df[2:41], function(x) {\n    mean(x == 0)\n})\n\n# extract variables with high proportion zeroes\nidx &lt;- which(zero_prop &gt; 0.25)\n\n# inspect\nidx\n\n\n   white_gypsy_or_irish_traveller            any_other_uk_languages \n                               27                                33 \n  oceanic_or_australian_languages north_or_south_american_languages \n                               37                                38 \n\n# remove variables with high proportion zeroes\nlsoa_df &lt;- lsoa_df |&gt;\n    select(-white_gypsy_or_irish_traveller, -any_other_uk_languages, -oceanic_or_australian_languages,\n        -north_or_south_american_languages)\n\n\n\n\n\n\n\nThe code above makes use of Boolean logic to calculate the proportion of zeroes within each variable. The x == 0 part checks each value in column x to see if it is equal to 0, returning TRUE or FALSE for each element. The mean() function is then used to calculate the average of the TRUE values in the column. Since TRUE is treated as 1 and FALSE as 0, this gives the proportion of values in the column that are equal to zero.\n\n\n\nWe can subsequently check for multicollinearity of the remaining variables. The easiest way to check the correlations between all variables is probably by visualising a correlation matrix:\n\n\n\nR code\n\n# inspect variable names\nnames(lsoa_df)\n\n\n [1] \"lower_layer_super_output_areas_code\"                                  \n [2] \"aged_15_years_and_under\"                                              \n [3] \"aged_16_to_24_years\"                                                  \n [4] \"aged_25_to_34_years\"                                                  \n [5] \"aged_35_to_49_years\"                                                  \n [6] \"aged_50_years_and_over\"                                               \n [7] \"europe_united_kingdom\"                                                \n [8] \"europe_ireland\"                                                       \n [9] \"europe_other_europe\"                                                  \n[10] \"africa\"                                                               \n[11] \"middle_east_and_asia\"                                                 \n[12] \"the_americas_and_the_caribbean\"                                       \n[13] \"antarctica_and_oceania_including_australasia_and_other\"               \n[14] \"asian_asian_british_or_asian_welsh_bangladeshi\"                       \n[15] \"asian_asian_british_or_asian_welsh_chinese\"                           \n[16] \"asian_asian_british_or_asian_welsh_indian\"                            \n[17] \"asian_asian_british_or_asian_welsh_pakistani\"                         \n[18] \"asian_asian_british_or_asian_welsh_other_asian\"                       \n[19] \"black_black_british_black_welsh_caribbean_or_african_african\"         \n[20] \"black_black_british_black_welsh_caribbean_or_african_caribbean\"       \n[21] \"black_black_british_black_welsh_caribbean_or_african_other_black\"     \n[22] \"mixed_or_multiple_ethnic_groups_white_and_asian\"                      \n[23] \"mixed_or_multiple_ethnic_groups_white_and_black_african\"              \n[24] \"mixed_or_multiple_ethnic_groups_white_and_black_caribbean\"            \n[25] \"mixed_or_multiple_ethnic_groups_other_mixed_or_multiple_ethnic_groups\"\n[26] \"white_english_welsh_scottish_northern_irish_or_british\"               \n[27] \"white_irish\"                                                          \n[28] \"white_roma\"                                                           \n[29] \"white_other_white\"                                                    \n[30] \"other_ethnic_group_arab\"                                              \n[31] \"other_ethnic_group_any_other_ethnic_group\"                            \n[32] \"english_or_welsh\"                                                     \n[33] \"european_languages_eu\"                                                \n[34] \"other_european_languages_non_eu\"                                      \n[35] \"asian_languages\"                                                      \n[36] \"african_languages\"                                                    \n[37] \"any_other_languages\"                                                  \n\n# change variable names to index to improve visualisation\nlsoa_df_vis &lt;- lsoa_df\nnames(lsoa_df_vis)[2:37] &lt;- paste0(\"v\", sprintf(\"%02d\", 1:36))\n\n# correlation matrix\ncor_mat &lt;- cor(lsoa_df_vis[, -1])\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\nFigure 2: Correlation plot of classification variables.\n\n\n\n\nFollowing the approach from Wyszomierski et al. (2024), we can define a weak correlation as lying between 0 and 0.40, moderate as between 0.41 and 0.65, strong as between 0.66 and 0.80, and very strong as between 0.81 and 1.\nA few strong and very strong correlations can be observed that potentially could be removed; however, to maintain representation, here we decide to retain all variables.\n\n\n\nIf the input data are heavily skewed or contain outliers, \\(k\\)-means may produce less meaningful clusters. While normality is not required per se, it has been common to do this nonetheless. More important is to standardise the input variables, especially when they are measured on different scales. This ensures that each variable contributes equally to the clustering process.\n\n\n\nR code\n\n# inverse hyperbolic sine\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], asinh)\n\n# range standardise\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], function(x) {\n    (x - min(x))/(max(x) - min(x))\n})\n\n\n\n\n\nNow our data are prepared we will start by creating an elbow plot. The elbow method is a visual tool that helps determine the optimal number of clusters in a dataset. This is important because with \\(k\\)-means clustering you need to specify the numbers of clusters a priori. The elbow method involves running the clustering algorithm with varying numbers of clusters (\\(k\\)) and plotting the total explained variation (known as the Within Sum of Squares) against the number of clusters. The goal is to identify the elbow point on the curve, where the rate of decrease in explained variation starts to slow. This point suggests that adding more clusters yields diminishing returns in terms of explained variation.\n\n\n\nR code\n\n# elbow plot\nfviz_nbclust(lsoa_df_vis[, -1], kmeans, nstart = 100, iter.max = 100, method = \"wss\")\n\n\n\n\n\nFigure 3: Elbow plot with Within Sum of Squares against number of clusters.\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 6 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective, and multiple options for the optimal number of clusters might be justified; for instance, 4, 5, or even 7 clusters could be reasonable choices. In addition to the elbow method, other techniques can aid in determining the optimal number of clusters, such as silhouette scores and the gap statistic. An alternative and helful approach is to use a clustergram, which is a two-dimensional plot that visualises the flows of observations between clusters as more clusters are added. This method illustrates how your data reshuffles with each additional cluster and provides insights into the quality of the splits. This method can be done in R, but currently easier to implement in Python.\n\n\n\n\n\n\nNow we have decided on the number of clusters, we can run our \\(k\\)-means analysis.\n\n\n\nR code\n\n# set seed for reproducibility\nset.seed(999)\n\n# k-means\nlsoa_clus &lt;- kmeans(lsoa_df_vis[, -1], centers = 6, nstart = 100, iter.max = 100)\n\n\nWe can inspect the object to get some information about our clusters:\n\n\n\nR code\n\n# inspect\nlsoa_clus\n\n\nK-means clustering with 6 clusters of sizes 796, 1097, 771, 1011, 851, 468\n\nCluster means:\n        v01       v02       v03       v04       v05       v06       v07\n1 0.4816225 0.1632210 0.2425566 0.4838983 0.4169123 0.5410477 0.1337158\n        v08       v09       v10        v11        v12        v13        v14\n1 0.3007540 0.2480613 0.2859754 0.08913663 0.05177222 0.14603013 0.06993627\n         v15        v16        v17        v18        v19        v20        v21\n1 0.12176548 0.10935022 0.20109979 0.18150482 0.11605092 0.12757934 0.09288236\n         v22        v23       v24       v25       v26        v27       v28\n1 0.07473711 0.14662903 0.1842217 0.3522638 0.1490577 0.02997040 0.2423784\n         v29       v30       v31       v32        v33        v34        v35\n1 0.07148504 0.2193009 0.5870244 0.2411272 0.07541661 0.23467242 0.10174187\n         v36\n1 0.10216507\n [ reached getOption(\"max.print\") -- omitted 5 rows ]\n\nClustering vector:\n [1] 4 4 4 1 1 5 5 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 2 1 1 2 2 2 1 1 1\n[39] 1 1 1 1 5 1 5 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4944 entries ]\n\nWithin cluster sum of squares by cluster:\n[1] 259.0272 177.7951 288.8625 232.7770 298.9145 160.1702\n (between_SS / total_SS =  48.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\nWe now need to perform some post-processing to extract useful summary data for each cluster. To characterise the clusters, we can compare the global mean values of each variable with the mean values specific to each cluster.\n\n\n\nR code\n\n# global means\nglob_means &lt;- colMeans(lsoa_df_vis[, -1])\n\n# add clusters to input data\nlsoa_df_vis &lt;- cbind(lsoa_df_vis, cluster = lsoa_clus$cluster)\n\n# cluster means\ncluster_means &lt;- lsoa_df_vis |&gt;\n    group_by(cluster) |&gt;\n    summarise(across(2:37, mean))\n\n# difference\ncluster_diffs &lt;- cluster_means |&gt;\n    mutate(across(2:37, ~. - glob_means[cur_column()]))\n\n\nThese comparisons can then be visualised using, for instance, a radial bar plot:\n\n\n\nR code\n\n# to long format\ncluster_diffs_long &lt;- cluster_diffs |&gt;\n    pivot_longer(!cluster, names_to = \"vars\", values_to = \"score\")\n\n# facet clusters\nggplot(cluster_diffs_long, aes(x = factor(vars), y = score)) + geom_bar(stat = \"identity\") +\n    coord_radial(rotate.angle = TRUE, expand = FALSE) + facet_wrap(~cluster, ncol = 3) +\n    theme_minimal() + theme(axis.text.x = element_text(size = 7)) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\nFigure 4: Radial barplots of cluster means for each input variable.\n\n\n\n\nThese plots can serve as a foundation for creating pen portraits by closely examining which variables drive each cluster.\n\n\n\n\n\n\nFor easier interpretation, these values can be transformed into index scores, allowing us to assess which variables are under- or overrepresented within each cluster group.\n\n\n\nOf course, we can also map the results:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join\nlsoa21 &lt;- cbind(lsoa21, cluster = lsoa_clus$cluster)\n\n# shape, polygon\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"cluster\",\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Cluster number\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 5: Classification of London LSOAs based on several demographic variables.\n\n\n\n\n\n\n\n\nThe creation of a geodemographic classification is an iterative process. This typically includes adding or removing variables, adjusting the number of clusters, and grouping data in different ways to achieve the most meaningful segmentation. Try to do the following:\n\nDownload the two datasets provided below and save them to your data folder. The datasets include:\n\nA csv file containing the number of people aged 16 years and older by occupational category, as defined by the Standard Occupational Classification 2020, aggregated by 2021 LSOAs.\nA csv file containing the number of people aged 16 years and older by their highest level of qualification, also aggregated to the 2021 LSOA level.\n\nPrepare these two datasets and retain only those variables that are potentially meaningful. Filter out any variables with a high proportion of zero values.\nMerge the education and occupation dataset with the dataset used to generate the initial geodemographic classification. Check for multicollinearity and consider removing any variables that are highly correlated.\nPerform \\(k\\)-means clustering on your extended dataset. Make sure to select an appropriate number of clusters for your analysis.\nInterpret the individual clusters in terms of the variables that are under- and overrepresented.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Occupation\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Education\ncsv\nDownload\n\n\n\n\n\n\nHaving finished this tutorial, you should now understand the basics of a geodemographic classification. That is all for this week!"
  },
  {
    "objectID": "07-geodemographics.html#lecture-slides",
    "href": "07-geodemographics.html#lecture-slides",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "07-geodemographics.html#reading-list",
    "href": "07-geodemographics.html#reading-list",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Longley, P. A. 2012. Geodemographics and the practices of geographic information science. International Journal of Geographical Information Science 26(12): 2227-2237. [Link]\nSingleton, A. and Longley, P. A. 2024. Classifying and mapping residential structure through the London Output Area Classification. Environment and Planning B: Urban Analytics and City Science 51(5): 1153-1164. [Link]\nWyszomierski, J., Longley, P. A., and Singleton, A. et al. 2024. A neighbourhood Output Area Classification from the 2021 and 2022 UK censuses. The Geographical Journal. 190(2): e12550. [Link]\n\n\n\n\n\nDalton, C. M. and Thatcher. J. 2015. Inflated granularity: Spatial “Big Data” and geodemographics. Big Data & Society 2(2): 1-15. [Link]\nFränti, P. and Sieronoja, S. 2019. How much can k-means be improved by using better initialization and repeats? Pattern Recognition 93: 95-112. [Link]\nSingleton, A. and Spielman, S. 2014. The past, present, and future of geodemographic research in the United States and United Kingdom. The Professional Geographer 66(4): 558-567. [Link]"
  },
  {
    "objectID": "07-geodemographics.html#classifying-london",
    "href": "07-geodemographics.html#classifying-london",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Today, we will create our own geodemographic classification to examine demographic clusters across London, drawing inspiration from London Output Area Classification. Specifically, we will try to identify clusters based on age group, self-identified ethnicity, country of birth, and first or preferred language.\nThe data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. These datasets have been extracted using the Custom Dataset Tool, and you can download each file via the links provided below. A copy of the 2021 London LSOAs spatial boundaries is also available. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Country of Birth\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Main Language\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have already downloaded some of these datasets in previous weeks, but for completeness, they are all provided here. Only download the datasets you do not already have or did not save.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w07-geodemographic-classification.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggcorrplot)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the individual csv files that we downloaded into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/attributes/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load country of birth data\nlsoa_cob &lt;- read_csv(\"data/attributes/London-LSOA-Country-of-Birth.csv\")\n\nRows: 39952 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Country of birth (8 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load language data\nlsoa_lan &lt;- read_csv(\"data/attributes/London-LSOA-MainLanguage.csv\")\n\nRows: 54934 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Main language (11 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect age data\nhead(lsoa_age)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect country of birth data\nhead(lsoa_cob)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Country of birth (8 …³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Country of birth (8 categories) Code`\n# ℹ 2 more variables: `Country of birth (8 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect ethnicity data\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect language data\nhead(lsoa_lan)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Main language (11 ca…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Main language (11 categories) Code`\n# ℹ 2 more variables: `Main language (11 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\nTo identify geodemographic clusters in our dataset, we will use a technique called \\(k\\)-means. \\(k\\)-means aims to partition a set of standardised observations into a specified number of clusters (\\(k\\)). To do this we first need to prepare the individual datasets, as well as transform and standardise the input variables.\n\n\n\n\n\n\n\\(k\\)-means clustering is an unsupervised machine learning algorithm used to group data into a predefined number of clusters, based on similarities between data points. It works by initially assigning \\(k\\) random centroids, then iteratively updating them by assigning each data point to the nearest centroid and recalculating the centroid’s position based on the mean of the points in each cluster. The process continues until the centroids stabilise, meaning they no longer change significantly. \\(k\\)-means is often used for tasks such as data segmentation, image compression, or anomaly detection. It is simple but may not work well with non-spherical or overlapping clusters.\n\n\n\nBecause all the data are stored in long format, with each London LSOA appearing on multiple rows for each category — such as separate rows for different age groups, ethnicities, countries of birth, and first or preferred languages - we need to transform it into a wide format. For example, instead of having multiple rows for an LSOA showing counts for different age groups all the information for each LSOA will be consolidated into a single row. Additionally, we will clean up the column names to follow standard R naming conventions and make the data easier to work with. Like we have done previously, we can automate this process using the janitor package.\nWe will begin with the age dataframe:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"age_5_categories\",\n        values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions and only retain those columns that are likely to be meaningful in the context of the classification:\n\n\n\nR code\n\n# total observations\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(age_pop = sum(across(2:6)))\n\n# total proportions, select columns\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(2:6, ~./age_pop)) |&gt;\n    select(1:6)\n\n# inspect\nhead(lsoa_age)\n\n\n# A tibble: 6 × 6\n# Rowwise: \n  lower_layer_super_output_areas_code aged_15_years_and_un…¹ aged_16_to_24_years\n  &lt;chr&gt;                                                &lt;dbl&gt;               &lt;dbl&gt;\n1 E01000001                                           0.0846              0.0744\n2 E01000002                                           0.0621              0.0889\n3 E01000003                                           0.0682              0.0706\n4 E01000005                                           0.127               0.178 \n5 E01000006                                           0.224               0.120 \n6 E01000007                                           0.257               0.103 \n# ℹ abbreviated name: ¹​aged_15_years_and_under\n# ℹ 3 more variables: aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;\n\n\nThis looks much better. We can do the same for the country of birth data:\n\n\n\nR code\n\n# prepare country of birth data\nlsoa_cob &lt;- lsoa_cob |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"country_of_birth_8_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_cob &lt;- lsoa_cob |&gt;\n    rowwise() |&gt;\n    mutate(cob_pop = sum(across(2:9))) |&gt;\n    mutate(across(2:9, ~./cob_pop)) |&gt;\n    select(-2, -10)\n\n\nAnd we can do the same for the ethnicity and language datasets:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2, -22)\n\n# prepare language data\nlsoa_lan &lt;- lsoa_lan |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"main_language_11_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_lan &lt;- lsoa_lan |&gt;\n    rowwise() |&gt;\n    mutate(lan_pop = sum(across(2:12))) |&gt;\n    mutate(across(2:12, ~./lan_pop)) |&gt;\n    select(-2, -11, -13)\n\n\nWe now have four separate datasets, each containing the proportions of usual residents classified into different groups based on age, country of birth, ethnicity, and language.\n\n\n\nWhere we initially selected variables from different demographic domains, not all variables may be suitable for inclusion. Firstly, the variables need to exhibit sufficient heterogeneity to ensure they capture meaningful differences between observations. Secondly, variables should not be highly correlated with one another, as this redundancy can skew the clustering results. Ensuring acceptable correlation between variables helps maintain the diversity of information and improves the robustness of the clustering outcome.\n\n\n\n\n\n\nVariable selection is often a time-consuming process that requires a combination of domain knowledge and more extensive exploratory analysis than is covered in this practical.\n\n\n\nA straightforward yet effective method to examine the distribution of our variables is to create boxplots for each variable. This can be efficiently achieved by using facet_wrap() from the ggplot2 library to generate a matrix of panels, allowing us to visualise all variables in a single view.\n\n\n\n\n\n\nggplot2 is a popular data visualisation package in R, designed for creating complex plots. It uses the Grammar of Graphics to build layered, customisable graphics by mapping data to visual elements like colour, size, and shape. We will explore the ggplot2 library further in Weeks 9 and 10. In the meantime, you can refer to the ggplot2 documentation for more details on facet_wrap().\n\n\n\n\n\n\nR code\n\n# wide to long\nlsoa_age_wd &lt;- lsoa_age |&gt;\n    pivot_longer(cols = c(2:5), names_to = \"agegroup\", values_to = \"count\")\n\n# facet age\nggplot(lsoa_age_wd, aes(y = count)) + geom_boxplot() + facet_wrap(~agegroup, ncol = 2) +\n    theme_minimal() + ylab(\"\")\n\n\n\n\n\nFigure 1: Boxplots of the distribution of the age dataset.\n\n\n\n\nWhen repeating this process for the birth, ethnicity, and language variables, you will notice that some variables have a very limited distribution. Specifically, some variables may have a value of 0 for the majority of London LSOAs. As a rule of thumb, we will retain only those variables where at least 75% of the LSOAs have values different from 0.\n\n\n\n\n\n\nThis threshold of 75% is arbitrary, and in practice, more thorough consideration should be given when deciding whether to include or exclude a variable.\n\n\n\n\n\n\nR code\n\n# join\nlsoa_df &lt;- lsoa_age |&gt;\n    left_join(lsoa_cob, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_eth, by = \"lower_layer_super_output_areas_code\") |&gt;\n    left_join(lsoa_lan, by = \"lower_layer_super_output_areas_code\")\n\n# calculate proportion of zeroes\nzero_prop &lt;- sapply(lsoa_df[2:41], function(x) {\n    mean(x == 0)\n})\n\n# extract variables with high proportion zeroes\nidx &lt;- which(zero_prop &gt; 0.25)\n\n# inspect\nidx\n\n\n   white_gypsy_or_irish_traveller            any_other_uk_languages \n                               27                                33 \n  oceanic_or_australian_languages north_or_south_american_languages \n                               37                                38 \n\n# remove variables with high proportion zeroes\nlsoa_df &lt;- lsoa_df |&gt;\n    select(-white_gypsy_or_irish_traveller, -any_other_uk_languages, -oceanic_or_australian_languages,\n        -north_or_south_american_languages)\n\n\n\n\n\n\n\nThe code above makes use of Boolean logic to calculate the proportion of zeroes within each variable. The x == 0 part checks each value in column x to see if it is equal to 0, returning TRUE or FALSE for each element. The mean() function is then used to calculate the average of the TRUE values in the column. Since TRUE is treated as 1 and FALSE as 0, this gives the proportion of values in the column that are equal to zero.\n\n\n\nWe can subsequently check for multicollinearity of the remaining variables. The easiest way to check the correlations between all variables is probably by visualising a correlation matrix:\n\n\n\nR code\n\n# inspect variable names\nnames(lsoa_df)\n\n\n [1] \"lower_layer_super_output_areas_code\"                                  \n [2] \"aged_15_years_and_under\"                                              \n [3] \"aged_16_to_24_years\"                                                  \n [4] \"aged_25_to_34_years\"                                                  \n [5] \"aged_35_to_49_years\"                                                  \n [6] \"aged_50_years_and_over\"                                               \n [7] \"europe_united_kingdom\"                                                \n [8] \"europe_ireland\"                                                       \n [9] \"europe_other_europe\"                                                  \n[10] \"africa\"                                                               \n[11] \"middle_east_and_asia\"                                                 \n[12] \"the_americas_and_the_caribbean\"                                       \n[13] \"antarctica_and_oceania_including_australasia_and_other\"               \n[14] \"asian_asian_british_or_asian_welsh_bangladeshi\"                       \n[15] \"asian_asian_british_or_asian_welsh_chinese\"                           \n[16] \"asian_asian_british_or_asian_welsh_indian\"                            \n[17] \"asian_asian_british_or_asian_welsh_pakistani\"                         \n[18] \"asian_asian_british_or_asian_welsh_other_asian\"                       \n[19] \"black_black_british_black_welsh_caribbean_or_african_african\"         \n[20] \"black_black_british_black_welsh_caribbean_or_african_caribbean\"       \n[21] \"black_black_british_black_welsh_caribbean_or_african_other_black\"     \n[22] \"mixed_or_multiple_ethnic_groups_white_and_asian\"                      \n[23] \"mixed_or_multiple_ethnic_groups_white_and_black_african\"              \n[24] \"mixed_or_multiple_ethnic_groups_white_and_black_caribbean\"            \n[25] \"mixed_or_multiple_ethnic_groups_other_mixed_or_multiple_ethnic_groups\"\n[26] \"white_english_welsh_scottish_northern_irish_or_british\"               \n[27] \"white_irish\"                                                          \n[28] \"white_roma\"                                                           \n[29] \"white_other_white\"                                                    \n[30] \"other_ethnic_group_arab\"                                              \n[31] \"other_ethnic_group_any_other_ethnic_group\"                            \n[32] \"english_or_welsh\"                                                     \n[33] \"european_languages_eu\"                                                \n[34] \"other_european_languages_non_eu\"                                      \n[35] \"asian_languages\"                                                      \n[36] \"african_languages\"                                                    \n[37] \"any_other_languages\"                                                  \n\n# change variable names to index to improve visualisation\nlsoa_df_vis &lt;- lsoa_df\nnames(lsoa_df_vis)[2:37] &lt;- paste0(\"v\", sprintf(\"%02d\", 1:36))\n\n# correlation matrix\ncor_mat &lt;- cor(lsoa_df_vis[, -1])\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\nFigure 2: Correlation plot of classification variables.\n\n\n\n\nFollowing the approach from Wyszomierski et al. (2024), we can define a weak correlation as lying between 0 and 0.40, moderate as between 0.41 and 0.65, strong as between 0.66 and 0.80, and very strong as between 0.81 and 1.\nA few strong and very strong correlations can be observed that potentially could be removed; however, to maintain representation, here we decide to retain all variables.\n\n\n\nIf the input data are heavily skewed or contain outliers, \\(k\\)-means may produce less meaningful clusters. While normality is not required per se, it has been common to do this nonetheless. More important is to standardise the input variables, especially when they are measured on different scales. This ensures that each variable contributes equally to the clustering process.\n\n\n\nR code\n\n# inverse hyperbolic sine\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], asinh)\n\n# range standardise\nlsoa_df_vis[, -1] &lt;- sapply(lsoa_df_vis[-1], function(x) {\n    (x - min(x))/(max(x) - min(x))\n})\n\n\n\n\n\nNow our data are prepared we will start by creating an elbow plot. The elbow method is a visual tool that helps determine the optimal number of clusters in a dataset. This is important because with \\(k\\)-means clustering you need to specify the numbers of clusters a priori. The elbow method involves running the clustering algorithm with varying numbers of clusters (\\(k\\)) and plotting the total explained variation (known as the Within Sum of Squares) against the number of clusters. The goal is to identify the elbow point on the curve, where the rate of decrease in explained variation starts to slow. This point suggests that adding more clusters yields diminishing returns in terms of explained variation.\n\n\n\nR code\n\n# elbow plot\nfviz_nbclust(lsoa_df_vis[, -1], kmeans, nstart = 100, iter.max = 100, method = \"wss\")\n\n\n\n\n\nFigure 3: Elbow plot with Within Sum of Squares against number of clusters.\n\n\n\n\nBased on the elbow plot, we can now choose the number of clusters and it looks like 6 clusters would be a reasonable choice.\n\n\n\n\n\n\nThe interpretation of an elbow plot can be quite subjective, and multiple options for the optimal number of clusters might be justified; for instance, 4, 5, or even 7 clusters could be reasonable choices. In addition to the elbow method, other techniques can aid in determining the optimal number of clusters, such as silhouette scores and the gap statistic. An alternative and helful approach is to use a clustergram, which is a two-dimensional plot that visualises the flows of observations between clusters as more clusters are added. This method illustrates how your data reshuffles with each additional cluster and provides insights into the quality of the splits. This method can be done in R, but currently easier to implement in Python.\n\n\n\n\n\n\nNow we have decided on the number of clusters, we can run our \\(k\\)-means analysis.\n\n\n\nR code\n\n# set seed for reproducibility\nset.seed(999)\n\n# k-means\nlsoa_clus &lt;- kmeans(lsoa_df_vis[, -1], centers = 6, nstart = 100, iter.max = 100)\n\n\nWe can inspect the object to get some information about our clusters:\n\n\n\nR code\n\n# inspect\nlsoa_clus\n\n\nK-means clustering with 6 clusters of sizes 796, 1097, 771, 1011, 851, 468\n\nCluster means:\n        v01       v02       v03       v04       v05       v06       v07\n1 0.4816225 0.1632210 0.2425566 0.4838983 0.4169123 0.5410477 0.1337158\n        v08       v09       v10        v11        v12        v13        v14\n1 0.3007540 0.2480613 0.2859754 0.08913663 0.05177222 0.14603013 0.06993627\n         v15        v16        v17        v18        v19        v20        v21\n1 0.12176548 0.10935022 0.20109979 0.18150482 0.11605092 0.12757934 0.09288236\n         v22        v23       v24       v25       v26        v27       v28\n1 0.07473711 0.14662903 0.1842217 0.3522638 0.1490577 0.02997040 0.2423784\n         v29       v30       v31       v32        v33        v34        v35\n1 0.07148504 0.2193009 0.5870244 0.2411272 0.07541661 0.23467242 0.10174187\n         v36\n1 0.10216507\n [ reached getOption(\"max.print\") -- omitted 5 rows ]\n\nClustering vector:\n [1] 4 4 4 1 1 5 5 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 2 1 1 2 2 2 1 1 1\n[39] 1 1 1 1 5 1 5 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4944 entries ]\n\nWithin cluster sum of squares by cluster:\n[1] 259.0272 177.7951 288.8625 232.7770 298.9145 160.1702\n (between_SS / total_SS =  48.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\nWe now need to perform some post-processing to extract useful summary data for each cluster. To characterise the clusters, we can compare the global mean values of each variable with the mean values specific to each cluster.\n\n\n\nR code\n\n# global means\nglob_means &lt;- colMeans(lsoa_df_vis[, -1])\n\n# add clusters to input data\nlsoa_df_vis &lt;- cbind(lsoa_df_vis, cluster = lsoa_clus$cluster)\n\n# cluster means\ncluster_means &lt;- lsoa_df_vis |&gt;\n    group_by(cluster) |&gt;\n    summarise(across(2:37, mean))\n\n# difference\ncluster_diffs &lt;- cluster_means |&gt;\n    mutate(across(2:37, ~. - glob_means[cur_column()]))\n\n\nThese comparisons can then be visualised using, for instance, a radial bar plot:\n\n\n\nR code\n\n# to long format\ncluster_diffs_long &lt;- cluster_diffs |&gt;\n    pivot_longer(!cluster, names_to = \"vars\", values_to = \"score\")\n\n# facet clusters\nggplot(cluster_diffs_long, aes(x = factor(vars), y = score)) + geom_bar(stat = \"identity\") +\n    coord_radial(rotate.angle = TRUE, expand = FALSE) + facet_wrap(~cluster, ncol = 3) +\n    theme_minimal() + theme(axis.text.x = element_text(size = 7)) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\nFigure 4: Radial barplots of cluster means for each input variable.\n\n\n\n\nThese plots can serve as a foundation for creating pen portraits by closely examining which variables drive each cluster.\n\n\n\n\n\n\nFor easier interpretation, these values can be transformed into index scores, allowing us to assess which variables are under- or overrepresented within each cluster group.\n\n\n\nOf course, we can also map the results:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join\nlsoa21 &lt;- cbind(lsoa21, cluster = lsoa_clus$cluster)\n\n# shape, polygon\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"cluster\",\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Cluster number\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 5: Classification of London LSOAs based on several demographic variables."
  },
  {
    "objectID": "07-geodemographics.html#assignment",
    "href": "07-geodemographics.html#assignment",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "The creation of a geodemographic classification is an iterative process. This typically includes adding or removing variables, adjusting the number of clusters, and grouping data in different ways to achieve the most meaningful segmentation. Try to do the following:\n\nDownload the two datasets provided below and save them to your data folder. The datasets include:\n\nA csv file containing the number of people aged 16 years and older by occupational category, as defined by the Standard Occupational Classification 2020, aggregated by 2021 LSOAs.\nA csv file containing the number of people aged 16 years and older by their highest level of qualification, also aggregated to the 2021 LSOA level.\n\nPrepare these two datasets and retain only those variables that are potentially meaningful. Filter out any variables with a high proportion of zero values.\nMerge the education and occupation dataset with the dataset used to generate the initial geodemographic classification. Check for multicollinearity and consider removing any variables that are highly correlated.\nPerform \\(k\\)-means clustering on your extended dataset. Make sure to select an appropriate number of clusters for your analysis.\nInterpret the individual clusters in terms of the variables that are under- and overrepresented.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Occupation\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Education\ncsv\nDownload"
  },
  {
    "objectID": "07-geodemographics.html#before-you-leave",
    "href": "07-geodemographics.html#before-you-leave",
    "title": "1 Geodemographic Classification",
    "section": "",
    "text": "Having finished this tutorial, you should now understand the basics of a geodemographic classification. That is all for this week!"
  },
  {
    "objectID": "06-raster.html",
    "href": "06-raster.html",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "So far, we have exclusively focused on the use of vector and tabular data. However, depending on the nature of your research problem, you may also encounter raster data. This week’s content introduces you to raster data, map algebra, and interpolation.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: A good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]\nPark, G. and Franklin, R. 2023. The changing demography of hurricane at-risk areas in the United States (1970–2018). Population, Space and Place 29(6): e2683. [Link]\n\n\n\n\n\nFor the first part of this week’s practical material we will be using raster datasets from WorldPop. These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These datasets can be used to explore, for example, changes in the demographic profiles or area deprivation at small spatial scales.\n\n\n\n\n\n\nThe key difference between vector and raster models lies in their structure. Vectors are made up of points, lines, and polygons. In contrast, raster data consists of pixels (or grid cells), similar to an image. Each cell holds a single value representing a geographic phenomenon, such as population density at that location. Common raster data types include remote sensing imagery, such as satellite or LIDAR data.\n\n\n\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files to your computer in your data folder.\n\n\n\n\n\n\n\nA GeoTIFF is a type of raster file format that embeds geographic information, enabling the image to be georeferenced to specific real-world coordinates. It includes metadata like projection, coordinate system, and geographic extent, making it compatible with GIS software for spatial analysis.\n\n\n\nTo focus the analysis on London, we need to clip our dataset to the boundaries of the city. For this, we will use the London Borough boundaries, which can be downloaded from the link below. Be sure to save the files in the data folder within your data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w06-raster-data-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. We can load the individual GeoTiff files that we downloaded into R and reproject them into British National Grid using the terra library.\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/spatial/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/spatial/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"EPSG:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"EPSG:27700\")\n\n\nCarefully examine each dataframe to understand its structure and the information it contains:\n\n\n\nR code\n\n# inspect 2010 data\nhead(pop2010)\n\n\n  gbr_ppp_2010_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n# inspect 2020 data\nhead(pop2020)\n\n  gbr_ppp_2020_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n\n\n\n\n\n\n\nA raster file is always rectangular, with areas lacking data stored as NA. For our population data, this means any pixels outside the land borders of Great Britain will have by definition an NA value.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can also plot the raster files for visual inspection:\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 1: WorldPop 2010 population estimates for the United Kingdom.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 2: WorldPop 2020 population estimates for the United Kingdom.\n\n\n\n\nYou will notice that while the maps appear similar, the legend indicates a significant increase in values over the decade from 2010 to 2021, with the maximum rising from approximately 12,000 people per cell to over 14,000.\nNow that we have our raster data loaded, we will focus on reducing it to display only the extent of London. We will use the London borough GeoPackage\n\n\n\n\n\n\nThe terra package does not accept sf objects, so after loading the London borough boundaries, we need to convert the file into a SpatRaster or SpatVector.\n\n\n\n\n\n\nR code\n\n# load data, to spatvector\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n    vect()\n\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# crop to extent\npop2010_london &lt;- crop(pop2010, borough)\npop2020_london &lt;- crop(pop2020, borough)\n\n# mask to boundaries\npop2010_london &lt;- mask(pop2010_london, borough)\npop2020_london &lt;- mask(pop2020_london, borough)\n\nWe should now have the raster cells that fall within the boundaries of London:\n\n\n\nR code\n\n# inspect\nplot(pop2010_london)\n\n\n\n\n\nFigure 3: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# inspect\nplot(pop2020_london)\n\n\n\n\n\nFigure 4: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 5: Population change in London 2010-2020.\n\n\n\n\n\n\n\nTo further analyse our population change raster, we can create a smoothed version of the lonpop_change raster using the focal() function. This function generates a raster that calculates the average (mean) value of the nearest neighbours for each cell.\n\n\n\nR code\n\n# smooth\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 6: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences may not be immediately apparent, but if you subtract the smoothed raster from the original raster, you will clearly see that changes have occurred.\n\n\n\nR code\n\n# substract\nlonpop_chang_smooth &lt;- lonpop_change - lonpop_smooth\n\n# inspect\nplot(lonpop_chang_smooth)\n\n\n\n\n\nFigure 7: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also use zonal functions to better represent population change by aggregating the data to coarser resolutions. For example, resizing the raster’s spatial resolution to contain larger grid cells simplifies the data, making broader trends more visible. However,it may also end up obfuscating more local patterns.\n\n\n\n\n\n\nWe can resize a raster using the aggregate() function, setting the factor parameter to the scale of resampling desired (e.g. doubling both the width and height of a cell). The function parameter determines how to aggregate the data.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# inspect\nplot(lonpop_agg)\n\n\n\n\n\nFigure 8: Aggregated cell values.\n\n\n\n\nWe can also aggregate raster cells to vector geographies. For example, we can aggregate the WorldPop gridded population estimates to the London borough boundaries:\n\n# aggregate\nlondon_borough_pop &lt;- extract(lonpop_change, borough, fun = sum)\n\n# bind to spatial boundaries\nborough &lt;- borough |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_borough_pop$gbr_ppp_2020_1km_Aggregated)\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"pop_change\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#0570b0\"),\n    title = \"\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 9: Absolute population change in London boroughs 2010-2020.\n\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have a vector dataset, which allows us to perform many of the analyses we have explored in previous weeks.\n\n\n\n\n\n\nCalculating population change, particularly over decades as we have done, can be challenging due to changes in administrative boundaries. Using raster data offers a helpful workaround, provided the rasters are of consistent size and extent.\n\n\n\n\n\n\n\nIn the second part of this week’s practical, we will explore various methods of spatial data interpolation, focusing on air pollution in London using data from Londonair. We will specifically look at Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nLondonair is the website of the London Air Quality Network (LAQN), which provides air pollution data for London and southeast England through the Environmental Research Group at Imperial College This data is publicly available and can be accessed directly using the openair R package, without needing to download files.\n\n\n\n\n\n\n\n\n\nSpatial interpolation predicts a phenomenon at unmeasured locations. It is often used when we want to estimate a variable across space, particularly in areas with sparse or no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2023:2023)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2023:2023, pollutant = \"no2\",\n    meta = TRUE)\n\n\n\n\n\n\n\n\nNot all measurements sites collect data on NO2 so it is normal to get some 404 Not Found warnings.\n\n\n\n\n\n\n\n\n\nThis code may take some time to run, as it will attempt to download data from all air measurement sites for an entire year, with many measurements taken hourly. If you experience too many errors or if it is taking too long, you can download a copy of the data here: [Download]. Once downloaded, place the zip file in your data folder. The file is large, so you can leave it unzipped.\n\n\n\nLet us start by loading and inspecting the data:\n\n\n\nR code\n\n# load from zip if downloaded through the link\npollution &lt;- read_csv(\"data/attributes/London-NO2-2023.zip\")\n\n\nMultiple files in zip: reading 'London-Pollution-2023.csv'\nRows: 1615976 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site       code  source latitude longitude site_type\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 2023-01-01 00:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n2 2023-01-01 01:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n3 2023-01-01 02:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n4 2023-01-01 03:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n5 2023-01-01 04:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n6 2023-01-01 05:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n\n\nIn the first five rows, we can see data from the same site, with the date field showing an observation for every hour. Given there are 24 hours in a day, 365 days in a year, and data from hundreds of sites, it is no surprise that the dataset is so large. To make the dataset more manageable, let us summarise the values by site.\n\n\n\nR code\n\n# mean site values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(longitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56  15.7 \n2 BG1       51.6     0.178 16.4 \n3 BG2       51.5     0.133 18.4 \n4 BH0       50.8    -0.148 10.8 \n5 BK0       53.8    -3.01   4.80\n6 BL0       51.5    -0.126 24.0 \n\n\nWe now have 177 measurement sites with their corresponding latitudes, longitudes, and average NO2 values. Let us have a look at the spatial distribution of these measurement sites.\n\n# load boroughs for background\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n  st_union()\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# clip measurement sites to london boundaries\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify colours\n  tm_symbols(\n    col = \"#fc9272\",\n    size = 0.3,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Measurement site\",\n    col = \"#fc9272\",\n    size = 0.5\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 10: KCL NO2 measurement sites in London.\n\n\n\n\nWe can also use proportional symbols to visualise the values, helping us observe how measurements vary across London.\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column\n  tm_bubbles(\n    size = \"no2\",\n    title.size = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 11: Proportional symbol map of average KCL NO2 measurement in London.\n\n\n\n\nFigure 11 shows heterogeneity in average NO2 measurements across London, both in terms of coverage and NO2 levels. To make reasonable assumptions about NO2 levels in areas without measurements, we can interpolate the missing values.\n\n\nA straightforward method for interpolating values across space is to create a Voronoi tessellation polygons. These polygons define the boundaries of areas closest to each unique point, meaning that each point in the dataset has a corresponding polygon.\n\n\n\n\n\n\nIn addition to Voronoi tessellation, you may encounter the term Thiessen polygons. These terms are often used interchangeably to describe the geometry created from point data.\n\n\n\n\n\n\nR code\n\n# function\nst_voronoi_points &lt;- function(points) {\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to voronoi\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# voronoi tessellation\nmeasurement_sites_voronoi &lt;- st_voronoi_points(measurement_sites)\n\n# replace point geometry with polygon geometry\nmeasurement_sites_tesselation &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_voronoi) |&gt;\n    st_intersection(borough)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nmeasurement_sites_tesselation\n\nSimple feature collection with 98 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 98 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.4 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    18.4 POLYGON ((545264.9 181815.5, 545402.2 184801.2, 547703.4 186697.…\n 3 BL0    24.0 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    13.9 POLYGON ((546306.3 181181, 549837.9 181565.4, 548349.8 175998.9,…\n 5 BT4    38.9 POLYGON ((519912.6 183741.2, 516082 187365.6, 520958.9 189405.3,…\n 6 BT5    24.9 POLYGON ((523877.3 190896.9, 524273.2 190424, 524781.6 189259.6,…\n 7 BT6    24.6 POLYGON ((521236.1 184358.2, 522982.8 184472, 522330 181976.4, 5…\n 8 BT8    23.3 POLYGON ((522982.8 184472, 524217.5 185711.1, 525595.2 182818.7,…\n 9 BX1    15.7 MULTIPOLYGON (((550125.4 169173.1, 550132.2 169161.6, 550144.3 1…\n10 BX2    15.4 POLYGON ((548349.8 175998.9, 549837.9 181565.4, 550403.4 181822.…\n# ℹ 88 more rows\n\n\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function; just know that it takes a point spatial data frame as input and produces a tessellated spatial data frame as output.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now visualise the results of the interpolation:\n\n# shape, polygon\ntm_shape(measurement_sites_tesselation) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"no2\",\n    palette = c(\"#ffffcc\", \"#c2e699\", \"#78c679\", \"#0570b0\"),\n    title = \"Average reading\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 12: Interpolation of average NO2 measurements in London using a Voronoi tessellation.\n\n\n\n\n\n\n\nA more sophisticated method for interpolating point data is Inverse Distance Weighting (IDW). IDW converts numerical point data into a continuous surface, allowing for visualisation of how the data is distributed across space. This technique estimates values at each location by calculating a weighted average from nearby points, with the weights inversely related to their distances.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before running the IDW.\n\n\n\nR code\n\n# create regular output grid\noutput_grid &lt;- borough |&gt;\n    st_make_grid(cellsize = c(1000, 1000))\n\n# execute\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    idp = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\nThe IDW interpolation may take some time to run because it involves calculating the weighted average of nearby points for each location on the grid. In this case, idp = 2 specifies a quadratic decay, meaning the influence of a point decreases with the square of the distance.\n\n\n\nAgain, we can map the results for visual inspection.\n\n\n\n\n\n\nThe values of the IDW output are stored in the raster grid as var1.pred.\n\n\n\n\n# shape, polygon\ntm_shape(measurement_sites_idw) +\n\n  # specify column, classes\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"cont\",\n    palette = \"Oranges\",\n    title = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 13: Interpolation of average NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have set the output cell size to 1000x1000 metres. While a smaller cell size can yield a smoother IDW output, it may introduce uncertainty due to the limited number of data points available for interpolation. Moreover, reducing the cell size will exponentially increase processing time.\n\n\n\n\n\n\n\nHaving run through all the steps during the tutorial, we can conduct some more granular analysis of the NO2 measurements. For example, instead of examining the annual average measurements, we could compare data across different months. Please try the following tasks:\n\nCreate monthly averages for the pollution data.\nFor both June and December, generate a dataframe containing the London monitoring sites along with their average NO₂ readings for these months.\nPerform Inverse Distance Weighting (IDW) interpolation for the data from both months.\nCombine the results to assess the differences between these months.\n\n\n\n\nThis week, we have explored raster datasets and how to manage and process them using the terra library. While you will typically encounter vector data, particularly in relation to government statistics and administrative boundaries, there are also many use cases where raster data may be encountered. With that being said: that is it for this week!"
  },
  {
    "objectID": "06-raster.html#lecture-slides",
    "href": "06-raster.html#lecture-slides",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "06-raster.html#reading-list",
    "href": "06-raster.html#reading-list",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "Gimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: A good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]\nPark, G. and Franklin, R. 2023. The changing demography of hurricane at-risk areas in the United States (1970–2018). Population, Space and Place 29(6): e2683. [Link]"
  },
  {
    "objectID": "06-raster.html#population-change-in-london",
    "href": "06-raster.html#population-change-in-london",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "For the first part of this week’s practical material we will be using raster datasets from WorldPop. These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These datasets can be used to explore, for example, changes in the demographic profiles or area deprivation at small spatial scales.\n\n\n\n\n\n\nThe key difference between vector and raster models lies in their structure. Vectors are made up of points, lines, and polygons. In contrast, raster data consists of pixels (or grid cells), similar to an image. Each cell holds a single value representing a geographic phenomenon, such as population density at that location. Common raster data types include remote sensing imagery, such as satellite or LIDAR data.\n\n\n\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files to your computer in your data folder.\n\n\n\n\n\n\n\nA GeoTIFF is a type of raster file format that embeds geographic information, enabling the image to be georeferenced to specific real-world coordinates. It includes metadata like projection, coordinate system, and geographic extent, making it compatible with GIS software for spatial analysis.\n\n\n\nTo focus the analysis on London, we need to clip our dataset to the boundaries of the city. For this, we will use the London Borough boundaries, which can be downloaded from the link below. Be sure to save the files in the data folder within your data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w06-raster-data-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. We can load the individual GeoTiff files that we downloaded into R and reproject them into British National Grid using the terra library.\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/spatial/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/spatial/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"EPSG:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"EPSG:27700\")\n\n\nCarefully examine each dataframe to understand its structure and the information it contains:\n\n\n\nR code\n\n# inspect 2010 data\nhead(pop2010)\n\n\n  gbr_ppp_2010_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n# inspect 2020 data\nhead(pop2020)\n\n  gbr_ppp_2020_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n\n\n\n\n\n\n\n\nA raster file is always rectangular, with areas lacking data stored as NA. For our population data, this means any pixels outside the land borders of Great Britain will have by definition an NA value.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can also plot the raster files for visual inspection:\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 1: WorldPop 2010 population estimates for the United Kingdom.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 2: WorldPop 2020 population estimates for the United Kingdom.\n\n\n\n\nYou will notice that while the maps appear similar, the legend indicates a significant increase in values over the decade from 2010 to 2021, with the maximum rising from approximately 12,000 people per cell to over 14,000.\nNow that we have our raster data loaded, we will focus on reducing it to display only the extent of London. We will use the London borough GeoPackage\n\n\n\n\n\n\nThe terra package does not accept sf objects, so after loading the London borough boundaries, we need to convert the file into a SpatRaster or SpatVector.\n\n\n\n\n\n\nR code\n\n# load data, to spatvector\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n    vect()\n\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# crop to extent\npop2010_london &lt;- crop(pop2010, borough)\npop2020_london &lt;- crop(pop2020, borough)\n\n# mask to boundaries\npop2010_london &lt;- mask(pop2010_london, borough)\npop2020_london &lt;- mask(pop2020_london, borough)\n\nWe should now have the raster cells that fall within the boundaries of London:\n\n\n\nR code\n\n# inspect\nplot(pop2010_london)\n\n\n\n\n\nFigure 3: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# inspect\nplot(pop2020_london)\n\n\n\n\n\nFigure 4: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 5: Population change in London 2010-2020.\n\n\n\n\n\n\n\nTo further analyse our population change raster, we can create a smoothed version of the lonpop_change raster using the focal() function. This function generates a raster that calculates the average (mean) value of the nearest neighbours for each cell.\n\n\n\nR code\n\n# smooth\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# inspect\nplot(lonpop_change)\n\n\n\n\n\nFigure 6: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences may not be immediately apparent, but if you subtract the smoothed raster from the original raster, you will clearly see that changes have occurred.\n\n\n\nR code\n\n# substract\nlonpop_chang_smooth &lt;- lonpop_change - lonpop_smooth\n\n# inspect\nplot(lonpop_chang_smooth)\n\n\n\n\n\nFigure 7: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also use zonal functions to better represent population change by aggregating the data to coarser resolutions. For example, resizing the raster’s spatial resolution to contain larger grid cells simplifies the data, making broader trends more visible. However,it may also end up obfuscating more local patterns.\n\n\n\n\n\n\nWe can resize a raster using the aggregate() function, setting the factor parameter to the scale of resampling desired (e.g. doubling both the width and height of a cell). The function parameter determines how to aggregate the data.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# inspect\nplot(lonpop_agg)\n\n\n\n\n\nFigure 8: Aggregated cell values.\n\n\n\n\nWe can also aggregate raster cells to vector geographies. For example, we can aggregate the WorldPop gridded population estimates to the London borough boundaries:\n\n# aggregate\nlondon_borough_pop &lt;- extract(lonpop_change, borough, fun = sum)\n\n# bind to spatial boundaries\nborough &lt;- borough |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_borough_pop$gbr_ppp_2020_1km_Aggregated)\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"pop_change\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#0570b0\"),\n    title = \"\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 9: Absolute population change in London boroughs 2010-2020.\n\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have a vector dataset, which allows us to perform many of the analyses we have explored in previous weeks.\n\n\n\n\n\n\nCalculating population change, particularly over decades as we have done, can be challenging due to changes in administrative boundaries. Using raster data offers a helpful workaround, provided the rasters are of consistent size and extent."
  },
  {
    "objectID": "06-raster.html#air-pollution-in-london",
    "href": "06-raster.html#air-pollution-in-london",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "In the second part of this week’s practical, we will explore various methods of spatial data interpolation, focusing on air pollution in London using data from Londonair. We will specifically look at Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nLondonair is the website of the London Air Quality Network (LAQN), which provides air pollution data for London and southeast England through the Environmental Research Group at Imperial College This data is publicly available and can be accessed directly using the openair R package, without needing to download files.\n\n\n\n\n\n\n\n\n\nSpatial interpolation predicts a phenomenon at unmeasured locations. It is often used when we want to estimate a variable across space, particularly in areas with sparse or no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2023:2023)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2023:2023, pollutant = \"no2\",\n    meta = TRUE)\n\n\n\n\n\n\n\n\nNot all measurements sites collect data on NO2 so it is normal to get some 404 Not Found warnings.\n\n\n\n\n\n\n\n\n\nThis code may take some time to run, as it will attempt to download data from all air measurement sites for an entire year, with many measurements taken hourly. If you experience too many errors or if it is taking too long, you can download a copy of the data here: [Download]. Once downloaded, place the zip file in your data folder. The file is large, so you can leave it unzipped.\n\n\n\nLet us start by loading and inspecting the data:\n\n\n\nR code\n\n# load from zip if downloaded through the link\npollution &lt;- read_csv(\"data/attributes/London-NO2-2023.zip\")\n\n\nMultiple files in zip: reading 'London-Pollution-2023.csv'\nRows: 1615976 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site       code  source latitude longitude site_type\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 2023-01-01 00:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n2 2023-01-01 01:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n3 2023-01-01 02:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n4 2023-01-01 03:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n5 2023-01-01 04:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n6 2023-01-01 05:00:00    NA City of L… CTA   kcl        51.5   -0.0921 Roadside \n\n\nIn the first five rows, we can see data from the same site, with the date field showing an observation for every hour. Given there are 24 hours in a day, 365 days in a year, and data from hundreds of sites, it is no surprise that the dataset is so large. To make the dataset more manageable, let us summarise the values by site.\n\n\n\nR code\n\n# mean site values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(longitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56  15.7 \n2 BG1       51.6     0.178 16.4 \n3 BG2       51.5     0.133 18.4 \n4 BH0       50.8    -0.148 10.8 \n5 BK0       53.8    -3.01   4.80\n6 BL0       51.5    -0.126 24.0 \n\n\nWe now have 177 measurement sites with their corresponding latitudes, longitudes, and average NO2 values. Let us have a look at the spatial distribution of these measurement sites.\n\n# load boroughs for background\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\") |&gt;\n  st_union()\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# clip measurement sites to london boundaries\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify colours\n  tm_symbols(\n    col = \"#fc9272\",\n    size = 0.3,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Measurement site\",\n    col = \"#fc9272\",\n    size = 0.5\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 10: KCL NO2 measurement sites in London.\n\n\n\n\nWe can also use proportional symbols to visualise the values, helping us observe how measurements vary across London.\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column\n  tm_bubbles(\n    size = \"no2\",\n    title.size = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\nFigure 11: Proportional symbol map of average KCL NO2 measurement in London.\n\n\n\n\nFigure 11 shows heterogeneity in average NO2 measurements across London, both in terms of coverage and NO2 levels. To make reasonable assumptions about NO2 levels in areas without measurements, we can interpolate the missing values.\n\n\nA straightforward method for interpolating values across space is to create a Voronoi tessellation polygons. These polygons define the boundaries of areas closest to each unique point, meaning that each point in the dataset has a corresponding polygon.\n\n\n\n\n\n\nIn addition to Voronoi tessellation, you may encounter the term Thiessen polygons. These terms are often used interchangeably to describe the geometry created from point data.\n\n\n\n\n\n\nR code\n\n# function\nst_voronoi_points &lt;- function(points) {\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to voronoi\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# voronoi tessellation\nmeasurement_sites_voronoi &lt;- st_voronoi_points(measurement_sites)\n\n# replace point geometry with polygon geometry\nmeasurement_sites_tesselation &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_voronoi) |&gt;\n    st_intersection(borough)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nmeasurement_sites_tesselation\n\nSimple feature collection with 98 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 98 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.4 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    18.4 POLYGON ((545264.9 181815.5, 545402.2 184801.2, 547703.4 186697.…\n 3 BL0    24.0 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    13.9 POLYGON ((546306.3 181181, 549837.9 181565.4, 548349.8 175998.9,…\n 5 BT4    38.9 POLYGON ((519912.6 183741.2, 516082 187365.6, 520958.9 189405.3,…\n 6 BT5    24.9 POLYGON ((523877.3 190896.9, 524273.2 190424, 524781.6 189259.6,…\n 7 BT6    24.6 POLYGON ((521236.1 184358.2, 522982.8 184472, 522330 181976.4, 5…\n 8 BT8    23.3 POLYGON ((522982.8 184472, 524217.5 185711.1, 525595.2 182818.7,…\n 9 BX1    15.7 MULTIPOLYGON (((550125.4 169173.1, 550132.2 169161.6, 550144.3 1…\n10 BX2    15.4 POLYGON ((548349.8 175998.9, 549837.9 181565.4, 550403.4 181822.…\n# ℹ 88 more rows\n\n\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function; just know that it takes a point spatial data frame as input and produces a tessellated spatial data frame as output.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now visualise the results of the interpolation:\n\n# shape, polygon\ntm_shape(measurement_sites_tesselation) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"no2\",\n    palette = c(\"#ffffcc\", \"#c2e699\", \"#78c679\", \"#0570b0\"),\n    title = \"Average reading\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 12: Interpolation of average NO2 measurements in London using a Voronoi tessellation.\n\n\n\n\n\n\n\nA more sophisticated method for interpolating point data is Inverse Distance Weighting (IDW). IDW converts numerical point data into a continuous surface, allowing for visualisation of how the data is distributed across space. This technique estimates values at each location by calculating a weighted average from nearby points, with the weights inversely related to their distances.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before running the IDW.\n\n\n\nR code\n\n# create regular output grid\noutput_grid &lt;- borough |&gt;\n    st_make_grid(cellsize = c(1000, 1000))\n\n# execute\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    idp = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(borough)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\nThe IDW interpolation may take some time to run because it involves calculating the weighted average of nearby points for each location on the grid. In this case, idp = 2 specifies a quadratic decay, meaning the influence of a point decreases with the square of the distance.\n\n\n\nAgain, we can map the results for visual inspection.\n\n\n\n\n\n\nThe values of the IDW output are stored in the raster grid as var1.pred.\n\n\n\n\n# shape, polygon\ntm_shape(measurement_sites_idw) +\n\n  # specify column, classes\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"cont\",\n    palette = \"Oranges\",\n    title = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\nFigure 13: Interpolation of average NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have set the output cell size to 1000x1000 metres. While a smaller cell size can yield a smoother IDW output, it may introduce uncertainty due to the limited number of data points available for interpolation. Moreover, reducing the cell size will exponentially increase processing time."
  },
  {
    "objectID": "06-raster.html#assignment",
    "href": "06-raster.html#assignment",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "Having run through all the steps during the tutorial, we can conduct some more granular analysis of the NO2 measurements. For example, instead of examining the annual average measurements, we could compare data across different months. Please try the following tasks:\n\nCreate monthly averages for the pollution data.\nFor both June and December, generate a dataframe containing the London monitoring sites along with their average NO₂ readings for these months.\nPerform Inverse Distance Weighting (IDW) interpolation for the data from both months.\nCombine the results to assess the differences between these months."
  },
  {
    "objectID": "06-raster.html#before-you-leave",
    "href": "06-raster.html#before-you-leave",
    "title": "1 Raster Data Analysis",
    "section": "",
    "text": "This week, we have explored raster datasets and how to manage and process them using the terra library. While you will typically encounter vector data, particularly in relation to government statistics and administrative boundaries, there are also many use cases where raster data may be encountered. With that being said: that is it for this week!"
  },
  {
    "objectID": "02-operations.html",
    "href": "02-operations.html",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "This week, we look at geometric operations and spatial queries: the fundamental building blocks when it comes to spatial data processing and analysis. This includes operations such as aggregating point data, calculating the distances separating one or more spatial objects, running a buffer analysis, and intersecting different spatial layers.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 13: Spatial Data Analysis, pp. 290-318. [Link]\n\n\n\n\n\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]\n\n\n\n\n\nThis week, we will examine to what extent reported bicycle theft in London cluster around train and underground stations. We will be using open data from data.police.uk on reported crimes alongside OpenStreetMap data for this analysis. We will use R to directly download the necessary data from OpenStreetMap, but the crime data will need to be manually downloaded from the data portal. We further have access to a GeoPackage that contains the London 2021 MSOA boundaries that we can use as reference layer. If you do not already have it on your computer, save this file in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\nThe UK Police Data Portal allows you to access and generate tabular data for crime recorded in the UK across the different police forces. To download recorded crime data for London:\n\nNavigate to data.police.uk and click on Downloads.\nUnder the data range select January 2023 to December 2023.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave the other settings unchanged and click on Generate file.\n\n\n\n\n\n\nFigure 1: Downloading data on reported crimes through data.police.uk\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the dataset.\nAfter downloading, unzip the file. You will find that the zip file contains 12 folders, one for each month of 2023. Each folder includes two files: one for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named London-Crime within your data/attributes directory, and copy all 12 folders with the data into this new folder.\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w02-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAlthough we could read each individual crime file into R one by one and then combine them, we can actually accomplish this in a single step:\n\n\n\nR code\n\n# list all csv files\ncrime_df &lt;- list.files(path = \"data/attributes/London-Crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # read individual csv files\n  lapply(read_csv) |&gt;\n  # bind together into one\n  bind_rows()\n\n# inspect\nhead(crime_df)\n\n\n# A tibble: 6 × 12\n  `Crime ID`      Month `Reported by` `Falls within` Longitude Latitude Location\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 4a14d4745da0a2… 2023… City of Lond… City of Londo…    -0.106     51.5 On or n…\n2 e6e32581c99c5b… 2023… City of Lond… City of Londo…    -0.107     51.5 On or n…\n3 7b7cb8e7debe8b… 2023… City of Lond… City of Londo…    -0.110     51.5 On or n…\n4 f7fc44e1e76332… 2023… City of Lond… City of Londo…    -0.108     51.5 On or n…\n5 8083dafd1770af… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n6 4587239a45f0e8… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n# ℹ 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime type` &lt;chr&gt;,\n#   `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;\n\n\n\n\n\n\n\n\nDepending on your computer, processing this data may take some time due to the large volume involved. Once completed, you should have a dataframe containing 1,144,329 observations.\n\n\n\n\n\n\n\n\n\nYou can further inspect the object using the View() function.\n\n\n\nThe column names contain spaces and are therefore not easily referenced. We can easily clean this up using the janitor package:\n\n\n\nR code\n\n# clean names\ncrime_df &lt;- crime_df |&gt;\n    clean_names()\n\n# inspect\nnames(crime_df)\n\n\n [1] \"crime_id\"              \"month\"                 \"reported_by\"          \n [4] \"falls_within\"          \"longitude\"             \"latitude\"             \n [7] \"location\"              \"lsoa_code\"             \"lsoa_name\"            \n[10] \"crime_type\"            \"last_outcome_category\" \"context\"              \n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nFor our analysis, we are currently only interested in reported bicycle thefts, so we need to filter our data based on the crime_type column. We can start by examining the unique values in this column and then subset the data accordingly:\n\n\n\nR code\n\n# unique types\nunique(crime_df$crime_type)\n\n\n [1] \"Other theft\"                  \"Other crime\"                 \n [3] \"Theft from the person\"        \"Public order\"                \n [5] \"Anti-social behaviour\"        \"Burglary\"                    \n [7] \"Criminal damage and arson\"    \"Drugs\"                       \n [9] \"Shoplifting\"                  \"Vehicle crime\"               \n[11] \"Violence and sexual offences\" \"Bicycle theft\"               \n[13] \"Robbery\"                      \"Possession of weapons\"       \n\n# filter\ntheft_bike &lt;- crime_df |&gt;\n    filter(crime_type == \"Bicycle theft\")\n\n# inspect\nhead(theft_bike)\n\n# A tibble: 6 × 12\n  crime_id  month reported_by falls_within longitude latitude location lsoa_code\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1 62b0f525… 2023… City of Lo… City of Lon…   -0.0916     51.5 On or n… E01000002\n2 9a078d63… 2023… City of Lo… City of Lon…   -0.0952     51.5 On or n… E01032739\n3 f175a32e… 2023… City of Lo… City of Lon…   -0.0872     51.5 On or n… E01032739\n4 137ec120… 2023… City of Lo… City of Lon…   -0.0783     51.5 On or n… E01032739\n5 4c3b4677… 2023… City of Lo… City of Lon…   -0.108      51.5 On or n… E01032740\n6 13b5eb5c… 2023… City of Lo… City of Lon…   -0.0980     51.5 On or n… E01032740\n# ℹ 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;,\n#   last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;\n\n\nNow that we have filtered the data to only include reported bicycle thefts, we need to convert our dataframe into a spatial dataframe that maps the locations of the crimes using the recorded latitude and longitude coordinates. We can then project this spatial dataframe into the British National Grid (EPSG:27700).\n\n\n\nR code\n\n# to spatial data\ntheft_bike &lt;- theft_bike |&gt;\n    filter(!is.na(longitude) & !is.na(latitude)) |&gt;\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4236) |&gt;\n    st_transform(27700)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 4 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# london outline\noutline &lt;- msoa21 |&gt;\n  st_union()\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify colours\n  tm_dots(\n    col = \"#fdc086\",\n    size = 0.05,\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\nFigure 2: Reported bicycle thefts in London.\n\n\n\n\nWe can save the prepared dataset as a GeoPackage so that we can use it some other time:\n\n\n\nR code\n\n# write\nst_write(theft_bike, \"data/spatial/London-BicycleTheft-2023.gpkg\")\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world. Each map element (whether a point, line, or polygon) in OSM is tagged with various attribute data. To download the station data we need, we must use the appropriate tags, represented as key and value pairs, to query the OSM database. In our case, we are looking for train stations, which fall under the Public Transport key, with a value of station. To limit our search to London, we can use the spatial extent of the 2021 MSOA boundaries as the bounding box for data extraction.\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_london &lt;- msoa21 |&gt;\n  st_transform(4326) |&gt;\n  st_bbox()\n\n# osm query\nosm_stations &lt;- opq(bbox = bbox_london) |&gt;\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Stations.RData')\n\n\n\nThe OSM query returns all data types, including lines and polygons tagged as stations. For our analysis, we only want to retain the point locations. In addition, we want to clip the results to the outline of London to exclude points that fall within the bounding box but outside the boundaries of Greater London.\n\n\n\nR code\n\n# extract points\nosm_stations &lt;- osm_stations$osm_points |&gt;\n    st_set_crs(4326) |&gt;\n    st_transform(27700) |&gt;\n    st_intersection(outline) |&gt;\n    select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nhead(osm_stations)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 506148.8 ymin: 168292.6 xmax: 546593.8 ymax: 191714.7\nProjected CRS: OSGB36 / British National Grid\n           osm_id                   name                      network\n780856     780856 Shepherd's Bush Market           London Underground\n1256794   1256794           West Drayton National Rail;Elizabeth Line\n2013971   2013971       Finchley Central           London Underground\n9780241   9780241           St Mary Cray                National Rail\n13330343 13330343               Woodford           London Underground\n13790683 13790683               Mile End           London Underground\n                     operator public_transport railway\n780856                   &lt;NA&gt;          station station\n1256794              TfL Rail          station station\n2013971  Transport for London          station station\n9780241          Southeastern          station station\n13330343   London Underground    stop_position    stop\n13790683   London Underground    stop_position    stop\n                          geometry\n780856   POINT (523195.9 180061.9)\n1256794  POINT (506148.8 180085.4)\n2013971    POINT (525294.7 190658)\n9780241  POINT (546593.8 168292.6)\n13330343 POINT (540946.1 191714.7)\n13790683   POINT (536524.6 182545)\n\n# inspect\nnrow(osm_stations)\n\n[1] 3867\n\n\nThe total number of data points seems rather high. In fact, looking at the railway variable, several points are not tagged as station or do not have a value at all:\n\n\n\nR code\n\n# inspect values\ncount(osm_stations, railway)\n\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n                 railway    n                       geometry\n1                station  616 MULTIPOINT ((505078.2 17673...\n2                   stop   17 MULTIPOINT ((528197.7 18571...\n3        subway_entrance   36 MULTIPOINT ((525226.4 18745...\n4 train_station_entrance    1      POINT (538196.3 184826.5)\n5                   &lt;NA&gt; 3197 MULTIPOINT ((505572.7 18421...\n\n\nThe number of points tagged as station in the railway field are most likely the only points in our dataset that represent actual stations, so we will only retain those points.\n\n\n\nR code\n\n# extract train and underground stations\nosm_stations &lt;- osm_stations |&gt;\n    filter(railway == \"station\")\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(osm_stations) +\n\n  # specify colours\n  tm_dots(\n    col = \"#beaed4\",\n    size = 0.05,\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 3: Train and underground stations in London.\n\n\n\n\nNow we have our data prepared, we can move on to analyse the extent to which bicycle theft in London cluster around stations. We can use both spatial queries and geometric operations to complete this analysis.\n\n\n\nA spatial query is used to retrieve data based on its geographic location or spatial relationships. It uses spatial information from one or more layers to find features that meet specific criteria, such as proximity, intersection, or containment. For instance, we can use a spatial query to count all the bicycle thefts that have occurred within 500 metres of a train or underground station:\n\n\n\nR code\n\n# create a single station geometry\nosm_stations_comb &lt;- osm_stations |&gt;\n    st_union()\n\n# spatial query\ntheft_bike$d500 &lt;- theft_bike |&gt;\n    st_is_within_distance(osm_stations_comb, dist = 500, sparse = FALSE)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 12\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 5 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;, d500 &lt;lgl[,1]&gt;\n\n\n\n\n\n\n\n\nThe above code converts the stations dataframe into a single geometry. This step is essential for sf to ensure that each point in the dataset is compared to every point in the stations dataframe. Without this conversion, the comparison would be done one station point at a time, storing only the last result rather than considering all station points simultaneously.\n\n\n\nWe can use the count() function to find out just how many thefts fall in each of these categories:\n\n\n\nR code\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500[,1]     n                                                        geometry\n* &lt;lgl&gt;    &lt;int&gt;                                                &lt;MULTIPOINT [m]&gt;\n1 FALSE     5507 ((384035.6 398089), (409725.4 101574.6), (494844.3 212310.1), …\n2 TRUE     10512 ((505266.1 184336.3), (505314.2 184265.3), (505323.2 184469.3)…\n\n\nMore than two-thirds of all reported bicycle thefts in London occur within 500 metres of a train or underground station. Of course, we can map the results for a visual inspection:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"d500\",\n    size = 0.01,\n    palette = c(\"#f1a340\", \"#998ec3\"),\n    legend.show = FALSE\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"&gt; 500m\",\n    col = \"#f1a340\"\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"&lt; 500m\",\n    col = \"#998ec3\"\n  ) +\n\n  # shape, points\n  tm_shape(osm_stations) +\n\n  # specify colours\n  tm_dots(\n    col = \"#636363\",\n    size = 0.03,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Station\",\n    col = \"#636363\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n  )\n\n\n\n\n\nFigure 4: Reported bicycle thefts in London within 500 metres from a train or underground station.\n\n\n\n\n\n\n\nGeometric operations are used to manipulate and analyse the shapes and spatial properties of geometric objects, such as points, lines, and polygons. These operations include tasks like calculating intersections, buffering, and determining the distance between shapes. In this case, we can create 500-metre buffers around each station and then count how many bicycle thefts fall within these buffers.\n\n\n\nR code\n\n# buffer\nosm_stations_buffer &lt;- osm_stations |&gt;\n    st_buffer(dist = 500) |&gt;\n    st_union()\n\n# inspect\nhead(osm_stations_buffer)\n\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 504578.2 ymin: 158527.2 xmax: 556685.7 ymax: 200638.6\nProjected CRS: OSGB36 / British National Grid\n\n\nMULTIPOLYGON (((518349.2 163095.5, 518336.7 163...\n\n\n\n\n\n\n\n\nWhen performing buffer analysis, the buffer sizes are determined by the units of the coordinate reference system (CRS) used. For instance, with the British National Grid, where the CRS is in metres, the buffer distance must be specified in metres.\n\n\n\nWe can map the results for a visual inspection:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, polygon\n  tm_shape(osm_stations_buffer) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#beaed4\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 5: Train and underground stations in London with a 500 metres buffer.\n\n\n\n\nWe can now use the st_intersects function to find out which reported bicycle thefts have occurred within 500 metres of a train or underground station.\n\n\n\nR code\n\n# intersect buffer with bicycle thefts\ntheft_bike$d500_buffer &lt;- theft_bike |&gt;\n    st_intersects(osm_stations_buffer, sparse = FALSE)\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500_buffer)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500_buffer[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE            5515 ((384035.6 398089), (409725.4 101574.6), (494844.3 2123…\n2 TRUE            10504 ((505266.1 184336.3), (505314.2 184265.3), (505323.2 18…\n\n\n\n\n\n\n\n\nThe results are almost identical, with a small difference due to how the two methods define within and handle spatial relationships and boundaries. For instance, a point on the buffer’s edge will be included in the intersect method, but may not meet the distance threshold required by st_within_distance().\n\n\n\n\n\n\n\nNow that we are familiar with basic spatial queries and geometric operations, we can conduct a similar analysis on the number of serious and fatal road crashed in London in 2022 and determine how many occurred on or near a main road. Try to do the following:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of road crashes that occurred in London in 2022, extracted from the UK’s official road traffic casualty database using the stats19 R library.\nA GeoPackage file that contains main roads in London, extracted from the Ordnance Survey Open Roads dataset.\n\nCalculate the number of serious and fatal road crashes that occurred within 100 metres and 500 metres of a main road, respectively.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\nLondon OS Open Roads - Main Roads\nGeoPackage\nDownload\n\n\n\n\n\n\nBoom. That is how you can conduct basic spatial queries and geometric operations and using R and sf. Yet more RGIS coming over the next couple of weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "02-operations.html#lecture-slides",
    "href": "02-operations.html#lecture-slides",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "02-operations.html#reading-list",
    "href": "02-operations.html#reading-list",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 13: Spatial Data Analysis, pp. 290-318. [Link]\n\n\n\n\n\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]"
  },
  {
    "objectID": "02-operations.html#bike-theft-in-london-i",
    "href": "02-operations.html#bike-theft-in-london-i",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "This week, we will examine to what extent reported bicycle theft in London cluster around train and underground stations. We will be using open data from data.police.uk on reported crimes alongside OpenStreetMap data for this analysis. We will use R to directly download the necessary data from OpenStreetMap, but the crime data will need to be manually downloaded from the data portal. We further have access to a GeoPackage that contains the London 2021 MSOA boundaries that we can use as reference layer. If you do not already have it on your computer, save this file in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\nThe UK Police Data Portal allows you to access and generate tabular data for crime recorded in the UK across the different police forces. To download recorded crime data for London:\n\nNavigate to data.police.uk and click on Downloads.\nUnder the data range select January 2023 to December 2023.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave the other settings unchanged and click on Generate file.\n\n\n\n\n\n\nFigure 1: Downloading data on reported crimes through data.police.uk\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the dataset.\nAfter downloading, unzip the file. You will find that the zip file contains 12 folders, one for each month of 2023. Each folder includes two files: one for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named London-Crime within your data/attributes directory, and copy all 12 folders with the data into this new folder.\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w02-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAlthough we could read each individual crime file into R one by one and then combine them, we can actually accomplish this in a single step:\n\n\n\nR code\n\n# list all csv files\ncrime_df &lt;- list.files(path = \"data/attributes/London-Crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # read individual csv files\n  lapply(read_csv) |&gt;\n  # bind together into one\n  bind_rows()\n\n# inspect\nhead(crime_df)\n\n\n# A tibble: 6 × 12\n  `Crime ID`      Month `Reported by` `Falls within` Longitude Latitude Location\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 4a14d4745da0a2… 2023… City of Lond… City of Londo…    -0.106     51.5 On or n…\n2 e6e32581c99c5b… 2023… City of Lond… City of Londo…    -0.107     51.5 On or n…\n3 7b7cb8e7debe8b… 2023… City of Lond… City of Londo…    -0.110     51.5 On or n…\n4 f7fc44e1e76332… 2023… City of Lond… City of Londo…    -0.108     51.5 On or n…\n5 8083dafd1770af… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n6 4587239a45f0e8… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n# ℹ 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime type` &lt;chr&gt;,\n#   `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;\n\n\n\n\n\n\n\n\nDepending on your computer, processing this data may take some time due to the large volume involved. Once completed, you should have a dataframe containing 1,144,329 observations.\n\n\n\n\n\n\n\n\n\nYou can further inspect the object using the View() function.\n\n\n\nThe column names contain spaces and are therefore not easily referenced. We can easily clean this up using the janitor package:\n\n\n\nR code\n\n# clean names\ncrime_df &lt;- crime_df |&gt;\n    clean_names()\n\n# inspect\nnames(crime_df)\n\n\n [1] \"crime_id\"              \"month\"                 \"reported_by\"          \n [4] \"falls_within\"          \"longitude\"             \"latitude\"             \n [7] \"location\"              \"lsoa_code\"             \"lsoa_name\"            \n[10] \"crime_type\"            \"last_outcome_category\" \"context\"              \n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nFor our analysis, we are currently only interested in reported bicycle thefts, so we need to filter our data based on the crime_type column. We can start by examining the unique values in this column and then subset the data accordingly:\n\n\n\nR code\n\n# unique types\nunique(crime_df$crime_type)\n\n\n [1] \"Other theft\"                  \"Other crime\"                 \n [3] \"Theft from the person\"        \"Public order\"                \n [5] \"Anti-social behaviour\"        \"Burglary\"                    \n [7] \"Criminal damage and arson\"    \"Drugs\"                       \n [9] \"Shoplifting\"                  \"Vehicle crime\"               \n[11] \"Violence and sexual offences\" \"Bicycle theft\"               \n[13] \"Robbery\"                      \"Possession of weapons\"       \n\n# filter\ntheft_bike &lt;- crime_df |&gt;\n    filter(crime_type == \"Bicycle theft\")\n\n# inspect\nhead(theft_bike)\n\n# A tibble: 6 × 12\n  crime_id  month reported_by falls_within longitude latitude location lsoa_code\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1 62b0f525… 2023… City of Lo… City of Lon…   -0.0916     51.5 On or n… E01000002\n2 9a078d63… 2023… City of Lo… City of Lon…   -0.0952     51.5 On or n… E01032739\n3 f175a32e… 2023… City of Lo… City of Lon…   -0.0872     51.5 On or n… E01032739\n4 137ec120… 2023… City of Lo… City of Lon…   -0.0783     51.5 On or n… E01032739\n5 4c3b4677… 2023… City of Lo… City of Lon…   -0.108      51.5 On or n… E01032740\n6 13b5eb5c… 2023… City of Lo… City of Lon…   -0.0980     51.5 On or n… E01032740\n# ℹ 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;,\n#   last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;\n\n\nNow that we have filtered the data to only include reported bicycle thefts, we need to convert our dataframe into a spatial dataframe that maps the locations of the crimes using the recorded latitude and longitude coordinates. We can then project this spatial dataframe into the British National Grid (EPSG:27700).\n\n\n\nR code\n\n# to spatial data\ntheft_bike &lt;- theft_bike |&gt;\n    filter(!is.na(longitude) & !is.na(latitude)) |&gt;\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4236) |&gt;\n    st_transform(27700)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 4 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# london outline\noutline &lt;- msoa21 |&gt;\n  st_union()\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify colours\n  tm_dots(\n    col = \"#fdc086\",\n    size = 0.05,\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\nFigure 2: Reported bicycle thefts in London.\n\n\n\n\nWe can save the prepared dataset as a GeoPackage so that we can use it some other time:\n\n\n\nR code\n\n# write\nst_write(theft_bike, \"data/spatial/London-BicycleTheft-2023.gpkg\")\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world. Each map element (whether a point, line, or polygon) in OSM is tagged with various attribute data. To download the station data we need, we must use the appropriate tags, represented as key and value pairs, to query the OSM database. In our case, we are looking for train stations, which fall under the Public Transport key, with a value of station. To limit our search to London, we can use the spatial extent of the 2021 MSOA boundaries as the bounding box for data extraction.\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_london &lt;- msoa21 |&gt;\n  st_transform(4326) |&gt;\n  st_bbox()\n\n# osm query\nosm_stations &lt;- opq(bbox = bbox_london) |&gt;\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Stations.RData')\n\n\n\nThe OSM query returns all data types, including lines and polygons tagged as stations. For our analysis, we only want to retain the point locations. In addition, we want to clip the results to the outline of London to exclude points that fall within the bounding box but outside the boundaries of Greater London.\n\n\n\nR code\n\n# extract points\nosm_stations &lt;- osm_stations$osm_points |&gt;\n    st_set_crs(4326) |&gt;\n    st_transform(27700) |&gt;\n    st_intersection(outline) |&gt;\n    select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nhead(osm_stations)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 506148.8 ymin: 168292.6 xmax: 546593.8 ymax: 191714.7\nProjected CRS: OSGB36 / British National Grid\n           osm_id                   name                      network\n780856     780856 Shepherd's Bush Market           London Underground\n1256794   1256794           West Drayton National Rail;Elizabeth Line\n2013971   2013971       Finchley Central           London Underground\n9780241   9780241           St Mary Cray                National Rail\n13330343 13330343               Woodford           London Underground\n13790683 13790683               Mile End           London Underground\n                     operator public_transport railway\n780856                   &lt;NA&gt;          station station\n1256794              TfL Rail          station station\n2013971  Transport for London          station station\n9780241          Southeastern          station station\n13330343   London Underground    stop_position    stop\n13790683   London Underground    stop_position    stop\n                          geometry\n780856   POINT (523195.9 180061.9)\n1256794  POINT (506148.8 180085.4)\n2013971    POINT (525294.7 190658)\n9780241  POINT (546593.8 168292.6)\n13330343 POINT (540946.1 191714.7)\n13790683   POINT (536524.6 182545)\n\n# inspect\nnrow(osm_stations)\n\n[1] 3867\n\n\nThe total number of data points seems rather high. In fact, looking at the railway variable, several points are not tagged as station or do not have a value at all:\n\n\n\nR code\n\n# inspect values\ncount(osm_stations, railway)\n\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n                 railway    n                       geometry\n1                station  616 MULTIPOINT ((505078.2 17673...\n2                   stop   17 MULTIPOINT ((528197.7 18571...\n3        subway_entrance   36 MULTIPOINT ((525226.4 18745...\n4 train_station_entrance    1      POINT (538196.3 184826.5)\n5                   &lt;NA&gt; 3197 MULTIPOINT ((505572.7 18421...\n\n\nThe number of points tagged as station in the railway field are most likely the only points in our dataset that represent actual stations, so we will only retain those points.\n\n\n\nR code\n\n# extract train and underground stations\nosm_stations &lt;- osm_stations |&gt;\n    filter(railway == \"station\")\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(osm_stations) +\n\n  # specify colours\n  tm_dots(\n    col = \"#beaed4\",\n    size = 0.05,\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 3: Train and underground stations in London.\n\n\n\n\nNow we have our data prepared, we can move on to analyse the extent to which bicycle theft in London cluster around stations. We can use both spatial queries and geometric operations to complete this analysis.\n\n\n\nA spatial query is used to retrieve data based on its geographic location or spatial relationships. It uses spatial information from one or more layers to find features that meet specific criteria, such as proximity, intersection, or containment. For instance, we can use a spatial query to count all the bicycle thefts that have occurred within 500 metres of a train or underground station:\n\n\n\nR code\n\n# create a single station geometry\nosm_stations_comb &lt;- osm_stations |&gt;\n    st_union()\n\n# spatial query\ntheft_bike$d500 &lt;- theft_bike |&gt;\n    st_is_within_distance(osm_stations_comb, dist = 500, sparse = FALSE)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 12\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 5 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;, d500 &lt;lgl[,1]&gt;\n\n\n\n\n\n\n\n\nThe above code converts the stations dataframe into a single geometry. This step is essential for sf to ensure that each point in the dataset is compared to every point in the stations dataframe. Without this conversion, the comparison would be done one station point at a time, storing only the last result rather than considering all station points simultaneously.\n\n\n\nWe can use the count() function to find out just how many thefts fall in each of these categories:\n\n\n\nR code\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500[,1]     n                                                        geometry\n* &lt;lgl&gt;    &lt;int&gt;                                                &lt;MULTIPOINT [m]&gt;\n1 FALSE     5507 ((384035.6 398089), (409725.4 101574.6), (494844.3 212310.1), …\n2 TRUE     10512 ((505266.1 184336.3), (505314.2 184265.3), (505323.2 184469.3)…\n\n\nMore than two-thirds of all reported bicycle thefts in London occur within 500 metres of a train or underground station. Of course, we can map the results for a visual inspection:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"d500\",\n    size = 0.01,\n    palette = c(\"#f1a340\", \"#998ec3\"),\n    legend.show = FALSE\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"&gt; 500m\",\n    col = \"#f1a340\"\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"&lt; 500m\",\n    col = \"#998ec3\"\n  ) +\n\n  # shape, points\n  tm_shape(osm_stations) +\n\n  # specify colours\n  tm_dots(\n    col = \"#636363\",\n    size = 0.03,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Station\",\n    col = \"#636363\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n  )\n\n\n\n\n\nFigure 4: Reported bicycle thefts in London within 500 metres from a train or underground station.\n\n\n\n\n\n\n\nGeometric operations are used to manipulate and analyse the shapes and spatial properties of geometric objects, such as points, lines, and polygons. These operations include tasks like calculating intersections, buffering, and determining the distance between shapes. In this case, we can create 500-metre buffers around each station and then count how many bicycle thefts fall within these buffers.\n\n\n\nR code\n\n# buffer\nosm_stations_buffer &lt;- osm_stations |&gt;\n    st_buffer(dist = 500) |&gt;\n    st_union()\n\n# inspect\nhead(osm_stations_buffer)\n\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 504578.2 ymin: 158527.2 xmax: 556685.7 ymax: 200638.6\nProjected CRS: OSGB36 / British National Grid\n\n\nMULTIPOLYGON (((518349.2 163095.5, 518336.7 163...\n\n\n\n\n\n\n\n\nWhen performing buffer analysis, the buffer sizes are determined by the units of the coordinate reference system (CRS) used. For instance, with the British National Grid, where the CRS is in metres, the buffer distance must be specified in metres.\n\n\n\nWe can map the results for a visual inspection:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, polygon\n  tm_shape(osm_stations_buffer) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#beaed4\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 5: Train and underground stations in London with a 500 metres buffer.\n\n\n\n\nWe can now use the st_intersects function to find out which reported bicycle thefts have occurred within 500 metres of a train or underground station.\n\n\n\nR code\n\n# intersect buffer with bicycle thefts\ntheft_bike$d500_buffer &lt;- theft_bike |&gt;\n    st_intersects(osm_stations_buffer, sparse = FALSE)\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500_buffer)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500_buffer[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE            5515 ((384035.6 398089), (409725.4 101574.6), (494844.3 2123…\n2 TRUE            10504 ((505266.1 184336.3), (505314.2 184265.3), (505323.2 18…\n\n\n\n\n\n\n\n\nThe results are almost identical, with a small difference due to how the two methods define within and handle spatial relationships and boundaries. For instance, a point on the buffer’s edge will be included in the intersect method, but may not meet the distance threshold required by st_within_distance()."
  },
  {
    "objectID": "02-operations.html#assignment",
    "href": "02-operations.html#assignment",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Now that we are familiar with basic spatial queries and geometric operations, we can conduct a similar analysis on the number of serious and fatal road crashed in London in 2022 and determine how many occurred on or near a main road. Try to do the following:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of road crashes that occurred in London in 2022, extracted from the UK’s official road traffic casualty database using the stats19 R library.\nA GeoPackage file that contains main roads in London, extracted from the Ordnance Survey Open Roads dataset.\n\nCalculate the number of serious and fatal road crashes that occurred within 100 metres and 500 metres of a main road, respectively.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\nLondon OS Open Roads - Main Roads\nGeoPackage\nDownload"
  },
  {
    "objectID": "02-operations.html#before-you-leave",
    "href": "02-operations.html#before-you-leave",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Boom. That is how you can conduct basic spatial queries and geometric operations and using R and sf. Yet more RGIS coming over the next couple of weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "10-datavis.html",
    "href": "10-datavis.html",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Most of the visualisations we have created over the past weeks have been maps. However, you will often need to use other types of visualisations for your data, such as histograms, scatterplots, dendrograms, and boxplots. While base R can be used for simple visualisations, it is best suited for quick data inspections. For publication-worthy and more complex visualisations, the ggplot2 library, which we used last week to create bivariate maps, offers a unified and effective approach to data visualisation based on the grammar of graphics.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nWickham, H. 2010. A layered grammar of graphics. Journal of Computational and Graphical Statistics 19(1): 3-28. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2014. London, The Information Capital: 100 Maps & Graphics That Will Change How You View the City. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 3: Data visualisation. [Link]\n\n\n\n\n\nToday, we will use the same dataset that we used in Week 8 on self-identified ethnicity. We will visualise the distribution of the self-identified White-British population across the 12 Inner London Boroughs. The LSOA data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. A copy of the 2021 London LSOAs spatial boundaries is also available. If you do not already have it on your computer, save these file in your data/attribues and data/spatial folders.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w10-ethnicity-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(treemapify)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load the files in the usual fashion:\n\n\n\nR code\n\n# load attribute dataset\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\") |&gt;\n    st_drop_geometry()\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect\nhead(lsoa21)\n\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\nWe will start by pivoting the data and transforming the raw counts into proportions:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2)\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 4,994 × 21\n# Rowwise: \n   lower_layer_super_output_area…¹ asian_asian_british_…² asian_asian_british_…³\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 E01000001                                      0.00271                0.0448 \n 2 E01000002                                      0.00505                0.0736 \n 3 E01000003                                      0.00682                0.0323 \n 4 E01000005                                      0.239                  0.0318 \n 5 E01000006                                      0.116                  0.00596\n 6 E01000007                                      0.113                  0.0148 \n 7 E01000008                                      0.110                  0.00445\n 8 E01000009                                      0.119                  0.0122 \n 9 E01000011                                      0.146                  0.00469\n10 E01000012                                      0.122                  0.00298\n# ℹ 4,984 more rows\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 18 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe column names are rather long, so let’s rename these manually:\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth)[2:20] &lt;- c(\"Asian - Bangladeshi\", \"Asian - Chinese\", \"Asian - Indian\",\n    \"Asian - Pakistani\", \"Asian - Other\", \"Black - African\", \"Black - Caribbean\",\n    \"Black - Other\", \"Mixed - Asian\", \"Mixed - Black African\", \"Mixed - Black Carribean\",\n    \"Mixed - Other\", \"White - British\", \"White - Irish\", \"White - Traveller\", \"White - Roma\",\n    \"White - Other\", \"Arab - Other\", \"Any Other Group\")\n\n\nThe last thing we need to do is extract the LSOAs that fall within the 12 Inner London Boroughs. We can do this by using the LSOA names that are inside the spatial dataframe:\n\n\n\nR code\n\n# boroughs\ninner_boroughs &lt;- c(\"Camden\", \"Greenwich\", \"Hackney\", \"Hammersmith and Fulham\", \"Islington\", \"Kensington and Chelsea\", \"Lambeth\", \"Lewisham\", \"Southwark\", \"Tower Hamlets\", \"Wandsworth\", \"Westminster\")\n\n# filter spatial data\nlsoa21_inner &lt;- lsoa21 |&gt;\n  filter(str_detect(lsoa21nm, paste(inner_boroughs, collapse = \"|\")))\n\n# filter attribute data, add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  filter(lower_layer_super_output_areas_code %in% lsoa21_inner$lsoa21cd)\n\n# add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  left_join(lsoa21[1:2], by = c(\"lower_layer_super_output_areas_code\" = \"lsoa21cd\"))\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 1,792 × 22\n# Rowwise: \n   lower_layer_super_output_areas_code `Asian - Bangladeshi` `Asian - Chinese`\n   &lt;chr&gt;                                               &lt;dbl&gt;             &lt;dbl&gt;\n 1 E01000842                                         0.00209            0.0265\n 2 E01000843                                         0.00167            0.0301\n 3 E01000844                                         0.00614            0.0258\n 4 E01000845                                         0.0193             0.0273\n 5 E01000846                                         0.0586             0.0179\n 6 E01000847                                         0.0506             0.0312\n 7 E01000848                                         0.00269            0.0249\n 8 E01000849                                         0.0125             0.0249\n 9 E01000850                                         0.0218             0.113 \n10 E01000851                                         0.0216             0.0824\n# ℹ 1,782 more rows\n# ℹ 19 more variables: `Asian - Indian` &lt;dbl&gt;, `Asian - Pakistani` &lt;dbl&gt;,\n#   `Asian - Other` &lt;dbl&gt;, `Black - African` &lt;dbl&gt;, `Black - Caribbean` &lt;dbl&gt;,\n#   `Black - Other` &lt;dbl&gt;, `Mixed - Asian` &lt;dbl&gt;,\n#   `Mixed - Black African` &lt;dbl&gt;, `Mixed - Black Carribean` &lt;dbl&gt;,\n#   `Mixed - Other` &lt;dbl&gt;, `White - British` &lt;dbl&gt;, `White - Irish` &lt;dbl&gt;,\n#   `White - Traveller` &lt;dbl&gt;, `White - Roma` &lt;dbl&gt;, `White - Other` &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf you want to know what the paste(inner_boroughs, collapse = '|') code does, you can run it separately in the console to find out.\n\n\n\n\n\nThe ggplot2 library is built on the layered grammar of graphics, which provides a structured approach to creating visualisations. This means that plots are constructed by adding layers, such as data, aesthetic mappings (e.g., axes, colours, sizes), geometric shapes (e.g., points, lines, bars), and optional elements like themes or statistical transformations. This modular design allows users to build complex and customisable plots step by step, ensuring flexibility and clarity in the visualisation process.\nLet’s try to use this approach by making a boxplot on the distribution of people that self-identify as White British across all Inner London Boroughs. With ggplot2, every plot begins with the ggplot() function, which creates a coordinate system to which layers can be added. The first argument of ggplot() specifies the dataset to use:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth)\n\n\nTo build your graph, you add one or more layers to ggplot(). For instance, geom_point() adds a layer of points to create a scatterplot. ggplot2 provides many geom functions, each adding a different type of layer to your plot. To create a boxplot, you add the geom_boxplot() layer. For boxplots, the mapping argument defines how dataset variables are linked to visual properties, such as the grouping or value axes. The mapping is paired with aes(), where y specifies the numeric variable:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot()\n\n\n\n\n\nFigure 1: Basic boxplot using ggplot2.\n\n\n\n\n\n\n\n\n\n\nAn aesthetic is a visual property of the elements in your plot. Aesthetics include attributes like size, shape, or colour of points. By modifying the values of these aesthetic properties, you can display a point in various ways, allowing for greater customisation and clarity in your visualisation.\n\n\n\nJust like with tmap, we can customise the basic plot by styling the boxplot, adding labels, and adjusting its overall appearancs:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 2: Stylised boxplot using ggplot2.\n\n\n\n\nBut what if we wwant to create a boxplot for all Inner London Boroughs? We can do this by adding a grouping variable:\n\n\n\nR code\n\n# add borough names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name = substr(lsoa21nm, 1, nchar(lsoa21nm) - 5))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    utlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\nWarning in geom_boxplot(fill = \"#f0f0f0\", color = \"#252525\", outlier.color =\n\"#ef3b2c\", : Ignoring unknown parameters: `utlier.shape`\n\n\n\n\n\nFigure 3: Stylised boxplot using ggplot2.\n\n\n\n\nThe boroughs are drawn in alphabetical order by default. To change this we need to adjust the order by creating a factor. For instance, we can sort the boroughs by their median values.\n\n\n\n\n\n\nIn R, a factor is a data structure used to represent categorical variables with a specific order or grouping. Factors allow you to define and manipulate the order of categories, which is especially useful for plotting or analysis.\n\n\n\n\n\n\nR code\n\n# median values\nlsoa_med &lt;- lsoa_eth |&gt;\n  group_by(borough_name) |&gt;\n  summarise(median = median(`White - British`))\n\n# create factor\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name_factor = factor(borough_name, levels = lsoa_med$borough_name[order(lsoa_med$median, decreasing = TRUE)]))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name_factor)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 4: Stylised boxplot using ggplot2.\n\n\n\n\n\n\n\nBoxplots are effective for visualising distributions, but histograms offer another way to explore the same data by showing the frequency of values. While histograms cannot be displayed alongside boxplots in the same image, we can create a series of histograms, each displayed in a separate panel. These panels can show the distributions for different groups, such as individual boroughs.\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_histogram() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Number of LSOAs\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5: Histograms presented in individual panels.\n\n\n\n\nWe could use the same approach to create a series of scatterplots to show the relationship between two variables:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = `White - Other`)) +\n  # add geometry\n  geom_point() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n\n\n\nFigure 6: Scatterplots presented in individual panels.\n\n\n\n\n\n\n\n\n\n\nTo export a ggplot, first assign your plot to an object. Then, use the ggsave() function to save the plot to a file, specifying the desired filename and format (e.g. .png or .pdf). You can specify the dimensions of the output using the width and height arguments.\n\n\n\n\n\n\nThe flexibility of ggplot2 extends beyond traditional plots through additional libraries that expand its functionality, allowing you to create specialised visualisations. For instance, we can use the treemapify library to create a treemap.\n\n\n\n\n\n\nA treemap is a data visualisation that displays hierarchical data as nested rectangles, with each rectangle representing a category or subcategory. The size of each rectangle is proportional to a specific variable, often reflecting values such as frequency or proportion, making it easier to compare the relative sizes of different elements. Treemaps are particularly useful for visualising large datasets with multiple categories or subcategories in a compact, space-efficient layout.\n\n\n\nLet’s try to create a treemap of the mean share of different population groups in the borough of Lambeth. We first need to calculate the mean of each population group in Lambeth and then transform the data from a wide format to a long format so that all proportions are in the same column.\n\n\n\nR code\n\n# mean group values lambeth\nlambeth_mean &lt;- lsoa_eth |&gt;\n    filter(borough_name == \"Lambeth\") |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# wide to long\nlambeth_mean &lt;- lambeth_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n\nWe can now visualise the share of each population group in Lambeth using a treemap:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lambeth_mean, aes(area = proportion, fill = population_group, label = population_group)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 7: Treemap of relative share of population groups in Lambeth.\n\n\n\n\nWe could create facets for these treemaps, but we can also use subgroups to create a nested representation of our data - weighted by the total population in each London borough.\n\n\n\nR code\n\n# mean group values london\nlondon_mean &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# total group values london\nlondon_sum &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(borough_population = sum(eth_pop))\n\n# wide to long,\nlondon_mean &lt;- london_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n# add total population, weigh\nlondon_mean &lt;- london_mean |&gt;\n    left_join(london_sum, by = c(borough_name = \"borough_name\")) |&gt;\n    mutate(proportion_weighted = proportion * borough_population)\n\n\nNow the data have been prepared, we can create a treemap again as follows:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = london_mean, aes(area = proportion_weighted, fill = population_group, label = population_group, subgroup = borough_name)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"#f0f0f0\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8,\n  ) +\n  # add border\n  geom_treemap_subgroup_border(\n    colour = \"#000000\"\n  ) +\n  # add text\n  geom_treemap_subgroup_text(\n    colour = \"#636363\",\n    place = \"bottomleft\",\n    size = 14,\n    fontface = \"bold\",\n    padding.x = grid::unit(2, \"mm\"),\n    padding.y = grid::unit(2, \"mm\"),\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\",\n  )\n\n\n\n\n\nFigure 8: Treemap of relative share of population groups in London, organised by borough\n\n\n\n\n\n\n\n\nThe ggplot2 library supports a wide variety of chart types, all based on the same core principles of layering elements such as data, aesthetics, and geometric shapes. So far, we have worked with boxplots, scatterplots, histograms, and treemaps. However, ggplot2 also offers many other geometries, including spatial geometries, that you can use to create more diverse visualisations.\nUsing the lsoa_eth dataset try to to complete the following tasks:\n\nCreate a violin plot: A violin plot combines aspects of a boxplot and a density plot, offering a compact view of the distribution of continuous data. Use the geom_violin() function to visualise the distribution of the self-identified Asian Bangladeshi population for each of the Inner London boroughs.\nCreate a map: Use the geom_sf() function to map the distribution of the self-identified Black Caribbean population across Greater London.\nCreate a faceted map: Create a faceted map showing the distribution of the self-identified Asian Bangladeshi, Asian Chinese, Black African, and White British populations across London.\n\n\n\n\n\n\n\nTo help you get familiar with ggplot2 and its principles, you can use the esquisse library, which allows you to interactively create plots and generate the corresponding ggplot2 code.\n\n\n\n\n\n\nThat is it for today, and indeed, you have now reached the end of Geocomputation! Over the course of this module, we have explored the fundamental principles of spatial analysis, data visualisation, and reproducible research. It is now inevitable: time for that reading list."
  },
  {
    "objectID": "10-datavis.html#lecture-slides",
    "href": "10-datavis.html#lecture-slides",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "10-datavis.html#reading-list",
    "href": "10-datavis.html#reading-list",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Wickham, H. 2010. A layered grammar of graphics. Journal of Computational and Graphical Statistics 19(1): 3-28. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2014. London, The Information Capital: 100 Maps & Graphics That Will Change How You View the City. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 3: Data visualisation. [Link]"
  },
  {
    "objectID": "10-datavis.html#population-groups-in-london",
    "href": "10-datavis.html#population-groups-in-london",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Today, we will use the same dataset that we used in Week 8 on self-identified ethnicity. We will visualise the distribution of the self-identified White-British population across the 12 Inner London Boroughs. The LSOA data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. A copy of the 2021 London LSOAs spatial boundaries is also available. If you do not already have it on your computer, save these file in your data/attribues and data/spatial folders.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w10-ethnicity-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(treemapify)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load the files in the usual fashion:\n\n\n\nR code\n\n# load attribute dataset\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\") |&gt;\n    st_drop_geometry()\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect\nhead(lsoa21)\n\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\nWe will start by pivoting the data and transforming the raw counts into proportions:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2)\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 4,994 × 21\n# Rowwise: \n   lower_layer_super_output_area…¹ asian_asian_british_…² asian_asian_british_…³\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 E01000001                                      0.00271                0.0448 \n 2 E01000002                                      0.00505                0.0736 \n 3 E01000003                                      0.00682                0.0323 \n 4 E01000005                                      0.239                  0.0318 \n 5 E01000006                                      0.116                  0.00596\n 6 E01000007                                      0.113                  0.0148 \n 7 E01000008                                      0.110                  0.00445\n 8 E01000009                                      0.119                  0.0122 \n 9 E01000011                                      0.146                  0.00469\n10 E01000012                                      0.122                  0.00298\n# ℹ 4,984 more rows\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 18 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe column names are rather long, so let’s rename these manually:\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth)[2:20] &lt;- c(\"Asian - Bangladeshi\", \"Asian - Chinese\", \"Asian - Indian\",\n    \"Asian - Pakistani\", \"Asian - Other\", \"Black - African\", \"Black - Caribbean\",\n    \"Black - Other\", \"Mixed - Asian\", \"Mixed - Black African\", \"Mixed - Black Carribean\",\n    \"Mixed - Other\", \"White - British\", \"White - Irish\", \"White - Traveller\", \"White - Roma\",\n    \"White - Other\", \"Arab - Other\", \"Any Other Group\")\n\n\nThe last thing we need to do is extract the LSOAs that fall within the 12 Inner London Boroughs. We can do this by using the LSOA names that are inside the spatial dataframe:\n\n\n\nR code\n\n# boroughs\ninner_boroughs &lt;- c(\"Camden\", \"Greenwich\", \"Hackney\", \"Hammersmith and Fulham\", \"Islington\", \"Kensington and Chelsea\", \"Lambeth\", \"Lewisham\", \"Southwark\", \"Tower Hamlets\", \"Wandsworth\", \"Westminster\")\n\n# filter spatial data\nlsoa21_inner &lt;- lsoa21 |&gt;\n  filter(str_detect(lsoa21nm, paste(inner_boroughs, collapse = \"|\")))\n\n# filter attribute data, add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  filter(lower_layer_super_output_areas_code %in% lsoa21_inner$lsoa21cd)\n\n# add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  left_join(lsoa21[1:2], by = c(\"lower_layer_super_output_areas_code\" = \"lsoa21cd\"))\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 1,792 × 22\n# Rowwise: \n   lower_layer_super_output_areas_code `Asian - Bangladeshi` `Asian - Chinese`\n   &lt;chr&gt;                                               &lt;dbl&gt;             &lt;dbl&gt;\n 1 E01000842                                         0.00209            0.0265\n 2 E01000843                                         0.00167            0.0301\n 3 E01000844                                         0.00614            0.0258\n 4 E01000845                                         0.0193             0.0273\n 5 E01000846                                         0.0586             0.0179\n 6 E01000847                                         0.0506             0.0312\n 7 E01000848                                         0.00269            0.0249\n 8 E01000849                                         0.0125             0.0249\n 9 E01000850                                         0.0218             0.113 \n10 E01000851                                         0.0216             0.0824\n# ℹ 1,782 more rows\n# ℹ 19 more variables: `Asian - Indian` &lt;dbl&gt;, `Asian - Pakistani` &lt;dbl&gt;,\n#   `Asian - Other` &lt;dbl&gt;, `Black - African` &lt;dbl&gt;, `Black - Caribbean` &lt;dbl&gt;,\n#   `Black - Other` &lt;dbl&gt;, `Mixed - Asian` &lt;dbl&gt;,\n#   `Mixed - Black African` &lt;dbl&gt;, `Mixed - Black Carribean` &lt;dbl&gt;,\n#   `Mixed - Other` &lt;dbl&gt;, `White - British` &lt;dbl&gt;, `White - Irish` &lt;dbl&gt;,\n#   `White - Traveller` &lt;dbl&gt;, `White - Roma` &lt;dbl&gt;, `White - Other` &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf you want to know what the paste(inner_boroughs, collapse = '|') code does, you can run it separately in the console to find out.\n\n\n\n\n\nThe ggplot2 library is built on the layered grammar of graphics, which provides a structured approach to creating visualisations. This means that plots are constructed by adding layers, such as data, aesthetic mappings (e.g., axes, colours, sizes), geometric shapes (e.g., points, lines, bars), and optional elements like themes or statistical transformations. This modular design allows users to build complex and customisable plots step by step, ensuring flexibility and clarity in the visualisation process.\nLet’s try to use this approach by making a boxplot on the distribution of people that self-identify as White British across all Inner London Boroughs. With ggplot2, every plot begins with the ggplot() function, which creates a coordinate system to which layers can be added. The first argument of ggplot() specifies the dataset to use:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth)\n\n\nTo build your graph, you add one or more layers to ggplot(). For instance, geom_point() adds a layer of points to create a scatterplot. ggplot2 provides many geom functions, each adding a different type of layer to your plot. To create a boxplot, you add the geom_boxplot() layer. For boxplots, the mapping argument defines how dataset variables are linked to visual properties, such as the grouping or value axes. The mapping is paired with aes(), where y specifies the numeric variable:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot()\n\n\n\n\n\nFigure 1: Basic boxplot using ggplot2.\n\n\n\n\n\n\n\n\n\n\nAn aesthetic is a visual property of the elements in your plot. Aesthetics include attributes like size, shape, or colour of points. By modifying the values of these aesthetic properties, you can display a point in various ways, allowing for greater customisation and clarity in your visualisation.\n\n\n\nJust like with tmap, we can customise the basic plot by styling the boxplot, adding labels, and adjusting its overall appearancs:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 2: Stylised boxplot using ggplot2.\n\n\n\n\nBut what if we wwant to create a boxplot for all Inner London Boroughs? We can do this by adding a grouping variable:\n\n\n\nR code\n\n# add borough names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name = substr(lsoa21nm, 1, nchar(lsoa21nm) - 5))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    utlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\nWarning in geom_boxplot(fill = \"#f0f0f0\", color = \"#252525\", outlier.color =\n\"#ef3b2c\", : Ignoring unknown parameters: `utlier.shape`\n\n\n\n\n\nFigure 3: Stylised boxplot using ggplot2.\n\n\n\n\nThe boroughs are drawn in alphabetical order by default. To change this we need to adjust the order by creating a factor. For instance, we can sort the boroughs by their median values.\n\n\n\n\n\n\nIn R, a factor is a data structure used to represent categorical variables with a specific order or grouping. Factors allow you to define and manipulate the order of categories, which is especially useful for plotting or analysis.\n\n\n\n\n\n\nR code\n\n# median values\nlsoa_med &lt;- lsoa_eth |&gt;\n  group_by(borough_name) |&gt;\n  summarise(median = median(`White - British`))\n\n# create factor\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name_factor = factor(borough_name, levels = lsoa_med$borough_name[order(lsoa_med$median, decreasing = TRUE)]))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name_factor)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 4: Stylised boxplot using ggplot2.\n\n\n\n\n\n\n\nBoxplots are effective for visualising distributions, but histograms offer another way to explore the same data by showing the frequency of values. While histograms cannot be displayed alongside boxplots in the same image, we can create a series of histograms, each displayed in a separate panel. These panels can show the distributions for different groups, such as individual boroughs.\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_histogram() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Number of LSOAs\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5: Histograms presented in individual panels.\n\n\n\n\nWe could use the same approach to create a series of scatterplots to show the relationship between two variables:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = `White - Other`)) +\n  # add geometry\n  geom_point() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n\n\n\nFigure 6: Scatterplots presented in individual panels.\n\n\n\n\n\n\n\n\n\n\nTo export a ggplot, first assign your plot to an object. Then, use the ggsave() function to save the plot to a file, specifying the desired filename and format (e.g. .png or .pdf). You can specify the dimensions of the output using the width and height arguments.\n\n\n\n\n\n\nThe flexibility of ggplot2 extends beyond traditional plots through additional libraries that expand its functionality, allowing you to create specialised visualisations. For instance, we can use the treemapify library to create a treemap.\n\n\n\n\n\n\nA treemap is a data visualisation that displays hierarchical data as nested rectangles, with each rectangle representing a category or subcategory. The size of each rectangle is proportional to a specific variable, often reflecting values such as frequency or proportion, making it easier to compare the relative sizes of different elements. Treemaps are particularly useful for visualising large datasets with multiple categories or subcategories in a compact, space-efficient layout.\n\n\n\nLet’s try to create a treemap of the mean share of different population groups in the borough of Lambeth. We first need to calculate the mean of each population group in Lambeth and then transform the data from a wide format to a long format so that all proportions are in the same column.\n\n\n\nR code\n\n# mean group values lambeth\nlambeth_mean &lt;- lsoa_eth |&gt;\n    filter(borough_name == \"Lambeth\") |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# wide to long\nlambeth_mean &lt;- lambeth_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n\nWe can now visualise the share of each population group in Lambeth using a treemap:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lambeth_mean, aes(area = proportion, fill = population_group, label = population_group)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 7: Treemap of relative share of population groups in Lambeth.\n\n\n\n\nWe could create facets for these treemaps, but we can also use subgroups to create a nested representation of our data - weighted by the total population in each London borough.\n\n\n\nR code\n\n# mean group values london\nlondon_mean &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# total group values london\nlondon_sum &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(borough_population = sum(eth_pop))\n\n# wide to long,\nlondon_mean &lt;- london_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n# add total population, weigh\nlondon_mean &lt;- london_mean |&gt;\n    left_join(london_sum, by = c(borough_name = \"borough_name\")) |&gt;\n    mutate(proportion_weighted = proportion * borough_population)\n\n\nNow the data have been prepared, we can create a treemap again as follows:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = london_mean, aes(area = proportion_weighted, fill = population_group, label = population_group, subgroup = borough_name)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"#f0f0f0\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8,\n  ) +\n  # add border\n  geom_treemap_subgroup_border(\n    colour = \"#000000\"\n  ) +\n  # add text\n  geom_treemap_subgroup_text(\n    colour = \"#636363\",\n    place = \"bottomleft\",\n    size = 14,\n    fontface = \"bold\",\n    padding.x = grid::unit(2, \"mm\"),\n    padding.y = grid::unit(2, \"mm\"),\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\",\n  )\n\n\n\n\n\nFigure 8: Treemap of relative share of population groups in London, organised by borough"
  },
  {
    "objectID": "10-datavis.html#assignment",
    "href": "10-datavis.html#assignment",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "The ggplot2 library supports a wide variety of chart types, all based on the same core principles of layering elements such as data, aesthetics, and geometric shapes. So far, we have worked with boxplots, scatterplots, histograms, and treemaps. However, ggplot2 also offers many other geometries, including spatial geometries, that you can use to create more diverse visualisations.\nUsing the lsoa_eth dataset try to to complete the following tasks:\n\nCreate a violin plot: A violin plot combines aspects of a boxplot and a density plot, offering a compact view of the distribution of continuous data. Use the geom_violin() function to visualise the distribution of the self-identified Asian Bangladeshi population for each of the Inner London boroughs.\nCreate a map: Use the geom_sf() function to map the distribution of the self-identified Black Caribbean population across Greater London.\nCreate a faceted map: Create a faceted map showing the distribution of the self-identified Asian Bangladeshi, Asian Chinese, Black African, and White British populations across London.\n\n\n\n\n\n\n\nTo help you get familiar with ggplot2 and its principles, you can use the esquisse library, which allows you to interactively create plots and generate the corresponding ggplot2 code."
  },
  {
    "objectID": "10-datavis.html#before-you-leave",
    "href": "10-datavis.html#before-you-leave",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "That is it for today, and indeed, you have now reached the end of Geocomputation! Over the course of this module, we have explored the fundamental principles of spatial analysis, data visualisation, and reproducible research. It is now inevitable: time for that reading list."
  },
  {
    "objectID": "04-autocorrelation.html",
    "href": "04-autocorrelation.html",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "This week, we will explore the concept of spatial dependence: the idea that the value of a variable at one location is influenced by the value of the same variable at nearby locations. This dependence can be statistically measured by assessing spatial autocorrelation, which refers to the degree of similarity between values of a variable at different locations or between multiple variables at the same location.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGriffith, D. 2017. Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge. [Link]\nGimond, M. 2023. Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation. [Link]\nLivings, M. and Wu, A-M. 2020. Local Measures of Spatial Association. The Geographic Information Science & Technology Body of Knowledge. [Link]\nSu, R. Newsham, N., and Dodge, S. 2024. Spatiotemporal dynamics of ethnoracial diversity and segregation in Los Angeles County: Insights from mobile phone data. Computers, Environment and Urban Systems 114: 102203. [Link]\n\n\n\n\n\nLee, S. 2019. Uncertainty in the effects of the modifiable areal unit problem under different levels of spatial autocorrelation: a simulation study. International Journal of Geographical Information Science 33: 1135-1154. [Link]\nHarris, R. 2020. Exploring the neighbourhood-level correlates of Covid-19 deaths in London using a difference across spatial boundaries method. Health & Place 66: 102446. [Link]\n\n\n\n\n\nThis week, we will investigate to what extent people in London who self-identified as Asian-Bangladeshi in the 2021 Census are clustered in London at the LSOA-level. The data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAn LSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 1,000 to 3,000 people and is designed to ensure consistent data reporting. LSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe data has been extracted using the Custom Dataset Tool and subsequently processed to include only the proportion of individuals who self-identify as belonging to one of the Asian groups defined in the Census. Along with this dataset, we also have access to a GeoPackage that contains the LSOA boundaries.\nYou can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Asian Population\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w04-spatial-autocorrelation.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Asian.csv\")\n\nRows: 4994 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lower_layer_super_output_areas_code\ndbl (5): asian_asian_british_or_asian_welsh_bangladeshi, asian_asian_british...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021                           geom\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473 MULTIPOLYGON (((532105.3 18...\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384 MULTIPOLYGON (((532634.5 18...\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613 MULTIPOLYGON (((532135.1 18...\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101 MULTIPOLYGON (((533808 1807...\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842 MULTIPOLYGON (((545122 1843...\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904 MULTIPOLYGON (((544180.3 18...\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 6\n  lower_layer_super_output_areas…¹ asian_asian_british_…² asian_asian_british_…³\n  &lt;chr&gt;                                             &lt;dbl&gt;                  &lt;dbl&gt;\n1 E01000001                                       0.00271                0.0448 \n2 E01000002                                       0.00505                0.0736 \n3 E01000003                                       0.00682                0.0323 \n4 E01000005                                       0.239                  0.0318 \n5 E01000006                                       0.116                  0.00596\n6 E01000007                                       0.113                  0.0148 \n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 3 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nYou will notice is that the column names are rather long, so let us rename the columns for easier reference.\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth) &lt;- c(\"lsoa21cd\", \"asian_bangladeshi\", \"asian_chinese\", \"asian_indian\",\n    \"asian_pakistani\", \"asian_other\")\n\n\n\n\nAs you should know by now, the first step when working with spatial data is to create a map:\n\n\n\nR code\n\n# join attribute data onto spatial data\nlsoa21 &lt;- lsoa21 |&gt;\n  left_join(lsoa_eth, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"asian_bangladeshi\",\n    palette = c(\"#f0f9e8\", \"#bae4bc\", \"#7bccc4\", \"#43a2ca\", \"#0868ac\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Proportion\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people that self-identify as Asian-Bangladeshi.\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that self-identify as Asian-Bangladeshi appears to be neither random nor uniform, with a tendency for similar values to be found in some neighbourhoods in East London. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# seed for reproducibility of random permutation\nset.seed(99)\n\n# random permutation\nlsoa21 &lt;- lsoa21 |&gt;\n  mutate(asian_bangladeshi_random = sample(lsoa21$asian_bangladeshi, replace = FALSE))\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"asian_bangladeshi_random\",\n    palette = c(\"#f0f9e8\", \"#bae4bc\", \"#7bccc4\", \"#43a2ca\", \"#0868ac\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Proportion\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people that self-identify as Asian-Bangladeshi with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation).\nUnderlying our Moran’s I test is the concept of a spatial lag. A spatial lag refers to a concept in spatial analysis where the value of a variable at a given location is influenced by the values of the same variable at neighboring locations. Essentially, it captures the idea that observations in close proximity are likely to be correlated, meaning that what happens in one area can ‘lag’ into or affect nearby areas. The Moran’s I statistic tries to capture the relationship between a value and its spatial lag. An Ordinary Least Squares (OLS) regression is applied, after both variables have been transformed to z-scores, to fit the data and produce a slope, which determines the Moran’s I statistic.\n\n\n\n\n\nFigure 3: Scatter plot of spatially lagged income (neighboring income) versus each areas income. Source: Manuel Gimond.\n\n\n\n\n\n\n\n\n\n\nMoran’s I values typically range from \\(-1\\) to \\(1\\):\n\n+1: Indicates perfect positive spatial autocorrelation. High values cluster near other high values, and low values near other low values.\n0: Suggests no spatial autocorrelation, meaning the spatial distribution of the variable is random.\n-1: Indicates perfect negative spatial autocorrelation. High values cluster near low values, and vice versa (a checkerboard pattern).\n\n\n\n\nThere are two approaches to estimating the significance of the Moran’s I statistic: an analytical method and a computational method. The analytical method relies on assumptions about the data, such as normality, which can sometimes limit its reliability. In contrast, the computational method, which is preferred here, does not make such assumptions and offers a more flexible and robust evaluation of significance.\nThe computational approach is based on a repeated random permutation of the observed values. The Moran’s I statistic is then calculated for each of these randomly reshuffled data sets, generating a reference distribution. By comparing the observed Moran’s I value to this reference distribution, we can assess whether our observed statistic is typical or an outlier and calculate a psuedo \\(p\\)-value (see Figure 4). If the observed Moran’s I value is an outlier, meaning it falls outside the range expected from random data distribution, it suggests a significant degree of clustering in the data.\n\n\n\n\n\nFigure 4: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond.\n\n\n\n\nWe can derive a pseudo-\\(p\\) value from these simulation results as follows:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nwhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme than our observed statistic and \\({N}\\) is the total number of simulations. In the example shown in Figure 4, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that there is a one percent probability that we would be wrong in rejecting the null hypothesis of spatial randomness.\n\n\n\nIf the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nContiguity\nSpatial units are considered neighbours if their polygon boundaries touch.\n\n\nFixed Distance\nSpatial units are considered neighbours if they fall within a specified distance.\n\n\nNearest Neighbours\nSpatial units are considered neighbours if they are among the closest neighbours.\n\n\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbour list\nlsoa21_nb &lt;- poly2nb(lsoa21, queen = TRUE)\n\n# inspect\nsummary(lsoa21_nb)\n\n\nNeighbour list object:\nNumber of regions: 4994 \nNumber of nonzero links: 29472 \nPercentage nonzero weights: 0.1181714 \nAverage number of links: 5.901482 \nLink number distribution:\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   3   19  185  670 1244 1331  841  404  185   70   21   13    3    1    2    1 \n  20 \n   1 \n3 least connected regions:\n2329 3915 4440 with 1 link\n1 most connected region:\n4990 with 20 links\n\n\nThe neighbour list object is a sparse matrix that lists the neighboring polygons for each LSOA. This matrix represents the spatial relationships between LSOAs, where each entry indicates which polygons share boundaries. These neighborhood relationships can be visualised as a graph by extracting the coordinate points of the centroids of the polygons representing each LSOA:\n\n\n\n\n\n\nRegardless of the neighborhood definition you choose, it is important to verify the results, particularly when using contiguity-based approaches. If your spatial file has issues such as polygons that appear adjacent but do not actually share a border, your results may be inaccurate. You could increase the default value of the snap distance parameter in the poly2nb() function to include these polygons only separated by small gaps.\n\n\n\n\n\n\nR code\n\n# extract centroids from polygons\nlsoa21_cent &lt;- st_centroid(lsoa21, of_largest_polygon = TRUE)\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# plot graph\npar(mai = c(0, 0, 0, 0))\nplot(st_geometry(lsoa21), border = \"#cccccc\")\nplot(lsoa21_nb, st_geometry(lsoa21_cent), add = TRUE)\n\n\n\n\nFigure 5: Neighbourhood graph using queen contiguity.\n\n\n\n\nWith nearly 5,000 LSOAs, the neighbourhood graph appears quite crowded. However, it seems acceptable, with no noticeable gaps and a dense network of neighbours in Central London, where many smaller LSOAs are located.\n\n\n\nThe neighbourhood list simply identifies which areas (polygons) are neighbours, but spatial weights take this a step further by assigning a weight to each neighbourhood connection. This is important because not all polygons have the same number of neighbours. To ensure that our spatially lagged values are comparable across neighbourhoods of different sizes, standardisation is required. The code below uses style = 'W' to row-standardise the values: if an LSOA has five neighbours, the value of the spatially lagged variable will be the average of that variable across those five neighbours, with each neighbour receiving equal weight.\n\n\n\nR code\n\n# create spatial weights matrix\nlsoa21_nb_weights &lt;- lsoa21_nb |&gt;\n    nb2listw(style = \"W\")\n\n# inspect - neigbhours of polygon '10'\nlsoa21_nb_weights$neighbours[[10]]\n\n\n[1]    6    7    9 3557 3559 3560 3561\n\n# inspect - weights of neighbours of polygon '10'\nlsoa21_nb_weights$weights[[10]]\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n\n\n\n\nNot all places have neighbours. Islands, by definition, will not be considered as neighbours using a contiguity approach. If you attempt to create spatial weights using the nb2listw() function with a neighbourhood list that includes places without neighbours, you will encounter an error message. Potential solutions include using a different neighbourhood definition (e.g. \\(k\\)-nearest neighbours) or manually editing the neighbourhood file if you wish to include these polygons. Alternatively, you can leave it as is but then you must specify the argument zero.policy = TRUE in nb2listw() to allow for empty sets.\n\n\n\n\n\n\nNow that everything is in place, we can begin by plotting the proportion of people without schooling against the spatially lagged values:\n\n\n\nR code\n\n# moran's plot\nmoran.plot(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, xlab = \"Variable: Asian-Bangladeshi\",\n    ylab = \"Spatially Lagged Variable: Asian Bangladeshi\")\n\n\n\n\n\nFigure 6: Plot of lagged values versus polygon values.\n\n\n\n\nWe observe a positive relationship between our asian_bangladeshi variable and the spatially lagged values, suggesting that our global Moran’s I test will likely yield a statistic reflective of the slope visible in the scatter plot.\n\n\n\nR code\n\n# moran's test\nmoran &lt;- moran.mc(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, nsim = 999)\n\n# results\nmoran\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  lsoa21$asian_bangladeshi \nweights: lsoa21_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.84992, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe results of the Monte Carlo simulation, visualised in Figure 7, suggest that there is statistically significant positive autocorrelation in our variable. This indicates that LSOAs with higher percentages of people that self-identify as Asian-Bangladeshi tend to be surrounded by other LSOAS with similarly high percentages. Likewise, LSOAs with lower percentages of people that self-identify as Asian Bangladeshi are generally surrounded by LSOAs with similarly low values.\n\n\n\nR code\n\n# permutation distribution\nplot(moran, main = \"\", xlab = \"Variable: Asian-Bangladeshi\")\n\n\n\n\n\nFigure 7: Density plot of permutation outcomes.\n\n\n\n\n\n\n\nAlthough we have established that there is positive spatial autocorrelation in our data, we still need to identify the specific spatial patterns. Looking back at Figure 3, you will notice that the plot is divided into four quadrants.\n\n\n\n\n\n\n\nQuadrant\nDescription\n\n\n\n\nTop-right quadrant\nThis area represents LSOAs that have a higher-than-average share of the population without schooling and are surrounded by other LSOAs with similarly high shares of the population without schooling. These are known as high-high clusters.\n\n\nBottom-left quadrant\nThis area represents LSOAs with a lower-than-average share of the population without schooling, surrounded by other LSOAs with similarly low shares. These are low-low clusters.\n\n\nTop-left quadrant\nLSOAs with a higher-than-average share of the population without schooling surrounded by LSOAs with a lower-than-average share. These are high-low clusters.\n\n\nBottom-right quadrant\nLSOAs with a lower-than-average share of the population without schooling surrounded by LSOAs with a higher-than-average share. These are low-high clusters.\n\n\n\nWe can show these area on a map by deconstructing the Moran’s I into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours.\n\n\n\nR code\n\n# local moran's test\nlmoran &lt;- localmoran_perm(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, nsim = 999)\n\n# results\nhead(lmoran)\n\n\n          Ii          E.Ii     Var.Ii      Z.Ii Pr(z != E(Ii))\n1 0.15069391 -0.0032726893 0.03484206 0.8248493   4.094572e-01\n2 0.13225460 -0.0016613502 0.03507478 0.7150473   4.745798e-01\n3 0.09935974 -0.0009791708 0.02573120 0.6255173   5.316316e-01\n4 4.20123420 -0.0098905991 1.02079945 4.1680018   3.072815e-05\n5 2.21814457  0.0064109676 0.29187729 4.0938570   4.242561e-05\n6 1.07427412  0.0125602786 0.17292266 2.5531805   1.067442e-02\n  Pr(z != E(Ii)) Sim Pr(folded) Sim  Skewness Kurtosis\n1              0.130          0.065 -1.903886 3.970373\n2              0.346          0.173 -1.949852 4.028935\n3              0.650          0.325 -1.874507 4.017294\n4              0.010          0.005  1.618687 2.813289\n5              0.010          0.005  2.054221 4.456165\n6              0.060          0.030  1.563633 2.379143\n\n\nWe are not given a single statistic as we did with our global Moran’s I, but rather we get a table of different statistics that are all related back to each of the LSOAs in our dataset. If we refer to the help page for the localmoran() function, we can find detailed explanations of these statistics. The most relevant ones include:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic.\n\n\nE.Ii\nExpectation (mean) of the local Moran’s I statistic.\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation (z-score) of the local Moran’s I statistic.\n\n\nPr()\nPseudo \\(p\\)-value of local Moran’s I statistic based on standard deviations and means from the permutation sample.\n\n\nPr() Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample, assuming a uniform distribution.\n\n\nPr(Folded) Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample using a one-sided test, assuming a uniform distribution.\n\n\n\nWe can further extract the quadrants to which of all these polygons have been assigned:\n\n\n\nR code\n\n# extract quadrants\nlmoran_quadrants &lt;- attr(lmoran, \"quadr\")\n\n# inspect\nhead(lmoran_quadrants)\n\n\n       mean    median     pysal\n1   Low-Low   Low-Low   Low-Low\n2   Low-Low   Low-Low   Low-Low\n3   Low-Low  Low-High   Low-Low\n4 High-High High-High High-High\n5 High-High High-High High-High\n6 High-High High-High High-High\n\n\nWe can now link these values back to our spatial dataframe and make a map using the tmap library:\n\n\n\nR code\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean\", \"lmoran_median\", \"lmoran_pysal\")\n\n# bind results\nlsoa21 &lt;- lsoa21 |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 8: Mapping the Local Moran’s I clusters.\n\n\n\n\nThis type of map is called a LISA map and is a great way of showing how a variable is actually clustering over space. However, we can improve on this further by only mapping the statistically significant clusters:\n\n\n\nR code\n\n# replace values if not significant\nlmoran_quadrants[lmoran[, 6] &gt; 0.05, ] &lt;- NA\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean_sig\", \"lmoran_median_sig\", \"lmoran_pysal_sig\")\n\n# bind results\nlsoa21 &lt;- lsoa21 |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean_sig\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 9: Mapping the significant Local Moran’s I clusters.\n\n\n\n\n\n\n\n\n\n\nThis new map may still not fully address the issue of statistical significance due to repeated testing, and some values may appear significant purely by chance. To correct for this, you can adjust the \\(p\\)-values using R’s p.adjust() function. For further details, refer to Manual Gimond’s explanation of the multiple comparison problem in the context of the pseudo-\\(p\\) values.\n\n\n\n\n\n\n\nAny statistic that includes spatial weights is dependent upon how those weights are defined. We have so far used first order contiguity, i.e. polygons that share a boundary, but there is no particular reason why we should not include second order contiguity polygons (i.e. neighbours of neighbours), use a fixed distance neighbours definitions, or adopt a \\(k\\) nearest neighbours definition. Try to do the following:\n\nExtract the centroids from the lsoa21 file.\nIdentify the 5 nearest neighbours for each LSOA, using the knearneigh() function.\nCreate a neigbhour list of these nearest neighbours, using the knn2nb() function.\nCompute the Global Moran’s I of the asian_indian variable using this new neighbourhood definition.\nMap the statistically significant clusters of Local Moran’s I based on this new neighbourhood definition.\nCompare these results to the output of our asian_bangladeshi values. Do both variables exhibit clustering? Are the clusters located in similar areas?\n\n\n\n\nAnd that is how you can measure spatial dependence in your dataset through different spatial autocorrelation measures. Next week we will focus on the last topic within our set of core spatial analysis methods and techniques, but this week we have covered enough! Probably time to get back to that pesky reading list."
  },
  {
    "objectID": "04-autocorrelation.html#lecture-slides",
    "href": "04-autocorrelation.html#lecture-slides",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "04-autocorrelation.html#reading-list",
    "href": "04-autocorrelation.html#reading-list",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Griffith, D. 2017. Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge. [Link]\nGimond, M. 2023. Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation. [Link]\nLivings, M. and Wu, A-M. 2020. Local Measures of Spatial Association. The Geographic Information Science & Technology Body of Knowledge. [Link]\nSu, R. Newsham, N., and Dodge, S. 2024. Spatiotemporal dynamics of ethnoracial diversity and segregation in Los Angeles County: Insights from mobile phone data. Computers, Environment and Urban Systems 114: 102203. [Link]\n\n\n\n\n\nLee, S. 2019. Uncertainty in the effects of the modifiable areal unit problem under different levels of spatial autocorrelation: a simulation study. International Journal of Geographical Information Science 33: 1135-1154. [Link]\nHarris, R. 2020. Exploring the neighbourhood-level correlates of Covid-19 deaths in London using a difference across spatial boundaries method. Health & Place 66: 102446. [Link]"
  },
  {
    "objectID": "04-autocorrelation.html#population-groups-in-london",
    "href": "04-autocorrelation.html#population-groups-in-london",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "This week, we will investigate to what extent people in London who self-identified as Asian-Bangladeshi in the 2021 Census are clustered in London at the LSOA-level. The data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAn LSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 1,000 to 3,000 people and is designed to ensure consistent data reporting. LSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe data has been extracted using the Custom Dataset Tool and subsequently processed to include only the proportion of individuals who self-identify as belonging to one of the Asian groups defined in the Census. Along with this dataset, we also have access to a GeoPackage that contains the LSOA boundaries.\nYou can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Asian Population\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w04-spatial-autocorrelation.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load ethnicity data\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Asian.csv\")\n\nRows: 4994 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lower_layer_super_output_areas_code\ndbl (5): asian_asian_british_or_asian_welsh_bangladeshi, asian_asian_british...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021                           geom\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473 MULTIPOLYGON (((532105.3 18...\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384 MULTIPOLYGON (((532634.5 18...\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613 MULTIPOLYGON (((532135.1 18...\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101 MULTIPOLYGON (((533808 1807...\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842 MULTIPOLYGON (((545122 1843...\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904 MULTIPOLYGON (((544180.3 18...\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 6\n  lower_layer_super_output_areas…¹ asian_asian_british_…² asian_asian_british_…³\n  &lt;chr&gt;                                             &lt;dbl&gt;                  &lt;dbl&gt;\n1 E01000001                                       0.00271                0.0448 \n2 E01000002                                       0.00505                0.0736 \n3 E01000003                                       0.00682                0.0323 \n4 E01000005                                       0.239                  0.0318 \n5 E01000006                                       0.116                  0.00596\n6 E01000007                                       0.113                  0.0148 \n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 3 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nYou will notice is that the column names are rather long, so let us rename the columns for easier reference.\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth) &lt;- c(\"lsoa21cd\", \"asian_bangladeshi\", \"asian_chinese\", \"asian_indian\",\n    \"asian_pakistani\", \"asian_other\")\n\n\n\n\nAs you should know by now, the first step when working with spatial data is to create a map:\n\n\n\nR code\n\n# join attribute data onto spatial data\nlsoa21 &lt;- lsoa21 |&gt;\n  left_join(lsoa_eth, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"asian_bangladeshi\",\n    palette = c(\"#f0f9e8\", \"#bae4bc\", \"#7bccc4\", \"#43a2ca\", \"#0868ac\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Proportion\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people that self-identify as Asian-Bangladeshi.\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that self-identify as Asian-Bangladeshi appears to be neither random nor uniform, with a tendency for similar values to be found in some neighbourhoods in East London. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# seed for reproducibility of random permutation\nset.seed(99)\n\n# random permutation\nlsoa21 &lt;- lsoa21 |&gt;\n  mutate(asian_bangladeshi_random = sample(lsoa21$asian_bangladeshi, replace = FALSE))\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"asian_bangladeshi_random\",\n    palette = c(\"#f0f9e8\", \"#bae4bc\", \"#7bccc4\", \"#43a2ca\", \"#0868ac\"),\n    border.col = \"#ffffff\",\n    border.alpha = 0.1,\n    title = \"Proportion\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people that self-identify as Asian-Bangladeshi with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation).\nUnderlying our Moran’s I test is the concept of a spatial lag. A spatial lag refers to a concept in spatial analysis where the value of a variable at a given location is influenced by the values of the same variable at neighboring locations. Essentially, it captures the idea that observations in close proximity are likely to be correlated, meaning that what happens in one area can ‘lag’ into or affect nearby areas. The Moran’s I statistic tries to capture the relationship between a value and its spatial lag. An Ordinary Least Squares (OLS) regression is applied, after both variables have been transformed to z-scores, to fit the data and produce a slope, which determines the Moran’s I statistic.\n\n\n\n\n\nFigure 3: Scatter plot of spatially lagged income (neighboring income) versus each areas income. Source: Manuel Gimond.\n\n\n\n\n\n\n\n\n\n\nMoran’s I values typically range from \\(-1\\) to \\(1\\):\n\n+1: Indicates perfect positive spatial autocorrelation. High values cluster near other high values, and low values near other low values.\n0: Suggests no spatial autocorrelation, meaning the spatial distribution of the variable is random.\n-1: Indicates perfect negative spatial autocorrelation. High values cluster near low values, and vice versa (a checkerboard pattern).\n\n\n\n\nThere are two approaches to estimating the significance of the Moran’s I statistic: an analytical method and a computational method. The analytical method relies on assumptions about the data, such as normality, which can sometimes limit its reliability. In contrast, the computational method, which is preferred here, does not make such assumptions and offers a more flexible and robust evaluation of significance.\nThe computational approach is based on a repeated random permutation of the observed values. The Moran’s I statistic is then calculated for each of these randomly reshuffled data sets, generating a reference distribution. By comparing the observed Moran’s I value to this reference distribution, we can assess whether our observed statistic is typical or an outlier and calculate a psuedo \\(p\\)-value (see Figure 4). If the observed Moran’s I value is an outlier, meaning it falls outside the range expected from random data distribution, it suggests a significant degree of clustering in the data.\n\n\n\n\n\nFigure 4: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond.\n\n\n\n\nWe can derive a pseudo-\\(p\\) value from these simulation results as follows:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nwhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme than our observed statistic and \\({N}\\) is the total number of simulations. In the example shown in Figure 4, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that there is a one percent probability that we would be wrong in rejecting the null hypothesis of spatial randomness.\n\n\n\nIf the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nContiguity\nSpatial units are considered neighbours if their polygon boundaries touch.\n\n\nFixed Distance\nSpatial units are considered neighbours if they fall within a specified distance.\n\n\nNearest Neighbours\nSpatial units are considered neighbours if they are among the closest neighbours.\n\n\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbour list\nlsoa21_nb &lt;- poly2nb(lsoa21, queen = TRUE)\n\n# inspect\nsummary(lsoa21_nb)\n\n\nNeighbour list object:\nNumber of regions: 4994 \nNumber of nonzero links: 29472 \nPercentage nonzero weights: 0.1181714 \nAverage number of links: 5.901482 \nLink number distribution:\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   3   19  185  670 1244 1331  841  404  185   70   21   13    3    1    2    1 \n  20 \n   1 \n3 least connected regions:\n2329 3915 4440 with 1 link\n1 most connected region:\n4990 with 20 links\n\n\nThe neighbour list object is a sparse matrix that lists the neighboring polygons for each LSOA. This matrix represents the spatial relationships between LSOAs, where each entry indicates which polygons share boundaries. These neighborhood relationships can be visualised as a graph by extracting the coordinate points of the centroids of the polygons representing each LSOA:\n\n\n\n\n\n\nRegardless of the neighborhood definition you choose, it is important to verify the results, particularly when using contiguity-based approaches. If your spatial file has issues such as polygons that appear adjacent but do not actually share a border, your results may be inaccurate. You could increase the default value of the snap distance parameter in the poly2nb() function to include these polygons only separated by small gaps.\n\n\n\n\n\n\nR code\n\n# extract centroids from polygons\nlsoa21_cent &lt;- st_centroid(lsoa21, of_largest_polygon = TRUE)\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# plot graph\npar(mai = c(0, 0, 0, 0))\nplot(st_geometry(lsoa21), border = \"#cccccc\")\nplot(lsoa21_nb, st_geometry(lsoa21_cent), add = TRUE)\n\n\n\n\nFigure 5: Neighbourhood graph using queen contiguity.\n\n\n\n\nWith nearly 5,000 LSOAs, the neighbourhood graph appears quite crowded. However, it seems acceptable, with no noticeable gaps and a dense network of neighbours in Central London, where many smaller LSOAs are located.\n\n\n\nThe neighbourhood list simply identifies which areas (polygons) are neighbours, but spatial weights take this a step further by assigning a weight to each neighbourhood connection. This is important because not all polygons have the same number of neighbours. To ensure that our spatially lagged values are comparable across neighbourhoods of different sizes, standardisation is required. The code below uses style = 'W' to row-standardise the values: if an LSOA has five neighbours, the value of the spatially lagged variable will be the average of that variable across those five neighbours, with each neighbour receiving equal weight.\n\n\n\nR code\n\n# create spatial weights matrix\nlsoa21_nb_weights &lt;- lsoa21_nb |&gt;\n    nb2listw(style = \"W\")\n\n# inspect - neigbhours of polygon '10'\nlsoa21_nb_weights$neighbours[[10]]\n\n\n[1]    6    7    9 3557 3559 3560 3561\n\n# inspect - weights of neighbours of polygon '10'\nlsoa21_nb_weights$weights[[10]]\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n\n\n\n\nNot all places have neighbours. Islands, by definition, will not be considered as neighbours using a contiguity approach. If you attempt to create spatial weights using the nb2listw() function with a neighbourhood list that includes places without neighbours, you will encounter an error message. Potential solutions include using a different neighbourhood definition (e.g. \\(k\\)-nearest neighbours) or manually editing the neighbourhood file if you wish to include these polygons. Alternatively, you can leave it as is but then you must specify the argument zero.policy = TRUE in nb2listw() to allow for empty sets.\n\n\n\n\n\n\nNow that everything is in place, we can begin by plotting the proportion of people without schooling against the spatially lagged values:\n\n\n\nR code\n\n# moran's plot\nmoran.plot(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, xlab = \"Variable: Asian-Bangladeshi\",\n    ylab = \"Spatially Lagged Variable: Asian Bangladeshi\")\n\n\n\n\n\nFigure 6: Plot of lagged values versus polygon values.\n\n\n\n\nWe observe a positive relationship between our asian_bangladeshi variable and the spatially lagged values, suggesting that our global Moran’s I test will likely yield a statistic reflective of the slope visible in the scatter plot.\n\n\n\nR code\n\n# moran's test\nmoran &lt;- moran.mc(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, nsim = 999)\n\n# results\nmoran\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  lsoa21$asian_bangladeshi \nweights: lsoa21_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.84992, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe results of the Monte Carlo simulation, visualised in Figure 7, suggest that there is statistically significant positive autocorrelation in our variable. This indicates that LSOAs with higher percentages of people that self-identify as Asian-Bangladeshi tend to be surrounded by other LSOAS with similarly high percentages. Likewise, LSOAs with lower percentages of people that self-identify as Asian Bangladeshi are generally surrounded by LSOAs with similarly low values.\n\n\n\nR code\n\n# permutation distribution\nplot(moran, main = \"\", xlab = \"Variable: Asian-Bangladeshi\")\n\n\n\n\n\nFigure 7: Density plot of permutation outcomes.\n\n\n\n\n\n\n\nAlthough we have established that there is positive spatial autocorrelation in our data, we still need to identify the specific spatial patterns. Looking back at Figure 3, you will notice that the plot is divided into four quadrants.\n\n\n\n\n\n\n\nQuadrant\nDescription\n\n\n\n\nTop-right quadrant\nThis area represents LSOAs that have a higher-than-average share of the population without schooling and are surrounded by other LSOAs with similarly high shares of the population without schooling. These are known as high-high clusters.\n\n\nBottom-left quadrant\nThis area represents LSOAs with a lower-than-average share of the population without schooling, surrounded by other LSOAs with similarly low shares. These are low-low clusters.\n\n\nTop-left quadrant\nLSOAs with a higher-than-average share of the population without schooling surrounded by LSOAs with a lower-than-average share. These are high-low clusters.\n\n\nBottom-right quadrant\nLSOAs with a lower-than-average share of the population without schooling surrounded by LSOAs with a higher-than-average share. These are low-high clusters.\n\n\n\nWe can show these area on a map by deconstructing the Moran’s I into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours.\n\n\n\nR code\n\n# local moran's test\nlmoran &lt;- localmoran_perm(lsoa21$asian_bangladeshi, listw = lsoa21_nb_weights, nsim = 999)\n\n# results\nhead(lmoran)\n\n\n          Ii          E.Ii     Var.Ii      Z.Ii Pr(z != E(Ii))\n1 0.15069391 -0.0032726893 0.03484206 0.8248493   4.094572e-01\n2 0.13225460 -0.0016613502 0.03507478 0.7150473   4.745798e-01\n3 0.09935974 -0.0009791708 0.02573120 0.6255173   5.316316e-01\n4 4.20123420 -0.0098905991 1.02079945 4.1680018   3.072815e-05\n5 2.21814457  0.0064109676 0.29187729 4.0938570   4.242561e-05\n6 1.07427412  0.0125602786 0.17292266 2.5531805   1.067442e-02\n  Pr(z != E(Ii)) Sim Pr(folded) Sim  Skewness Kurtosis\n1              0.130          0.065 -1.903886 3.970373\n2              0.346          0.173 -1.949852 4.028935\n3              0.650          0.325 -1.874507 4.017294\n4              0.010          0.005  1.618687 2.813289\n5              0.010          0.005  2.054221 4.456165\n6              0.060          0.030  1.563633 2.379143\n\n\nWe are not given a single statistic as we did with our global Moran’s I, but rather we get a table of different statistics that are all related back to each of the LSOAs in our dataset. If we refer to the help page for the localmoran() function, we can find detailed explanations of these statistics. The most relevant ones include:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic.\n\n\nE.Ii\nExpectation (mean) of the local Moran’s I statistic.\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation (z-score) of the local Moran’s I statistic.\n\n\nPr()\nPseudo \\(p\\)-value of local Moran’s I statistic based on standard deviations and means from the permutation sample.\n\n\nPr() Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample, assuming a uniform distribution.\n\n\nPr(Folded) Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample using a one-sided test, assuming a uniform distribution.\n\n\n\nWe can further extract the quadrants to which of all these polygons have been assigned:\n\n\n\nR code\n\n# extract quadrants\nlmoran_quadrants &lt;- attr(lmoran, \"quadr\")\n\n# inspect\nhead(lmoran_quadrants)\n\n\n       mean    median     pysal\n1   Low-Low   Low-Low   Low-Low\n2   Low-Low   Low-Low   Low-Low\n3   Low-Low  Low-High   Low-Low\n4 High-High High-High High-High\n5 High-High High-High High-High\n6 High-High High-High High-High\n\n\nWe can now link these values back to our spatial dataframe and make a map using the tmap library:\n\n\n\nR code\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean\", \"lmoran_median\", \"lmoran_pysal\")\n\n# bind results\nlsoa21 &lt;- lsoa21 |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 8: Mapping the Local Moran’s I clusters.\n\n\n\n\nThis type of map is called a LISA map and is a great way of showing how a variable is actually clustering over space. However, we can improve on this further by only mapping the statistically significant clusters:\n\n\n\nR code\n\n# replace values if not significant\nlmoran_quadrants[lmoran[, 6] &gt; 0.05, ] &lt;- NA\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean_sig\", \"lmoran_median_sig\", \"lmoran_pysal_sig\")\n\n# bind results\nlsoa21 &lt;- lsoa21 |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(lsoa21) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean_sig\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 9: Mapping the significant Local Moran’s I clusters.\n\n\n\n\n\n\n\n\n\n\nThis new map may still not fully address the issue of statistical significance due to repeated testing, and some values may appear significant purely by chance. To correct for this, you can adjust the \\(p\\)-values using R’s p.adjust() function. For further details, refer to Manual Gimond’s explanation of the multiple comparison problem in the context of the pseudo-\\(p\\) values."
  },
  {
    "objectID": "04-autocorrelation.html#assignment",
    "href": "04-autocorrelation.html#assignment",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Any statistic that includes spatial weights is dependent upon how those weights are defined. We have so far used first order contiguity, i.e. polygons that share a boundary, but there is no particular reason why we should not include second order contiguity polygons (i.e. neighbours of neighbours), use a fixed distance neighbours definitions, or adopt a \\(k\\) nearest neighbours definition. Try to do the following:\n\nExtract the centroids from the lsoa21 file.\nIdentify the 5 nearest neighbours for each LSOA, using the knearneigh() function.\nCreate a neigbhour list of these nearest neighbours, using the knn2nb() function.\nCompute the Global Moran’s I of the asian_indian variable using this new neighbourhood definition.\nMap the statistically significant clusters of Local Moran’s I based on this new neighbourhood definition.\nCompare these results to the output of our asian_bangladeshi values. Do both variables exhibit clustering? Are the clusters located in similar areas?"
  },
  {
    "objectID": "04-autocorrelation.html#before-you-leave",
    "href": "04-autocorrelation.html#before-you-leave",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "And that is how you can measure spatial dependence in your dataset through different spatial autocorrelation measures. Next week we will focus on the last topic within our set of core spatial analysis methods and techniques, but this week we have covered enough! Probably time to get back to that pesky reading list."
  },
  {
    "objectID": "05-models.html",
    "href": "05-models.html",
    "title": "1 Spatial Models",
    "section": "",
    "text": "Last week, we explored spatial dependency and methods for measuring it. This week, we will examine how spatial autocorrelation can impact our analyses, especially in models where independence assumptions are crucial. To address these dependencies, we will use the GWmodel library to calculate local summary statistics for geographical areas and to fit basic spatially explicit models to our data.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nHarris, R. 2019. Chapter 8: Not just nuisance: Spatialising social statistics. In: Whitworth, A. (ed). Towards a Spatial Social Policy: Bridging the Gap Between Geography and Social Policy. Bristol: Policy Press. [Link]\nFranklin, R. 2022. Quantitative methods I: Reckoning with uncertainty. Progress in Human Geography 46(2): 689-697. [Link]\n\n\n\n\n\nBrundson, C., Fotheringham, M., and Charlton, M. 2002. Geographically Weighted Regression. Journal of the Royal Statistical Society: Series D (The Statistician) 47(3): 431-443. [Link]\nComber, A., Brundson, C., Charlton, M. et al.. 2022. A route map for successful applications of Geographically Weighted Regression. Geographical Analysis 55(1): 155-178. [Link]\n\n\n\n\n\nThis week we will investigate the political geography of England and Wales, focusing on the results of the July 2024 General Election, which was won by the Labour Party led by Keir Starmer. You will work with data extracted from two data sources: the constituency results from the election and socio-demographic information relating to age groups, economic status, and ethnic background from the 2021 Census, extracted using the Custom Dataset Tool. These datasets have been prepared and merged. Along with this dataset, we also have access to a GeoPackage that contains the boundaries of the parliamentary constituencies.\nYou can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies GE2024\ncsv\nDownload\n\n\nEngland and Wales Parliamentary Constituencies Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w05-election-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(GWmodel)\nlibrary(easystats)\nlibrary(spdep)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# load election dataset\nelec_24 &lt;- read_csv(\"data/attributes/England-Wales-GE2024-Constituency-Vars.csv\")\n\n\nRows: 575 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): constituency_code, constituency_name, region_name, winning_party, ...\ndbl (22): eligible_voters, valid_votes, conservative_votes, labour_votes, li...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load constituency boundaries\ncons_24 &lt;- st_read(\"data/spatial/England-Wales-GE2024-Boundaries.gpkg\")\n\nReading layer `England-Wales-GE2024-Boundaries' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/England-Wales-GE2024-Boundaries.gpkg' \n  using driver `GPKG'\nSimple feature collection with 575 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 85713.71 ymin: 7054.1 xmax: 655644.8 ymax: 657536.5\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(elec_24)\n\n# A tibble: 6 × 28\n  constituency_code constituency_name  region_name winning_party eligible_voters\n  &lt;chr&gt;             &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt;\n1 W07000081         Aberafan Maesteg   Wales       Lab                     72580\n2 E14001063         Aldershot          South East  Lab                     78553\n3 E14001064         Aldridge-Brownhil… West Midla… Con                     70268\n4 E14001065         Altrincham and Sa… North West  Lab                     74025\n5 W07000082         Alyn and Deeside   Wales       Lab                     75790\n6 E14001066         Amber Valley       East Midla… Lab                     71546\n# ℹ 23 more variables: valid_votes &lt;dbl&gt;, conservative_votes &lt;dbl&gt;,\n#   labour_votes &lt;dbl&gt;, libdem_votes &lt;dbl&gt;, conservative_vote_share &lt;dbl&gt;,\n#   labour_vote_share &lt;dbl&gt;, libdem_vote_share &lt;dbl&gt;,\n#   aged_15_years_and_under &lt;dbl&gt;, aged_16_to_24_years &lt;dbl&gt;,\n#   aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;, eco_not_applicable &lt;dbl&gt;,\n#   eco_active_employed &lt;dbl&gt;, eco_active_unemployed &lt;dbl&gt;, …\n\n# inspect\nhead(cons_24)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 368283 ymin: 100922.9 xmax: 524969.7 ymax: 393552.1\nProjected CRS: OSGB36 / British National Grid\n   pcon24cd                 pcon24nm pcon24nmw  bng_e  bng_n     long      lat\n1 E14001063                Aldershot           484716 155270 -0.78648 51.29027\n2 E14001064      Aldridge-Brownhills           404720 301030 -1.93172 52.60704\n3 E14001065 Altrincham and Sale West           374132 389051 -2.39049 53.39766\n4 E14001066             Amber Valley           440478 349674 -1.39771 53.04282\n5 E14001067  Arundel and South Downs           497309 118530 -0.61584 50.95798\n6 E14001068                 Ashfield           450035 356564 -1.25410 53.10395\n                                globalid                           geom\n1 {3F742659-208E-4859-8905-1CE5E2B87AAC} MULTIPOLYGON (((485406.9 15...\n2 {4668E55F-5248-45E4-9927-30562A6812D5} MULTIPOLYGON (((406519.1 30...\n3 {56FDBCDF-25DA-4EE5-AB66-6FC9C71D2927} MULTIPOLYGON (((379104.1 39...\n4 {9F9742E9-AD8C-4878-9841-1DF40BBA2619} MULTIPOLYGON (((444868.4 35...\n5 {4AFD2380-5BB7-4A01-8712-8637117E96D4} MULTIPOLYGON (((523813.2 11...\n6 {E0D1319E-FD1E-43AF-B809-79127FB22B22} MULTIPOLYGON (((455809 3579...\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nIn GEOG0018 Research Methods in Human Geography, we already worked with this electoral dataset and we hypothesised that predominantly older voters tend to support the Conservative Party. To explore this, we examined the relationship between the proportion of individuals over 50 years old in each parliamentary constituency (aged_50_years_and_over) and the proportion of votes cast for the Conservative Party (conservative_vote_share). A scatterplot and Pearson’s correlation revealed a moderate association between the two variables, suggesting a possible link between age demographics and Conservative voting patterns:\n\n\n\nR code\n\n# correlation\ncor.test(elec_24$aged_50_years_and_over, elec_24$conservative_vote_share, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  elec_24$aged_50_years_and_over and elec_24$conservative_vote_share\nt = 16.472, df = 573, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5086971 0.6199173\nsample estimates:\n      cor \n0.5668849 \n\n# scatterplot\nplot(elec_24$aged_50_years_and_over, elec_24$conservative_vote_share, xlab = \"Proportion of population over 50 years old\",\n    ylab = \"Proportion of votes for the Conservative party\")\n\n\n\n\nFigure 1: Quick scatterplot\n\n\n\n\nHowever, as you should know by now, the first step when working with spatial data is to map your variables:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  left_join(elec_24, by = c(\"pcon24cd\" = \"constituency_code\"))\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"conservative_vote_share\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#2b8cbe\", \"#045a8d\"),\n    title = \"Conservative vote share\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of votes cast for the Conservative party in the 2024 elections.\n\n\n\n\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"aged_50_years_and_over\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#f0f0f0\", \"#d9d9d9\", \"#bdbdbd\", \"#969696\", \"#737373\"),\n    title = \"50+ population share\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 3: Proportions of population over 50 years old.\n\n\n\n\nLooking at the maps, the spatial distribution of Conservative vote share and the proportion of the population over 50 appears neither random nor uniform. This raises the question to what extent the observed correlation in Figure 1 holds consistently across different constituencies? To answer this we can run a Geographically Weighted Correlation using the GWmodel library.\nGeographically Weighted Correlation (GWC) allows us to investigate whether the strength and direction of the association between variables vary across space. By applying a localised correlation technique, GWC calculates the correlation coefficient within specified spatial windows or kernels across the study area. This can reveal geographic areas where the relationship is stronger or weaker, providing more insight than a single global correlation and helping us to understand spatial heterogeneity in the data.\n\n\n\n\n\n\nThe GWmodel library allows us to calculate local statistics, such as means, standard deviations, variances, and correlations. But as with spatial autocorrelation, we must define what local means. One approach is to use a kernel function to select which values contribute to each local estimate. Kernels operate on point locations (e.g. polygon centroids) and apply a window with a specified shape and bandwidth. The bandwidth, which defines the kernel’s size, can be set in absolute terms (e.g. within 10 km) or in relative terms (e.g. the 10 nearest centroids), the latter known as an adaptive kernel.\n\n\n\nThe GWmodel library uses the older sp data format for handling spatial data. We therefore need to convert our current sf object to sp before continuing:\n\n\n\nR code\n\n# to sp\ncons_24_sp &lt;- as_Spatial(cons_24)\n\n\n\n\n\n\n\n\nThe sf package is now preferred over sp in R for its modern, efficient handling of spatial data. By using simple features, a standardised format for spatial geometries, sf is more compatible with other geospatial tools and integrates smoothly with the tidyverse, simplifying data manipulation and analysis. However, some libraries still rely on sp and have not yet transitioned to sf.\n\n\n\nWe can now calculate a geographically weighted correlation. We will use an adaptive bandwidth that estimates the local correlation using the 25 nearest constituencies:\n\n\n\nR code\n\n# geographically weighted correlation\ncons_24_cor &lt;- gwss(cons_24_sp, vars = c(\"conservative_vote_share\", \"aged_50_years_and_over\"),\n    bw = 25, adaptive = TRUE)\n\n\nWe can now extract the values and bind these back to our original sf object:\n\n\n\nR code\n\n# extract correlation\ncons_24 &lt;- cons_24 |&gt;\n    mutate(cons_age_cor = cons_24_cor$SDF$Corr_conservative_vote_share.aged_50_years_and_over)\n\n# inspect\nsummary(cons_24$cons_age_cor)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.02121  0.57869  0.72341  0.68576  0.83984  0.95214 \n\n\n\n\n\n\n\n\nThe results of the outcomes of gwss function can be accessed through the $SDF data frame.\n\n\n\nThe summary shows that there indeed seems to be variation of this relationship across the country, with the local Pearson correlation ranging from very weak to very strong. Of course, we can map this in the usual way:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"cons_age_cor\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Local correlation\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 4: Local correlation values conservative_vote_share and aged_50_years_and_over variables.\n\n\n\n\nWhile the map shows correlation patterns, it doesn not indicate statistical significance. To evaluate this, we can use a Monte Carlo simulation with the gwss.montecarlo() function. However, for a large dataset like ours, this process is computationally intensive and time-consuming, especially with more than the default 99 simulations.\n\n\n\n\n\n\nAlthough we will not run a Monte Carlo simulation here, the code below demonstrates how to run this and link the results back to the cons_24 spatial dataframe\n\n\n\n\n\n\nR code\n\n# geographically weighted correlation, monte carlo simulation\ncons_24_cor_sig &lt;- gwss.montecarlo(cons_24_sp, vars = c(\"conservative_vote_share\",\n    \"aged_50_years_and_over\"), bw = 2, adaptive = TRUE, nsim = 99) |&gt;\n    as_tibble() |&gt;\n    select(Corr_conservative_vote_share.aged_50_years_and_over)\n\n# replace names\nnames(cons_24_cor_sig) &lt;- \"cons_age_cor_p\"\n\n# bind results\ncons_24 &lt;- cons_24 |&gt;\n    cbind(cons_24_cor_sig) |&gt;\n    mutate(cons_24_cor = if_else(cons_age_cor_p &lt; 0.025, cons_age_cor_p, if_else(cons_age_cor_p &gt;\n        0.975, cons_age_cor_p, NA)))\n\n\n\n\n\n\n\n\nIt took more than 2 minutes to run the Monte Carlo simulation on an Apple MacBook Pro M1 (16GB RAM) with the default of 99 simulations.\n\n\n\n\n\n\nA correlation describes the strength and direction of a linear relationship, but if we want to quantify the change in a dependent variable (y) for a one-unit change in the independent variable(s) (x) we need to run a regression. Geographically weighted regression (GWR) is used when the relationship between a dependent and set of independent variables is not constant across space, meaning the model coefficients vary by location. This is useful when you suspect that the relationship between variables may change depending on the geographic context. GWR provides a localised understanding of the relationships by allowing each observation to have its own set of regression coefficients, which can provide insights into how relationships differ across the study area.\nGWR fits a separate regression equation at each location in the study area, weighting nearby observations more heavily than those farther away. Again, the weighting is typically based on a kernel function. The basic GWR equation is:\n\\[\ny_{i} = \\beta_{0}(\\upsilon_{i}, v_{i}) + \\sum_{k=1}^{p}\\beta_{k}(\\upsilon_{i}, v_{i})x_{ik} + \\epsilon_{i}\n\\]\nwhere \\((\\upsilon_{i}, v_{i})\\) are the coordinates of location \\(i\\) and \\(\\beta_{k}(\\upsilon_{i}, v_{i})\\) are the location-specific coefficients.\n\n\nTo predict the Conservative voter share, we hypothesise that certain socio-demographic variables may play a significant role. These independent variables include:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\neco_active_employed\nProportion of economically active employed individuals.\n\n\neco_active_unemployed\nProportion of economically active unemployed individuals.\n\n\neth_white\nProportion of the population identifying as white.\n\n\neth_black\nProportion of the population identifying as black.\n\n\naged_25_to_34_years\nProportion of the population between ages 25 and 34.\n\n\naged_50_years_and_over\nProportion of the population over age 50.\n\n\n\nWe can run the multivariate regression as follows:\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ eco_active_employed + \n    eco_active_unemployed + eth_white + eth_black + aged_25_to_34_years + \n    aged_50_years_and_over, data = cons_24)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.286730 -0.040843  0.001182  0.044496  0.220419 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.16210    0.08779  -1.846  0.06535 .  \neco_active_employed     1.36140    0.13208  10.308  &lt; 2e-16 ***\neco_active_unemployed  -0.32339    0.91429  -0.354  0.72369    \neth_white              -0.28028    0.03567  -7.857 1.97e-14 ***\neth_black              -0.29751    0.10190  -2.920  0.00364 ** \naged_25_to_34_years    -1.52695    0.20954  -7.287 1.07e-12 ***\naged_50_years_and_over  0.59711    0.09827   6.076 2.26e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06797 on 568 degrees of freedom\nMultiple R-squared:  0.5537,    Adjusted R-squared:  0.5489 \nF-statistic: 117.4 on 6 and 568 DF,  p-value: &lt; 2.2e-16\n\n\nThe model summary indicates that most variables are statistically significant predictors of Conservative vote share. The results suggest that older and employed populations are linked to higher Conservative support, while younger age groups and certain ethnic proportions show a negative relationship.\nWe can now run the diagnostics to see whether the regression assumptions are met:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"qq\", \"normality\"))\n\n\n\n\n\nFigure 5: Check for normality of residuals.\n\n\n\n\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"linearity\", \"homogeneity\"))\n\n\n\n\n\nFigure 6: Check for linearity and homogeneity\n\n\n\n\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"vif\", \"outliers\"))\n\n\n\n\n\nFigure 7: Check for influential observations and multicollinearity.\n\n\n\n\n\n\n\nThe model diagnostics seem generally fine, perhaps except for the linearity check. This could indicate a curved relationship or, alternatively, suggest spatial dependence in the relationship. If spatial dependence is present, it may imply that the observations are not independent. We can examine this by assessing whether the model residuals display spatial dependence. Let us begin with a map:\n\n\n\nR code\n\n# join model residuals onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  mutate(lm_residuals = lm_voteshare$residuals)\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"lm_residuals\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#ffffb2\", \"#fecc5c\", \"#fd8d3c\", \"#f03b20\", \"#f03b20\"),\n    title = \"Regression residuals\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 8: Regression model residuals.\n\n\n\n\nLooking at Figure 8 there appears to be some spatial structure in the model residuals. We can test this more formally using Moran’s I test to assess spatial autocorrelation:\n\n\n\nR code\n\n# create neighbour list\ncons_24_nb &lt;- poly2nb(cons_24, queen = TRUE)\n\n\nWarning in poly2nb(cons_24, queen = TRUE): some observations have no neighbours;\nif this seems unexpected, try increasing the snap argument.\n\n\nWarning in poly2nb(cons_24, queen = TRUE): neighbour object has 3 sub-graphs;\nif this sub-graph count seems unexpected, try increasing the snap argument.\n\n# create spatial weights matrix\ncons_24_nb_weights &lt;- cons_24_nb |&gt;\n    nb2listw(style = \"W\", zero.policy = TRUE)\n\n# moran's test\nmoran &lt;- moran.mc(cons_24$lm_residuals, listw = cons_24_nb_weights, nsim = 999)\n\n# results\nmoran\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  cons_24$lm_residuals \nweights: cons_24_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.4155, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe Moran’s I test indicates significant spatial autocorrelation in our model residuals, suggesting that the observations are not independent. As a result, we can proceed with a Geographically Weighted Regression to account for spatial variation in the relationship between the variables.\nWe will first estimate the ‘optimal’ bandwidth using an automated bandwidth selection procedure:\n\n\n\nR code\n\n# bandwidth selection\ncons_24_bw &lt;- bw.gwr(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24,\n    adaptive = TRUE)\n\n\nAdaptive bandwidth: 363 CV score: 2.208986 \nAdaptive bandwidth: 232 CV score: 2.105893 \nAdaptive bandwidth: 151 CV score: 1.973309 \nAdaptive bandwidth: 100 CV score: 1.940199 \nAdaptive bandwidth: 70 CV score: 2.008461 \nAdaptive bandwidth: 120 CV score: 1.945603 \nAdaptive bandwidth: 89 CV score: 1.942555 \nAdaptive bandwidth: 108 CV score: 1.938444 \nAdaptive bandwidth: 112 CV score: 1.942082 \nAdaptive bandwidth: 104 CV score: 1.938601 \nAdaptive bandwidth: 109 CV score: 1.940144 \nAdaptive bandwidth: 106 CV score: 1.939139 \nAdaptive bandwidth: 108 CV score: 1.938444 \n\n\nThe model can then be fitted.\n\n\n\nR code\n\n# run gwr\ncons_24_gwr &lt;- gwr.basic(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24,\n    adaptive = TRUE, bw = cons_24_bw)\n\n# inspect\ncons_24_gwr\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-22 18:07:55.179033 \n   Call:\n   gwr.basic(formula = conservative_vote_share ~ eco_active_employed + \n    eco_active_unemployed + eth_white + eth_black + aged_25_to_34_years + \n    aged_50_years_and_over, data = cons_24, bw = cons_24_bw, \n    adaptive = TRUE)\n\n   Dependent (y) variable:  conservative_vote_share\n   Independent variables:  eco_active_employed eco_active_unemployed eth_white eth_black aged_25_to_34_years aged_50_years_and_over\n   Number of data points: 575\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n      Min        1Q    Median        3Q       Max \n-0.286730 -0.040843  0.001182  0.044496  0.220419 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)            -0.16210    0.08779  -1.846  0.06535 .  \n   eco_active_employed     1.36140    0.13208  10.308  &lt; 2e-16 ***\n   eco_active_unemployed  -0.32339    0.91429  -0.354  0.72369    \n   eth_white              -0.28028    0.03567  -7.857 1.97e-14 ***\n   eth_black              -0.29751    0.10190  -2.920  0.00364 ** \n   aged_25_to_34_years    -1.52695    0.20954  -7.287 1.07e-12 ***\n   aged_50_years_and_over  0.59711    0.09827   6.076 2.26e-09 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 0.06797 on 568 degrees of freedom\n   Multiple R-squared: 0.5537\n   Adjusted R-squared: 0.5489 \n   F-statistic: 117.4 on 6 and 568 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 2.624389\n   Sigma(hat): 0.06767633\n   AIC:  -1451.196\n   AICc:  -1450.941\n   BIC:  -1940.526\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 108 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept               -1.089291  -0.441131  -0.160803   0.209568 1.1375\n   eco_active_employed     -1.045077   0.151632   0.930912   1.498222 2.8325\n   eco_active_unemployed  -10.747683  -4.837597   0.830042   3.163451 9.8531\n   eth_white               -1.411777  -0.365168  -0.230988  -0.109584 0.1738\n   eth_black               -3.276551  -0.308415  -0.121533   0.071703 1.1200\n   aged_25_to_34_years     -3.659487  -1.575036  -1.288467  -0.342851 1.9350\n   aged_50_years_and_over  -0.287576   0.503695   1.075497   1.383006 2.1875\n   ************************Diagnostic information*************************\n   Number of data points: 575 \n   Effective number of parameters (2trace(S) - trace(S'S)): 103.6449 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 471.3551 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): -1622.807 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): -1730.89 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): -1880.236 \n   Residual sum of squares: 1.444834 \n   R-square value:  0.7542715 \n   Adjusted R-square value:  0.7001241 \n\n   ***********************************************************************\n   Program stops at: 2024-11-22 18:07:55.272881 \n\n\nThe global regression results are derived from the standard OLS model, representing a single regression for the entire study area without accounting for spatial variation. In contrast, the geographically weighted regression (GWR) results stem from 575 separate but spatially overlapping regression models, each fitted with geographic weighting to different spatial subsets of the data, allowing for local variation in the regression estimates. We can also see that the model fit (adjusted \\(R^2\\)) has increased to 70%.\n\n\n\n\n\n\nAll the local estimates are contained in the spatial data frame cons_24_gwr$SDF. You can inspect the results using the View() function.\n\n\n\nWe can now also map all the local \\(R^2\\) values to get an idea of how well the model fit varies spatially:\n\n\n\nR code\n\n# join local r squared values onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  mutate(gwr_r2 = cons_24_gwr$SDF$Local_R2)\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"gwr_r2\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Local R-squared\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 9: Model performance of the Geographically Weighted Regression.\n\n\n\n\nIn similar fashion, and with some data wrangling, we can map the individual local coefficients of our independent variables:\n\n\n\nR code\n\n# join local coefficient values onto spatial data, select, pivot\ncons_24_coef &lt;- cons_24 |&gt;\n  mutate(\n    eco_active_employed_coef = cons_24_gwr$SDF$eco_active_employed,\n    eco_active_unemployed_coef = cons_24_gwr$SDF$eco_active_unemployed,\n    eth_white_coef = cons_24_gwr$SDF$eth_white,\n    eth_black_coef = cons_24_gwr$SDF$eth_black,\n    aged_25_to_34_years_coef = cons_24_gwr$SDF$aged_25_to_34_years,\n    aged_50_years_and_over_coef = cons_24_gwr$SDF$aged_50_years_and_over\n  ) |&gt;\n  select(40:45) |&gt;\n  pivot_longer(cols = !geom, names_to = \"variable\", values_to = \"coefficient\")\n\n# shape, polygons\ntm_shape(cons_24_coef) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"coefficient\",\n    style = \"jenks\",\n    n = 9,\n    palette = c(\"#e66101\", \"#fdb863\", \"#cccccc\", \"#b2abd2\", \"#5e3c99\"),\n    title = \"Local GWR coefficient\"\n  ) +\n\n  # facet\n  tm_facets(\n    by = \"variable\",\n    ncol = 2\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.show = TRUE,\n    legend.outside = FALSE,\n    frame = FALSE,\n    panel.labels = c(\"Age: 25-34\", \"Age: &gt;50\", \"Employed\", \"Unemployed\", \"Ethnicity: Black\", \"Ethnicity: White\"),\n  )\n\n\nUse \"fisher\" instead of \"jenks\" for larger data sets\n\n\nVariable(s) \"coefficient\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nFigure 10: Local coefficients of the Geographically Weighted Regression.\n\n\n\n\nMapping these local estimates reveals an interesting geography, where, for example, the proportion of the population over 50 years old is a better predictor for the Conservative vote share in regions like the South West, Greater London, and parts of the North West and Yorkshire and the Humber.\n\n\n\n\n\n\nNot all of the local regression estimates may be statistically significant. We can use use the estimated \\(t\\)-values, as indicated by the _TV suffix in the SDF data frame of the GWR results, to filter out those are not. For example, to filter at 95% confidence, remove \\(t\\)-values outside the range of -1.96 to +1.96.\n\n\n\n\n\n\n\n\n\nFor a more comprehensive dive into Geographically Weighted Regression, the Spatial Modelling for Data Scientists course by Liverpool-based Professors Francisco Rowe and Dani Arribas-Bel provides an excellent introduction.\n\n\n\n\n\n\n\nIn this tutorial, we have looked at geographically weighted correlation and regression. However, whilst out of the scope of this module, there are many other approaches to account for space in statistical models. Examples of such models are the spatial error model and the spatial lag model.\n\n\nThe spatial error model is used when the error terms in a regression model exhibit spatial autocorrelation, meaning the error terms are not independent across space. This can happen due to omitted variables that have a spatial pattern or unmeasured factors that affect the dependent variable similarly across nearby locations.\nThe model adjusts for spatial autocorrelation by adding a spatially lagged error term (a weighted sum of the errors from neighbouring locations) to the regression equation:\n\\[\ny = X\\beta + \\upsilon, \\upsilon = \\lambda W \\upsilon + \\epsilon\n\\]\nwhere \\(X\\beta\\) represents the standard regression components, \\(\\lambda\\) is a spatial autoregressive parameter, \\(W\\) is a spatial weights matrix, and \\(\\upsilon\\) is a vector of spatially autocorrelated errors.\n\n\n\n\n\n\nSpatial error models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial lag model is appropriate when the dependent variable itself exhibits spatial dependence. This means the value of the dependent variable in one location depends on the values in neighbouring locations. This model is used to capture the spillover effects or diffusion processes, where the outcome in one area is influenced by outcomes in nearby areas (e.g. house prices, crime rates).\nThe model incorporates a spatially lagged dependent variable, which is the weighted sum of the dependent variable values in neighbouring locations, into the regression equation:\n\\[\ny = \\rho Wy + X\\beta + \\epsilon\n\\]\nwhere \\(\\rho\\) is the spatial autoregressive coefficient, \\(Wy\\) represents the spatially lagged dependent variable, and \\(X\\beta\\) represents the standard regression components.\n\n\n\n\n\n\nSpatial lag models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial error model adjusts for spatial autocorrelation in the error terms, whereas the spatial lag model adjusts for spatial dependence in the dependent variable itself. The spatial error model does not alter the interpretation of the coefficients of the independent variables, while the spatial lag model introduces a feedback loop where changes in one area can influence neighbouring areas.\nBoth the spatial error and spatial lag models assume that the relationships between variables are the same across the study area, with adjustments made only for spatial dependencies. GWR, on the other hand, allows the relationships themselves to vary across space. GWR is more flexible but also more complex and computationally intensive, providing local instead of global estimates of coefficients.\n\n\n\n\n\nThe Geographically Weighted Regression (GWR) reveals some varying patterns in the sign and directiion of the Conservative voter share, but it is likely that other factors are at play. One could hypothesise that another potential predictor of the Conservative vote share is the proportion of the population employed in a particular industry. Try to do the following:\n\nDownload the dataset provided below and save it to the appropriate subfolder within your data directory. The csv, extracted using the Custom Dataset Tool, contains the number of employed individuals as recorded in the 2021 Census by their Standard Industrial Classification (SIC) code.\nFormulate a hypothesis regarding which of the nine main industries presented in the dataset might help predict the Conservative vote share.\nFor the chosen industry variable:\n\nCalculate the share of individuals employed in that industry.\nExtend the Geographically Weighted Regression (GWR) by including this new variable.\n\nWhat do the results suggest? Is this variable associated with any changes in Conservative vote share?\nCreate a map of the local coefficients for this variable.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies Employed by Industry\ncsv\nDownload\n\n\n\n\n\n\nThis week, we explored decomposing global measures to account for non-stationarity in spatial variables. Geographically Weighted Regression (GWR) is particularly useful when the relationship between variables varies across space, as it allows for localised regression coefficients and helps identify spatial heterogeneity that might be overlooked in global models. This was a rather heavy one, time for some light reading?"
  },
  {
    "objectID": "05-models.html#lecture-slides",
    "href": "05-models.html#lecture-slides",
    "title": "1 Spatial Models",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "05-models.html#reading-list",
    "href": "05-models.html#reading-list",
    "title": "1 Spatial Models",
    "section": "",
    "text": "Harris, R. 2019. Chapter 8: Not just nuisance: Spatialising social statistics. In: Whitworth, A. (ed). Towards a Spatial Social Policy: Bridging the Gap Between Geography and Social Policy. Bristol: Policy Press. [Link]\nFranklin, R. 2022. Quantitative methods I: Reckoning with uncertainty. Progress in Human Geography 46(2): 689-697. [Link]\n\n\n\n\n\nBrundson, C., Fotheringham, M., and Charlton, M. 2002. Geographically Weighted Regression. Journal of the Royal Statistical Society: Series D (The Statistician) 47(3): 431-443. [Link]\nComber, A., Brundson, C., Charlton, M. et al.. 2022. A route map for successful applications of Geographically Weighted Regression. Geographical Analysis 55(1): 155-178. [Link]"
  },
  {
    "objectID": "05-models.html#elections-results-in-england-and-wales",
    "href": "05-models.html#elections-results-in-england-and-wales",
    "title": "1 Spatial Models",
    "section": "",
    "text": "This week we will investigate the political geography of England and Wales, focusing on the results of the July 2024 General Election, which was won by the Labour Party led by Keir Starmer. You will work with data extracted from two data sources: the constituency results from the election and socio-demographic information relating to age groups, economic status, and ethnic background from the 2021 Census, extracted using the Custom Dataset Tool. These datasets have been prepared and merged. Along with this dataset, we also have access to a GeoPackage that contains the boundaries of the parliamentary constituencies.\nYou can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies GE2024\ncsv\nDownload\n\n\nEngland and Wales Parliamentary Constituencies Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w05-election-analysis.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(GWmodel)\nlibrary(easystats)\nlibrary(spdep)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# load election dataset\nelec_24 &lt;- read_csv(\"data/attributes/England-Wales-GE2024-Constituency-Vars.csv\")\n\n\nRows: 575 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): constituency_code, constituency_name, region_name, winning_party, ...\ndbl (22): eligible_voters, valid_votes, conservative_votes, labour_votes, li...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load constituency boundaries\ncons_24 &lt;- st_read(\"data/spatial/England-Wales-GE2024-Boundaries.gpkg\")\n\nReading layer `England-Wales-GE2024-Boundaries' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/England-Wales-GE2024-Boundaries.gpkg' \n  using driver `GPKG'\nSimple feature collection with 575 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 85713.71 ymin: 7054.1 xmax: 655644.8 ymax: 657536.5\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(elec_24)\n\n# A tibble: 6 × 28\n  constituency_code constituency_name  region_name winning_party eligible_voters\n  &lt;chr&gt;             &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt;\n1 W07000081         Aberafan Maesteg   Wales       Lab                     72580\n2 E14001063         Aldershot          South East  Lab                     78553\n3 E14001064         Aldridge-Brownhil… West Midla… Con                     70268\n4 E14001065         Altrincham and Sa… North West  Lab                     74025\n5 W07000082         Alyn and Deeside   Wales       Lab                     75790\n6 E14001066         Amber Valley       East Midla… Lab                     71546\n# ℹ 23 more variables: valid_votes &lt;dbl&gt;, conservative_votes &lt;dbl&gt;,\n#   labour_votes &lt;dbl&gt;, libdem_votes &lt;dbl&gt;, conservative_vote_share &lt;dbl&gt;,\n#   labour_vote_share &lt;dbl&gt;, libdem_vote_share &lt;dbl&gt;,\n#   aged_15_years_and_under &lt;dbl&gt;, aged_16_to_24_years &lt;dbl&gt;,\n#   aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;, eco_not_applicable &lt;dbl&gt;,\n#   eco_active_employed &lt;dbl&gt;, eco_active_unemployed &lt;dbl&gt;, …\n\n# inspect\nhead(cons_24)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 368283 ymin: 100922.9 xmax: 524969.7 ymax: 393552.1\nProjected CRS: OSGB36 / British National Grid\n   pcon24cd                 pcon24nm pcon24nmw  bng_e  bng_n     long      lat\n1 E14001063                Aldershot           484716 155270 -0.78648 51.29027\n2 E14001064      Aldridge-Brownhills           404720 301030 -1.93172 52.60704\n3 E14001065 Altrincham and Sale West           374132 389051 -2.39049 53.39766\n4 E14001066             Amber Valley           440478 349674 -1.39771 53.04282\n5 E14001067  Arundel and South Downs           497309 118530 -0.61584 50.95798\n6 E14001068                 Ashfield           450035 356564 -1.25410 53.10395\n                                globalid                           geom\n1 {3F742659-208E-4859-8905-1CE5E2B87AAC} MULTIPOLYGON (((485406.9 15...\n2 {4668E55F-5248-45E4-9927-30562A6812D5} MULTIPOLYGON (((406519.1 30...\n3 {56FDBCDF-25DA-4EE5-AB66-6FC9C71D2927} MULTIPOLYGON (((379104.1 39...\n4 {9F9742E9-AD8C-4878-9841-1DF40BBA2619} MULTIPOLYGON (((444868.4 35...\n5 {4AFD2380-5BB7-4A01-8712-8637117E96D4} MULTIPOLYGON (((523813.2 11...\n6 {E0D1319E-FD1E-43AF-B809-79127FB22B22} MULTIPOLYGON (((455809 3579...\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nIn GEOG0018 Research Methods in Human Geography, we already worked with this electoral dataset and we hypothesised that predominantly older voters tend to support the Conservative Party. To explore this, we examined the relationship between the proportion of individuals over 50 years old in each parliamentary constituency (aged_50_years_and_over) and the proportion of votes cast for the Conservative Party (conservative_vote_share). A scatterplot and Pearson’s correlation revealed a moderate association between the two variables, suggesting a possible link between age demographics and Conservative voting patterns:\n\n\n\nR code\n\n# correlation\ncor.test(elec_24$aged_50_years_and_over, elec_24$conservative_vote_share, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  elec_24$aged_50_years_and_over and elec_24$conservative_vote_share\nt = 16.472, df = 573, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5086971 0.6199173\nsample estimates:\n      cor \n0.5668849 \n\n# scatterplot\nplot(elec_24$aged_50_years_and_over, elec_24$conservative_vote_share, xlab = \"Proportion of population over 50 years old\",\n    ylab = \"Proportion of votes for the Conservative party\")\n\n\n\n\nFigure 1: Quick scatterplot\n\n\n\n\nHowever, as you should know by now, the first step when working with spatial data is to map your variables:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  left_join(elec_24, by = c(\"pcon24cd\" = \"constituency_code\"))\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"conservative_vote_share\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#2b8cbe\", \"#045a8d\"),\n    title = \"Conservative vote share\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of votes cast for the Conservative party in the 2024 elections.\n\n\n\n\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"aged_50_years_and_over\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#f0f0f0\", \"#d9d9d9\", \"#bdbdbd\", \"#969696\", \"#737373\"),\n    title = \"50+ population share\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 3: Proportions of population over 50 years old.\n\n\n\n\nLooking at the maps, the spatial distribution of Conservative vote share and the proportion of the population over 50 appears neither random nor uniform. This raises the question to what extent the observed correlation in Figure 1 holds consistently across different constituencies? To answer this we can run a Geographically Weighted Correlation using the GWmodel library.\nGeographically Weighted Correlation (GWC) allows us to investigate whether the strength and direction of the association between variables vary across space. By applying a localised correlation technique, GWC calculates the correlation coefficient within specified spatial windows or kernels across the study area. This can reveal geographic areas where the relationship is stronger or weaker, providing more insight than a single global correlation and helping us to understand spatial heterogeneity in the data.\n\n\n\n\n\n\nThe GWmodel library allows us to calculate local statistics, such as means, standard deviations, variances, and correlations. But as with spatial autocorrelation, we must define what local means. One approach is to use a kernel function to select which values contribute to each local estimate. Kernels operate on point locations (e.g. polygon centroids) and apply a window with a specified shape and bandwidth. The bandwidth, which defines the kernel’s size, can be set in absolute terms (e.g. within 10 km) or in relative terms (e.g. the 10 nearest centroids), the latter known as an adaptive kernel.\n\n\n\nThe GWmodel library uses the older sp data format for handling spatial data. We therefore need to convert our current sf object to sp before continuing:\n\n\n\nR code\n\n# to sp\ncons_24_sp &lt;- as_Spatial(cons_24)\n\n\n\n\n\n\n\n\nThe sf package is now preferred over sp in R for its modern, efficient handling of spatial data. By using simple features, a standardised format for spatial geometries, sf is more compatible with other geospatial tools and integrates smoothly with the tidyverse, simplifying data manipulation and analysis. However, some libraries still rely on sp and have not yet transitioned to sf.\n\n\n\nWe can now calculate a geographically weighted correlation. We will use an adaptive bandwidth that estimates the local correlation using the 25 nearest constituencies:\n\n\n\nR code\n\n# geographically weighted correlation\ncons_24_cor &lt;- gwss(cons_24_sp, vars = c(\"conservative_vote_share\", \"aged_50_years_and_over\"),\n    bw = 25, adaptive = TRUE)\n\n\nWe can now extract the values and bind these back to our original sf object:\n\n\n\nR code\n\n# extract correlation\ncons_24 &lt;- cons_24 |&gt;\n    mutate(cons_age_cor = cons_24_cor$SDF$Corr_conservative_vote_share.aged_50_years_and_over)\n\n# inspect\nsummary(cons_24$cons_age_cor)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.02121  0.57869  0.72341  0.68576  0.83984  0.95214 \n\n\n\n\n\n\n\n\nThe results of the outcomes of gwss function can be accessed through the $SDF data frame.\n\n\n\nThe summary shows that there indeed seems to be variation of this relationship across the country, with the local Pearson correlation ranging from very weak to very strong. Of course, we can map this in the usual way:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"cons_age_cor\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Local correlation\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 4: Local correlation values conservative_vote_share and aged_50_years_and_over variables.\n\n\n\n\nWhile the map shows correlation patterns, it doesn not indicate statistical significance. To evaluate this, we can use a Monte Carlo simulation with the gwss.montecarlo() function. However, for a large dataset like ours, this process is computationally intensive and time-consuming, especially with more than the default 99 simulations.\n\n\n\n\n\n\nAlthough we will not run a Monte Carlo simulation here, the code below demonstrates how to run this and link the results back to the cons_24 spatial dataframe\n\n\n\n\n\n\nR code\n\n# geographically weighted correlation, monte carlo simulation\ncons_24_cor_sig &lt;- gwss.montecarlo(cons_24_sp, vars = c(\"conservative_vote_share\",\n    \"aged_50_years_and_over\"), bw = 2, adaptive = TRUE, nsim = 99) |&gt;\n    as_tibble() |&gt;\n    select(Corr_conservative_vote_share.aged_50_years_and_over)\n\n# replace names\nnames(cons_24_cor_sig) &lt;- \"cons_age_cor_p\"\n\n# bind results\ncons_24 &lt;- cons_24 |&gt;\n    cbind(cons_24_cor_sig) |&gt;\n    mutate(cons_24_cor = if_else(cons_age_cor_p &lt; 0.025, cons_age_cor_p, if_else(cons_age_cor_p &gt;\n        0.975, cons_age_cor_p, NA)))\n\n\n\n\n\n\n\n\nIt took more than 2 minutes to run the Monte Carlo simulation on an Apple MacBook Pro M1 (16GB RAM) with the default of 99 simulations.\n\n\n\n\n\n\nA correlation describes the strength and direction of a linear relationship, but if we want to quantify the change in a dependent variable (y) for a one-unit change in the independent variable(s) (x) we need to run a regression. Geographically weighted regression (GWR) is used when the relationship between a dependent and set of independent variables is not constant across space, meaning the model coefficients vary by location. This is useful when you suspect that the relationship between variables may change depending on the geographic context. GWR provides a localised understanding of the relationships by allowing each observation to have its own set of regression coefficients, which can provide insights into how relationships differ across the study area.\nGWR fits a separate regression equation at each location in the study area, weighting nearby observations more heavily than those farther away. Again, the weighting is typically based on a kernel function. The basic GWR equation is:\n\\[\ny_{i} = \\beta_{0}(\\upsilon_{i}, v_{i}) + \\sum_{k=1}^{p}\\beta_{k}(\\upsilon_{i}, v_{i})x_{ik} + \\epsilon_{i}\n\\]\nwhere \\((\\upsilon_{i}, v_{i})\\) are the coordinates of location \\(i\\) and \\(\\beta_{k}(\\upsilon_{i}, v_{i})\\) are the location-specific coefficients.\n\n\nTo predict the Conservative voter share, we hypothesise that certain socio-demographic variables may play a significant role. These independent variables include:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\neco_active_employed\nProportion of economically active employed individuals.\n\n\neco_active_unemployed\nProportion of economically active unemployed individuals.\n\n\neth_white\nProportion of the population identifying as white.\n\n\neth_black\nProportion of the population identifying as black.\n\n\naged_25_to_34_years\nProportion of the population between ages 25 and 34.\n\n\naged_50_years_and_over\nProportion of the population over age 50.\n\n\n\nWe can run the multivariate regression as follows:\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ eco_active_employed + \n    eco_active_unemployed + eth_white + eth_black + aged_25_to_34_years + \n    aged_50_years_and_over, data = cons_24)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.286730 -0.040843  0.001182  0.044496  0.220419 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.16210    0.08779  -1.846  0.06535 .  \neco_active_employed     1.36140    0.13208  10.308  &lt; 2e-16 ***\neco_active_unemployed  -0.32339    0.91429  -0.354  0.72369    \neth_white              -0.28028    0.03567  -7.857 1.97e-14 ***\neth_black              -0.29751    0.10190  -2.920  0.00364 ** \naged_25_to_34_years    -1.52695    0.20954  -7.287 1.07e-12 ***\naged_50_years_and_over  0.59711    0.09827   6.076 2.26e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06797 on 568 degrees of freedom\nMultiple R-squared:  0.5537,    Adjusted R-squared:  0.5489 \nF-statistic: 117.4 on 6 and 568 DF,  p-value: &lt; 2.2e-16\n\n\nThe model summary indicates that most variables are statistically significant predictors of Conservative vote share. The results suggest that older and employed populations are linked to higher Conservative support, while younger age groups and certain ethnic proportions show a negative relationship.\nWe can now run the diagnostics to see whether the regression assumptions are met:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"qq\", \"normality\"))\n\n\n\n\n\nFigure 5: Check for normality of residuals.\n\n\n\n\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"linearity\", \"homogeneity\"))\n\n\n\n\n\nFigure 6: Check for linearity and homogeneity\n\n\n\n\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"vif\", \"outliers\"))\n\n\n\n\n\nFigure 7: Check for influential observations and multicollinearity.\n\n\n\n\n\n\n\nThe model diagnostics seem generally fine, perhaps except for the linearity check. This could indicate a curved relationship or, alternatively, suggest spatial dependence in the relationship. If spatial dependence is present, it may imply that the observations are not independent. We can examine this by assessing whether the model residuals display spatial dependence. Let us begin with a map:\n\n\n\nR code\n\n# join model residuals onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  mutate(lm_residuals = lm_voteshare$residuals)\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"lm_residuals\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#ffffb2\", \"#fecc5c\", \"#fd8d3c\", \"#f03b20\", \"#f03b20\"),\n    title = \"Regression residuals\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 8: Regression model residuals.\n\n\n\n\nLooking at Figure 8 there appears to be some spatial structure in the model residuals. We can test this more formally using Moran’s I test to assess spatial autocorrelation:\n\n\n\nR code\n\n# create neighbour list\ncons_24_nb &lt;- poly2nb(cons_24, queen = TRUE)\n\n\nWarning in poly2nb(cons_24, queen = TRUE): some observations have no neighbours;\nif this seems unexpected, try increasing the snap argument.\n\n\nWarning in poly2nb(cons_24, queen = TRUE): neighbour object has 3 sub-graphs;\nif this sub-graph count seems unexpected, try increasing the snap argument.\n\n# create spatial weights matrix\ncons_24_nb_weights &lt;- cons_24_nb |&gt;\n    nb2listw(style = \"W\", zero.policy = TRUE)\n\n# moran's test\nmoran &lt;- moran.mc(cons_24$lm_residuals, listw = cons_24_nb_weights, nsim = 999)\n\n# results\nmoran\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  cons_24$lm_residuals \nweights: cons_24_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.4155, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe Moran’s I test indicates significant spatial autocorrelation in our model residuals, suggesting that the observations are not independent. As a result, we can proceed with a Geographically Weighted Regression to account for spatial variation in the relationship between the variables.\nWe will first estimate the ‘optimal’ bandwidth using an automated bandwidth selection procedure:\n\n\n\nR code\n\n# bandwidth selection\ncons_24_bw &lt;- bw.gwr(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24,\n    adaptive = TRUE)\n\n\nAdaptive bandwidth: 363 CV score: 2.208986 \nAdaptive bandwidth: 232 CV score: 2.105893 \nAdaptive bandwidth: 151 CV score: 1.973309 \nAdaptive bandwidth: 100 CV score: 1.940199 \nAdaptive bandwidth: 70 CV score: 2.008461 \nAdaptive bandwidth: 120 CV score: 1.945603 \nAdaptive bandwidth: 89 CV score: 1.942555 \nAdaptive bandwidth: 108 CV score: 1.938444 \nAdaptive bandwidth: 112 CV score: 1.942082 \nAdaptive bandwidth: 104 CV score: 1.938601 \nAdaptive bandwidth: 109 CV score: 1.940144 \nAdaptive bandwidth: 106 CV score: 1.939139 \nAdaptive bandwidth: 108 CV score: 1.938444 \n\n\nThe model can then be fitted.\n\n\n\nR code\n\n# run gwr\ncons_24_gwr &lt;- gwr.basic(conservative_vote_share ~ eco_active_employed + eco_active_unemployed +\n    eth_white + eth_black + aged_25_to_34_years + aged_50_years_and_over, data = cons_24,\n    adaptive = TRUE, bw = cons_24_bw)\n\n# inspect\ncons_24_gwr\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-22 18:07:55.179033 \n   Call:\n   gwr.basic(formula = conservative_vote_share ~ eco_active_employed + \n    eco_active_unemployed + eth_white + eth_black + aged_25_to_34_years + \n    aged_50_years_and_over, data = cons_24, bw = cons_24_bw, \n    adaptive = TRUE)\n\n   Dependent (y) variable:  conservative_vote_share\n   Independent variables:  eco_active_employed eco_active_unemployed eth_white eth_black aged_25_to_34_years aged_50_years_and_over\n   Number of data points: 575\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n      Min        1Q    Median        3Q       Max \n-0.286730 -0.040843  0.001182  0.044496  0.220419 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)            -0.16210    0.08779  -1.846  0.06535 .  \n   eco_active_employed     1.36140    0.13208  10.308  &lt; 2e-16 ***\n   eco_active_unemployed  -0.32339    0.91429  -0.354  0.72369    \n   eth_white              -0.28028    0.03567  -7.857 1.97e-14 ***\n   eth_black              -0.29751    0.10190  -2.920  0.00364 ** \n   aged_25_to_34_years    -1.52695    0.20954  -7.287 1.07e-12 ***\n   aged_50_years_and_over  0.59711    0.09827   6.076 2.26e-09 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 0.06797 on 568 degrees of freedom\n   Multiple R-squared: 0.5537\n   Adjusted R-squared: 0.5489 \n   F-statistic: 117.4 on 6 and 568 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 2.624389\n   Sigma(hat): 0.06767633\n   AIC:  -1451.196\n   AICc:  -1450.941\n   BIC:  -1940.526\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: bisquare \n   Adaptive bandwidth: 108 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept               -1.089291  -0.441131  -0.160803   0.209568 1.1375\n   eco_active_employed     -1.045077   0.151632   0.930912   1.498222 2.8325\n   eco_active_unemployed  -10.747683  -4.837597   0.830042   3.163451 9.8531\n   eth_white               -1.411777  -0.365168  -0.230988  -0.109584 0.1738\n   eth_black               -3.276551  -0.308415  -0.121533   0.071703 1.1200\n   aged_25_to_34_years     -3.659487  -1.575036  -1.288467  -0.342851 1.9350\n   aged_50_years_and_over  -0.287576   0.503695   1.075497   1.383006 2.1875\n   ************************Diagnostic information*************************\n   Number of data points: 575 \n   Effective number of parameters (2trace(S) - trace(S'S)): 103.6449 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 471.3551 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): -1622.807 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): -1730.89 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): -1880.236 \n   Residual sum of squares: 1.444834 \n   R-square value:  0.7542715 \n   Adjusted R-square value:  0.7001241 \n\n   ***********************************************************************\n   Program stops at: 2024-11-22 18:07:55.272881 \n\n\nThe global regression results are derived from the standard OLS model, representing a single regression for the entire study area without accounting for spatial variation. In contrast, the geographically weighted regression (GWR) results stem from 575 separate but spatially overlapping regression models, each fitted with geographic weighting to different spatial subsets of the data, allowing for local variation in the regression estimates. We can also see that the model fit (adjusted \\(R^2\\)) has increased to 70%.\n\n\n\n\n\n\nAll the local estimates are contained in the spatial data frame cons_24_gwr$SDF. You can inspect the results using the View() function.\n\n\n\nWe can now also map all the local \\(R^2\\) values to get an idea of how well the model fit varies spatially:\n\n\n\nR code\n\n# join local r squared values onto spatial data\ncons_24 &lt;- cons_24 |&gt;\n  mutate(gwr_r2 = cons_24_gwr$SDF$Local_R2)\n\n# shape, polygons\ntm_shape(cons_24) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"gwr_r2\",\n    style = \"jenks\",\n    n = \"5\",\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Local R-squared\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 9: Model performance of the Geographically Weighted Regression.\n\n\n\n\nIn similar fashion, and with some data wrangling, we can map the individual local coefficients of our independent variables:\n\n\n\nR code\n\n# join local coefficient values onto spatial data, select, pivot\ncons_24_coef &lt;- cons_24 |&gt;\n  mutate(\n    eco_active_employed_coef = cons_24_gwr$SDF$eco_active_employed,\n    eco_active_unemployed_coef = cons_24_gwr$SDF$eco_active_unemployed,\n    eth_white_coef = cons_24_gwr$SDF$eth_white,\n    eth_black_coef = cons_24_gwr$SDF$eth_black,\n    aged_25_to_34_years_coef = cons_24_gwr$SDF$aged_25_to_34_years,\n    aged_50_years_and_over_coef = cons_24_gwr$SDF$aged_50_years_and_over\n  ) |&gt;\n  select(40:45) |&gt;\n  pivot_longer(cols = !geom, names_to = \"variable\", values_to = \"coefficient\")\n\n# shape, polygons\ntm_shape(cons_24_coef) +\n\n  # specify column, colours\n  tm_fill(\n    col = \"coefficient\",\n    style = \"jenks\",\n    n = 9,\n    palette = c(\"#e66101\", \"#fdb863\", \"#cccccc\", \"#b2abd2\", \"#5e3c99\"),\n    title = \"Local GWR coefficient\"\n  ) +\n\n  # facet\n  tm_facets(\n    by = \"variable\",\n    ncol = 2\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.show = TRUE,\n    legend.outside = FALSE,\n    frame = FALSE,\n    panel.labels = c(\"Age: 25-34\", \"Age: &gt;50\", \"Employed\", \"Unemployed\", \"Ethnicity: Black\", \"Ethnicity: White\"),\n  )\n\n\nUse \"fisher\" instead of \"jenks\" for larger data sets\n\n\nVariable(s) \"coefficient\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nFigure 10: Local coefficients of the Geographically Weighted Regression.\n\n\n\n\nMapping these local estimates reveals an interesting geography, where, for example, the proportion of the population over 50 years old is a better predictor for the Conservative vote share in regions like the South West, Greater London, and parts of the North West and Yorkshire and the Humber.\n\n\n\n\n\n\nNot all of the local regression estimates may be statistically significant. We can use use the estimated \\(t\\)-values, as indicated by the _TV suffix in the SDF data frame of the GWR results, to filter out those are not. For example, to filter at 95% confidence, remove \\(t\\)-values outside the range of -1.96 to +1.96.\n\n\n\n\n\n\n\n\n\nFor a more comprehensive dive into Geographically Weighted Regression, the Spatial Modelling for Data Scientists course by Liverpool-based Professors Francisco Rowe and Dani Arribas-Bel provides an excellent introduction.\n\n\n\n\n\n\n\nIn this tutorial, we have looked at geographically weighted correlation and regression. However, whilst out of the scope of this module, there are many other approaches to account for space in statistical models. Examples of such models are the spatial error model and the spatial lag model.\n\n\nThe spatial error model is used when the error terms in a regression model exhibit spatial autocorrelation, meaning the error terms are not independent across space. This can happen due to omitted variables that have a spatial pattern or unmeasured factors that affect the dependent variable similarly across nearby locations.\nThe model adjusts for spatial autocorrelation by adding a spatially lagged error term (a weighted sum of the errors from neighbouring locations) to the regression equation:\n\\[\ny = X\\beta + \\upsilon, \\upsilon = \\lambda W \\upsilon + \\epsilon\n\\]\nwhere \\(X\\beta\\) represents the standard regression components, \\(\\lambda\\) is a spatial autoregressive parameter, \\(W\\) is a spatial weights matrix, and \\(\\upsilon\\) is a vector of spatially autocorrelated errors.\n\n\n\n\n\n\nSpatial error models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial lag model is appropriate when the dependent variable itself exhibits spatial dependence. This means the value of the dependent variable in one location depends on the values in neighbouring locations. This model is used to capture the spillover effects or diffusion processes, where the outcome in one area is influenced by outcomes in nearby areas (e.g. house prices, crime rates).\nThe model incorporates a spatially lagged dependent variable, which is the weighted sum of the dependent variable values in neighbouring locations, into the regression equation:\n\\[\ny = \\rho Wy + X\\beta + \\epsilon\n\\]\nwhere \\(\\rho\\) is the spatial autoregressive coefficient, \\(Wy\\) represents the spatially lagged dependent variable, and \\(X\\beta\\) represents the standard regression components.\n\n\n\n\n\n\nSpatial lag models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial error model adjusts for spatial autocorrelation in the error terms, whereas the spatial lag model adjusts for spatial dependence in the dependent variable itself. The spatial error model does not alter the interpretation of the coefficients of the independent variables, while the spatial lag model introduces a feedback loop where changes in one area can influence neighbouring areas.\nBoth the spatial error and spatial lag models assume that the relationships between variables are the same across the study area, with adjustments made only for spatial dependencies. GWR, on the other hand, allows the relationships themselves to vary across space. GWR is more flexible but also more complex and computationally intensive, providing local instead of global estimates of coefficients."
  },
  {
    "objectID": "05-models.html#assignment",
    "href": "05-models.html#assignment",
    "title": "1 Spatial Models",
    "section": "",
    "text": "The Geographically Weighted Regression (GWR) reveals some varying patterns in the sign and directiion of the Conservative voter share, but it is likely that other factors are at play. One could hypothesise that another potential predictor of the Conservative vote share is the proportion of the population employed in a particular industry. Try to do the following:\n\nDownload the dataset provided below and save it to the appropriate subfolder within your data directory. The csv, extracted using the Custom Dataset Tool, contains the number of employed individuals as recorded in the 2021 Census by their Standard Industrial Classification (SIC) code.\nFormulate a hypothesis regarding which of the nine main industries presented in the dataset might help predict the Conservative vote share.\nFor the chosen industry variable:\n\nCalculate the share of individuals employed in that industry.\nExtend the Geographically Weighted Regression (GWR) by including this new variable.\n\nWhat do the results suggest? Is this variable associated with any changes in Conservative vote share?\nCreate a map of the local coefficients for this variable.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies Employed by Industry\ncsv\nDownload"
  },
  {
    "objectID": "05-models.html#before-you-leave",
    "href": "05-models.html#before-you-leave",
    "title": "1 Spatial Models",
    "section": "",
    "text": "This week, we explored decomposing global measures to account for non-stationarity in spatial variables. Geographically Weighted Regression (GWR) is particularly useful when the relationship between variables varies across space, as it allows for localised regression coefficients and helps identify spatial heterogeneity that might be overlooked in global models. This was a rather heavy one, time for some light reading?"
  },
  {
    "objectID": "08-network.html",
    "href": "08-network.html",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Accessibility is often described as the ease with which individuals can reach places and opportunities, such as employment, public services, and cultural activities. We can utilise transport network data to quantify accessibility and characterise areas based on their accessibility levels. This week, we will use the dodgr R library to measure accessibility between different points of interest by calculating the network distances between them.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nGeurs, K. and Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\nVerduzco Torres, J. R. and McArthur, D.P. 2024. Public transport accessibility indicators to urban and regional services in Great Britain. Scientific Data 11: 53. [Link]\n\n\n\n\n\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]\n\n\n\n\n\nThis week, we will analyse the accessibility of fast-food outlets in the London Borough of Lambeth. Specifically, we will examine how closely these outlets are located within walking distance of primary and secondary schools, and explore any potential relationships between their proximity and the relative levels of deprivation in the area.\nWe will extract the points of interest that we will use for this analysis from the Point of Interest (POI) data for the United Kingdom, obtained from the Overture Maps Foundation and pre-processed by the Consumer Data Research Centre to provide users with easy access.\nYou can download a subset of the POI dataset via the link provided below. A copy of the 2011 London LSOAs spatial boundaries, the boundaries of the London Boroughs, and the 2019 English Index of Multiple Deprivation. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLambeth Overture Points of Interest 2024\nGeoPackage\nDownload\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\nEngland 2019 Index of Multiple Deprivation\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nYou may have already downloaded some of these datasets in previous weeks, but for completeness, they are all provided here. Only download the datasets you do not already have or did not save.\n\n\n\n\n\n\n\n\n\nTo extract the Lambeth Overture Points of Interest data, a 2-kilometre buffer was applied around the boundaries of Lambeth. This approach ensures that points just outside the study area are included, as locations beyond the borough boundary may still be accessible to residents and could represent the nearest available options.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w08-accessibility-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the spatial data into R.\n\n\n\nR code\n\n# read poi data\npoi24 &lt;- st_read(\"data/spatial/Lambeth-POI-2024.gpkg\")\n\n\nReading layer `Lambeth-POI-2024' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/Lambeth-POI-2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 65060 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526556.6 ymin: 167827 xmax: 535640.4 ymax: 182673.8\nProjected CRS: OSGB36 / British National Grid\n\n# read lsoa dataset\nlsoa11 &lt;- st_read(\"data/spatial/London-LSOA-2011.gpkg\")\n\nReading layer `London-LSOA-2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4835 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# read borough dataset\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\")\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect poi data\nhead(poi24)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526913.4 ymin: 169695.2 xmax: 526945.5 ymax: 169970.8\nProjected CRS: OSGB36 / British National Grid\n                                id    primary_name       main_category\n1 08f194ada9716b86030eab41bbd4207e \"Gorgeous Grub\" \"burger_restaurant\"\n2 08f194ada9715a1903d73f4aef170602    \"TLC Direct\"   \"wholesale_store\"\n3 08f194ada944cba203fa613de4f5e6d5     \"JD Sports\"       \"sports_wear\"\n4 08f194ada9449a8a0345a466a0a6ece9       \"Lidl GB\"       \"supermarket\"\n                    alternate_category                                 address\n1   eat_and_drink|fast_food_restaurant                 \"1 Prince Georges Road\"\n2 professional_services|lighting_store                      \"280 Western Road\"\n3            sporting_goods|shoe_store \"Unit 2 Tandem Centre Top Of Church Rd\"\n4          retail|fast_food_restaurant                         \"Colliers Wood\"\n         locality   postcode region country source   source_record_id\n1        \"London\"   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\"  \"232538816864698\"\n2        \"London\" \"SW19 2QA\"  \"ENG\"    \"GB\" \"meta\" \"1959707454355017\"\n3 \"Colliers Wood\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"644899945690935\"\n4        \"London\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"111430837210163\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526921.1 16969...\n3 MULTIPOINT ((526915.7 16997...\n4 MULTIPOINT ((526922.2 16988...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect lsoa dataset\nhead(lsoa11)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa11cd            lsoa11nm           lsoa11nmw  bng_e  bng_n     long\n1 E01000001 City of London 001A City of London 001A 532129 181625 -0.09706\n2 E01000002 City of London 001B City of London 001B 532480 181699 -0.09197\n3 E01000003 City of London 001C City of London 001C 532245 182036 -0.09523\n4 E01000005 City of London 001E City of London 001E 533581 181265 -0.07628\n       lat                               globalid          lsoa11_name pop2011\n1 51.51810 {283B0EAD-F8FC-40B6-9A79-1DDD7E5C0758}  City of London 001A    1465\n2 51.51868 {DDCE266B-7825-428C-9E0A-DF66B0179A55}  City of London 001B    1436\n3 51.52176 {C45E358E-A794-485A-BF76-D96E5D458EA4}  City of London 001C    1346\n4 51.51452 {4DDAF5E4-E47F-4312-89A0-923FFEC028A6}  City of London 001E     985\n                            geom\n1 MULTIPOLYGON (((532105.1 18...\n2 MULTIPOLYGON (((532634.5 18...\n3 MULTIPOLYGON (((532135.1 18...\n4 MULTIPOLYGON (((533808 1807...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect borough dataset\nhead(borough)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 507007.4 ymin: 155850.8 xmax: 561957.5 ymax: 194889.3\nProjected CRS: OSGB36 / British National Grid\n  objectid                 name  gss_code  hectares nonld_area ons_inner\n1        1 Kingston upon Thames E09000021  3726.117      0.000         F\n2        2              Croydon E09000008  8649.441      0.000         F\n3        3              Bromley E09000006 15013.487      0.000         F\n4        4             Hounslow E09000018  5658.541     60.755         F\n5        5               Ealing E09000009  5554.428      0.000         F\n6        6             Havering E09000016 11445.735    210.763         F\n  sub_2011                           geom\n1    South POLYGON ((516401.6 160201.8...\n2    South POLYGON ((535009.2 159504.7...\n3    South POLYGON ((540373.6 157530.4...\n4     West POLYGON ((509703.4 175356.6...\n5     West POLYGON ((515647.2 178787.8...\n6     East POLYGON ((553564 179127.1, ...\n\n\n\n\nThe inspection shows that the POI dataset contains a wide variety of location types, with each point tagged under a main and alternative category, as provided by the Overture Maps Foundation via Meta and Microsoft. However, these tags may not be consistent across the dataset, so we will need to identify specific keywords to filter the main_category and alternate_category columns.\nWe will start by filtering out all POIs where the word school features in the main_category column:\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"school\"))\n\n# inspect\nhead(unique(poi_schools$main_category), n = 50)\n\n\n [1] \"\\\"day_care_preschool\\\"\"              \"\\\"driving_school\\\"\"                 \n [3] \"\\\"elementary_school\\\"\"               \"\\\"school\\\"\"                         \n [5] \"\\\"language_school\\\"\"                 \"\\\"music_school\\\"\"                   \n [7] \"\\\"specialty_school\\\"\"                \"\\\"preschool\\\"\"                      \n [9] \"\\\"dance_school\\\"\"                    \"\\\"high_school\\\"\"                    \n[11] \"\\\"drama_school\\\"\"                    \"\\\"cooking_school\\\"\"                 \n[13] \"\\\"middle_school\\\"\"                   \"\\\"vocational_and_technical_school\\\"\"\n[15] \"\\\"art_school\\\"\"                      \"\\\"private_school\\\"\"                 \n[17] \"\\\"religious_school\\\"\"                \"\\\"nursing_school\\\"\"                 \n[19] \"\\\"montessori_school\\\"\"               \"\\\"public_school\\\"\"                  \n[21] \"\\\"cosmetology_school\\\"\"              \"\\\"medical_school\\\"\"                 \n[23] \"\\\"engineering_schools\\\"\"             \"\\\"massage_school\\\"\"                 \n[25] \"\\\"business_schools\\\"\"                \"\\\"law_schools\\\"\"                    \n[27] \"\\\"medical_sciences_schools\\\"\"        \"\\\"sports_school\\\"\"                  \n[29] \"\\\"flight_school\\\"\"                  \n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nThis is still a very large list, and looking at the categories not all POIs containing the string school should be included. However, this initial selection has given us a more manageable list from which we can choose the relevant tags. We can now further filter the dataset as well as clip the dataset to the administrative boundaries of Lambeth.\n\n\n\nR code\n\n# remove quotes for easier processing\npoi_schools &lt;- poi_schools |&gt;\n    mutate(main_category = str_replace_all(main_category, \"\\\"\", \"\"))\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(main_category == \"elementary_school\" | main_category == \"high_school\" |\n        main_category == \"middle_school\" | main_category == \"private_school\" | main_category ==\n        \"public_school\" | main_category == \"school\")\n\n# filter school poi data\nlambeth &lt;- borough |&gt;\n    filter(name == \"Lambeth\")\n\npoi_schools &lt;- poi_schools |&gt;\n    st_intersection(lambeth) |&gt;\n    select(1:11)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 141 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 169846.4 xmax: 533065.9 ymax: 180398\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                 id\n6  08f194ad1a394235035f3ab7c2e4721d\n7  08f194ad1a8da734035945d69c357ddd\n8  08f194ad1abb648603defd9d76b4c314\n27 08f194ad130f0cd303c1c9f9b42438f8\n                                               primary_name     main_category\n6                         \"Woodmansterne Children's Centre\" elementary_school\n7   \"Immanuel & St Andrew Church of England Primary School\"            school\n8  \"Monkey Puzzle Day Nursery & Preschool Streatham Common\"            school\n27                                     \"Campsbourne School\"            school\n                        alternate_category                   address locality\n6                         school|education            \"Stockport Rd\"     &lt;NA&gt;\n7              elementary_school|education           \"Northanger Rd\"     &lt;NA&gt;\n8  education|public_service_and_government \"496 Streatham High Road\" \"London\"\n27                               education                      &lt;NA&gt; \"London\"\n     postcode region country source   source_record_id\n6  \"SW16 5XE\"   &lt;NA&gt;    \"GB\" \"meta\"  \"114577088601307\"\n7  \"SW16 5SL\"   &lt;NA&gt;    \"GB\" \"meta\"  \"128479257200832\"\n8  \"SW16 3QB\"  \"ENG\"    \"GB\" \"meta\" \"1092187950854118\"\n27       &lt;NA&gt;   &lt;NA&gt;    \"GB\" \"meta\"  \"114411542481619\"\n                        geom\n6  POINT (529701.5 169846.4)\n7  POINT (530016.4 170574.1)\n8  POINT (530208.6 170587.9)\n27 POINT (528819.8 174228.7)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nThis is still a rather long list and likely inaccurate. According to Lambeth Council Education Statistics, there should be 80 primary and secondary schools across the borough. We can use the alternate_category column to further narrow down our results.\n\n\n\n\n\n\nYou can inspect the different tags and their frequencies easily by creating a frequency table: table(poi_schools$alternate_category).\n\n\n\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(str_detect(alternate_category, \"elementary_school\") | str_detect(alternate_category,\n        \"high_school\") | str_detect(alternate_category, \"middle_school\") | str_detect(alternate_category,\n        \"private_school\") | str_detect(alternate_category, \"public_school\"))\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 58 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id\n1 08f194ad1a8da734035945d69c357ddd\n2 08f194ad1a70460d037da737c256001b\n3 08f194ad1c2dc81c032e9e0aa296a8d1\n4 08f194ad1e4cec5903fafb7496a2d2f3\n                                             primary_name     main_category\n1 \"Immanuel & St Andrew Church of England Primary School\"            school\n2                                \"Granton Primary School\" elementary_school\n3                 \"Kingswood Primary School (Upper Site)\" elementary_school\n4                              \"Battersea Grammar School\"            school\n           alternate_category          address locality   postcode region\n1 elementary_school|education  \"Northanger Rd\"     &lt;NA&gt; \"SW16 5SL\"   &lt;NA&gt;\n2        school|public_school     \"Granton Rd\"     &lt;NA&gt; \"SW16 5AN\"   &lt;NA&gt;\n3          school|high_school \"193 Gipsy Road\" \"London\"   \"SE27 9\"  \"ENG\"\n4       high_school|education             &lt;NA&gt; \"London\"       &lt;NA&gt;   &lt;NA&gt;\n  country source  source_record_id                      geom\n1    \"GB\" \"meta\" \"128479257200832\" POINT (530016.4 170574.1)\n2    \"GB\" \"meta\" \"235737420093504\" POINT (529299.7 170025.3)\n3    \"GB\" \"meta\" \"110066125723254\" POINT (532897.2 171498.4)\n4    \"GB\" \"meta\" \"103107239728950\" POINT (529523.9 172310.9)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nSince the POI dataset is compiled from various open sources, the data quality is not guaranteed. Some schools may be missing, while others could be duplicated, perhaps under slightly different names or because different buildings have been assigned separate point locations. However, it is unlikely that more than one school would share the same postcode. Therefore, we will use postcode information (where available) to finalise our school selection and remove any likely duplicates.\n\n\n\nR code\n\n# identify duplicate postcodes\npoi_schools &lt;- poi_schools |&gt;\n    group_by(postcode) |&gt;\n    mutate(rank = rank(primary_name)) |&gt;\n    ungroup()\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(is.na(postcode) | rank == 1) |&gt;\n    select(-rank)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 54 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 54 × 12\n   id    primary_name main_category alternate_category address locality postcode\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1 08f1… \"\\\"Immanuel… school        elementary_school… \"\\\"Nor…  &lt;NA&gt;    \"\\\"SW16…\n 2 08f1… \"\\\"Granton … elementary_s… school|public_sch… \"\\\"Gra…  &lt;NA&gt;    \"\\\"SW16…\n 3 08f1… \"\\\"Kingswoo… elementary_s… school|high_school \"\\\"193… \"\\\"Lond… \"\\\"SE27…\n 4 08f1… \"\\\"Batterse… school        high_school|educa…  &lt;NA&gt;   \"\\\"Lond…  &lt;NA&gt;   \n 5 08f1… \"\\\"St Bede'… school        elementary_school… \"\\\"St …  &lt;NA&gt;    \"\\\"SW12…\n 6 08f1… \"\\\"St Leona… school        elementary_school… \"\\\"42 … \"\\\"Lond… \"\\\"SW16…\n 7 08f1… \"\\\"Richard … elementary_s… college_universit… \"\\\"New…  &lt;NA&gt;    \"\\\"SW2 …\n 8 08f1… \"\\\"Henry Ca… school        high_school|eleme… \"\\\"Hyd…  &lt;NA&gt;    \"\\\"SW12…\n 9 08f1… \"\\\"South Ba… school        high_school|b2b_s… \"\\\"56 … \"\\\"Lond… \"\\\"SW2 …\n10 08f1… \"\\\"Glenbroo… elementary_s… school|public_sch… \"\\\"Cla…  &lt;NA&gt;    \"\\\"SW4 …\n# ℹ 44 more rows\n# ℹ 5 more variables: region &lt;chr&gt;, country &lt;chr&gt;, source &lt;chr&gt;,\n#   source_record_id &lt;chr&gt;, geom &lt;POINT [m]&gt;\n\n\nAlthough we now have fewer schools than we had expected, either due to overly restrictive filtering of tags or because some school locations are not recorded in the dataset, we will proceed with the current data.\n\n\n\n\n\n\nVariable preparation can be a time-consuming process that often necessitates a more extensive exploratory analysis to ensure sufficient data quality. This may involve sourcing additional data to supplement your existing dataset.\n\n\n\nWe can use a similar approach to approximate the locations of fast food outlets in the Borough.\n\n\n\nR code\n\n# filter fast food poi data\npoi_fastfood &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"fast_food_restaurant\") | str_detect(alternate_category,\n        \"fast_food_restaurant\") | str_detect(alternate_category, \"chicken_restaurant\") |\n        str_detect(alternate_category, \"burger_restaurant\"))\n\n# inspect\npoi_fastfood\n\n\nSimple feature collection with 1444 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526666.3 ymin: 168272.9 xmax: 535546.9 ymax: 182554\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id     primary_name          main_category\n1 08f194ada9716b86030eab41bbd4207e  \"Gorgeous Grub\"    \"burger_restaurant\"\n2 08f194ada9449a8a0345a466a0a6ece9        \"Lidl GB\"          \"supermarket\"\n3 08f194ada944daa80328c6604dab3503     \"Moss Bros.\" \"men's_clothing_store\"\n4 08f194ada932ad8603db11bbb7f953a7 \"Livi's Cuisine\"   \"african_restaurant\"\n                  alternate_category                          address  locality\n1 eat_and_drink|fast_food_restaurant          \"1 Prince Georges Road\"  \"London\"\n2        retail|fast_food_restaurant                  \"Colliers Wood\"  \"London\"\n3               fast_food_restaurant \"Unit 5, Tandem Shopping Centre\"  \"London\"\n4       caterer|fast_food_restaurant                   \"1 Locks Lane\" \"Mitcham\"\n    postcode region country source  source_record_id\n1   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\" \"232538816864698\"\n2 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"111430837210163\"\n3 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"478090646011341\"\n4    \"CR4 2\"  \"ENG\"    \"GB\" \"meta\" \"231745500530140\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526922.2 16988...\n3 MULTIPOINT ((526945.5 16992...\n4 MULTIPOINT ((527970.3 16955...\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nLet’s map both datasets to get an idea of how the data look like:\n\n\n\nR code\n\n# combine for mapping\npoi_schools &lt;- poi_schools |&gt;\n  mutate(type = \"School\")\npoi_fastfood &lt;- poi_fastfood |&gt;\n  mutate(type = \"Fast food\")\npoi_lambeth &lt;- rbind(poi_schools, poi_fastfood)\n\n# shape, polygon\ntm_shape(lambeth) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(poi_lambeth) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"type\",\n    size = 0.05,\n    palette = c(\"#beaed4\", \"#fdc086\"),\n    title = \"\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Extracted school and fast food locations for Lambeth.\n\n\n\n\n\n\n\nIn addition to the locations of interest, we need network data to assess the accessibility of schools in relation to fast food outlets. We will use OpenStreetMap to extract road segment data. Similar to the POI dataset, OSM uses key and value tags to categorise the features within its dataset.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world, but its coverage is uneven globally. However, the accuracy and quality of the data can at times be questionable, with details such as road types and speed limits missing. The OpenStreetMap Wiki provides more details on the tagging system.\n\n\n\nTo download the Lambeth road network dataset, we first need to define our bounding box coordinates. We will then use these coordinates in our OSM query to extract specific types of road segments within the defined search area. Our focus will be on selecting all OSM features with the highway tag that are likely to be used by pedestrians (e.g. excluding motorways).\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_lambeth &lt;- poi24 |&gt;\n    st_transform(4326) |&gt;\n    st_bbox()\n\n# osm query\nosm_network &lt;- opq(bbox = bbox_lambeth) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Roads.RData')\n\n\n\nThe returned osm_network object contains a variety of elements with the specified tags. Our next step is to extract the spatial data from this object to create our road network dataset. Specifically, we will extract the edges of the network, which represent the lines of the roads, as well as the nodes, which represent the points where the roads start, end, or intersect.\n\n\n\nR code\n\n# extract the nodes, with their osm_id\nosm_network_nodes &lt;- osm_network$osm_points[, \"osm_id\"]\n\n# extract the edges, with their osm_id and relevant columns\nosm_network_edges &lt;- osm_network$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n# inspect\nhead(osm_network_nodes)\n\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1541499 ymin: 51.52434 xmax: -0.1457924 ymax: 51.52698\nGeodetic CRS:  WGS 84\n      osm_id                    geometry\n78112  78112 POINT (-0.1457924 51.52698)\n99878  99878 POINT (-0.1529787 51.52434)\n99879  99879 POINT (-0.1532934 51.52482)\n99880  99880 POINT (-0.1535802 51.52508)\n99882  99882 POINT (-0.1541499 51.52567)\n99883  99883 POINT (-0.1541362 51.52598)\n\n# inspect\nhead(osm_network_edges)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -0.1398347 ymin: 51.50608 xmax: -0.0821093 ymax: 51.5246\nGeodetic CRS:  WGS 84\n         osm_id                 name     highway maxspeed oneway\n31030     31030          Grafton Way     primary   20 mph    yes\n31039     31039 Tottenham Court Road     primary   20 mph   &lt;NA&gt;\n31959     31959     Cleveland Street residential   20 mph    yes\n554369   554369  King William Street    tertiary   20 mph    yes\n554526   554526     Fenchurch Street    tertiary   20 mph   &lt;NA&gt;\n1530592 1530592  Borough High Street     primary   30 mph    yes\n                              geometry\n31030   LINESTRING (-0.1349153 51.5...\n31039   LINESTRING (-0.1303693 51.5...\n31959   LINESTRING (-0.139512 51.52...\n554369  LINESTRING (-0.08745 51.511...\n554526  LINESTRING (-0.085135 51.51...\n1530592 LINESTRING (-0.0882957 51.5...\n\n\nWe can quickly map the network edges to see how the road network looks like:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(osm_network_edges) +\n\n  # specify column, classes\n  tm_lines(\n    col = \"#bdbdbd\",\n    lwd = 0.2,\n  ) +\n\n  # shape, polygon\n  tm_shape(lambeth) +\n\n  # specify column, classes\n  tm_borders(\n    col = \"#252525\",\n    lwd = 2\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Road segments\",\n    col = \"#bdbdbd\"\n  ) +\n\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Outline Lambeth\",\n    col = \"#252525\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1\n  )\n\n\n\n\n\nFigure 2: Extracted OpenStreetMap road network data for Lambeth.\n\n\n\n\n\n\n\nSince our focus is on schoolchildren and walking distances, we will overwrite the oneway variable to assume that none of the road segments are restricted to one-way traffic. This adjustment will ensure our analysis is not skewed by such restrictions and will help maintain a more accurate representation of the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nosm_network_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics with our prepared point of interest data.\nIn any network analysis, the primary data structure is a graph composed of nodes and edges. The dodgr library utilises weighting profiles to assign weights based on road types, tailored to the mode of transport that each profile is designed to model. In this instance, we will use the foot weighting profile, as our focus is on modelling walking accessibility. To prevent errors related to the weighting profile, we will replace any NA values in the highway tag with the value unclassified.\n\n\n\nR code\n\n# replace missing highway tags with unclassified\nosm_network_edges &lt;- osm_network_edges |&gt;\n    mutate(highway = if_else(is.na(highway), \"unclassified\", highway))\n\n# create network graph\nosm_network_graph &lt;- weight_streetnet(osm_network_edges, wt_profile = \"foot\")\n\n\nOnce we have constructed our graph, we can use it to calculate network distances between our points of interest. One important consideration is that not all individual components in the extracted network may be connected. This can occur, for example, if the bounding box cuts off access to the road of a cul-de-sac. To ensure that our entire extracted network is connected, we will therefore extract the largest connected component of the graph.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always representing the largest component. It is essential to inspect the resulting subgraph to ensure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\nnetx_connected &lt;- osm_network_graph[osm_network_graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(netx_connected)\n\n\n[1] 440474\n\n\n\n\n\n\n\n\nOpenStreetMap is a dynamic dataset, meaning that changes are made on a continuous basis. As a result, it is quite possible that the number of remaining road segments, as shown above, may differ slightly when you run this analysis.\n\n\n\n\n\n\nNow that we have our connected subgraph, we can use the dodgr_distances() function to calculate the network distances between every possible origin (i.e. school) and destination (i.e. fast food outlet). For all combinations, the function will map the point of interest locations to the nearest point on the network and return the corresponding shortest-path distances.\n\n\n\n\n\n\nThe dodgr package requires data to be projected in WGS84, so we need to reproject our point of interest data accordingly.\n\n\n\n\n\n\nR code\n\n# reproject\npoi_schools &lt;- poi_schools |&gt;\n    st_transform(4326)\npoi_fastfood &lt;- poi_fastfood |&gt;\n    st_transform(4326)\n\n# distance matrix\ndistance_matrix &lt;- dodgr_distances(netx_connected, from = st_coordinates(poi_schools),\n    to = st_coordinates(poi_fastfood), shortest = FALSE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance matrix that contains the network distances between all origins (i.e. schools) and all destinations (i.e. fast-food outlets):\n\n\n\nR code\n\n# inspect\ndistance_matrix[1:5, 1:5]\n\n\n            6807494201 7110321980 7110321980 11371586827 33148215\n8796433764    4660.831   4661.009   4661.009    3128.948 3087.031\n8820889464    3611.758   3753.383   3753.383    1957.011 1915.094\n11479633279   8497.581   8497.760   8497.760    6940.464 6898.547\n292521291     4917.554   4917.732   4917.732    4222.538 4287.953\n12331531180   6270.840   6271.018   6271.018    5575.824 5641.240\n\n\n\n\n\n\n\n\nThe above output displays the distance (in metres) between the first five schools and the first five fast-food outlets. The row and column IDs refer to the nearest nodes on the OSM network to which the schools and fast-food outlets were mapped.\n\n\n\nNow that we have the distance matrix, we can aggregate the data and perform accessibility analysis. For example, we can count the number of fast-food outlets within 500 or 1,000 metres walking distance from each school:\n\n\n\nR code\n\n# fast-food outlets within 500m\npoi_schools$fastfood_500m &lt;- rowSums(distance_matrix &lt;= 500)\n\n# fast-food outlets within 1000m\npoi_schools$fastfood_1000m &lt;- rowSums(distance_matrix &lt;= 1000)\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nIn the final step, we can investigate whether there is a relationship between the proximity of fast-food outlets and the relative levels of deprivation in the area. One approach is to calculate the average number of fast-food outlets within 1,000 metres of a school for each LSOA, and then compare these figures to their corresponding IMD deciles.\n\n\n\nR code\n\n# read imd dataset\nimd19 &lt;- read_csv(\"data/attributes/England-IMD-2019.csv\")\n\n\nRows: 32844 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lsoa11cd\ndbl (2): imd_rank, imd_dec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# join imd\nlsoa11 &lt;- lsoa11 |&gt;\n  left_join(imd19, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n# join schools to their parent lsoa\npoi_schools &lt;- poi_schools |&gt;\n  st_transform(27700) |&gt;\n  st_join(lsoa11)\n\nWe can use this approach to derive the average number of fast-food by IMD decile:\n\n\n\nR code\n\n# average counts by imd decile\nfastfood_imd &lt;- poi_schools |&gt;\n    group_by(imd_dec) |&gt;\n    mutate(avg_cnt = mean(fastfood_1000m)) |&gt;\n    distinct(imd_dec, avg_cnt) |&gt;\n    arrange(imd_dec)\n\n# inspect\nfastfood_imd\n\n\n# A tibble: 7 × 2\n# Groups:   imd_dec [7]\n  imd_dec avg_cnt\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       2   20.1 \n2       3   14.3 \n3       4   17.5 \n4       5    9.83\n5       6    3   \n6       7    8.4 \n7       8   23.5 \n\n\nThere appears to be a weak relationship, with schools in more deprived areas having, on average, a higher number of fast-food outlets within a 1,000-metre walking distance. However, this trend is not consistent, as schools in the least deprived areas of Lambeth show the highest accessibility on average.\n\n\n\n\nAccessibility analysis involves evaluating how easily people can reach essential services, destinations, or opportunities, such as schools, healthcare facilities, or workplaces, from a given location. The CDRC Access to Healthy Assets & Hazards (AHAH) dataset, for instance, uses accessibility analysis to quantify how easy it is to reach ‘unhealthy’ places, such as pubs and gambling outlets, for each neighbourhood in Great Britain.\nHaving run through all the steps during the tutorial, we can recreate this analysis ourselves. Using Lambeth as a case study, try to complete the following tasks:\n\nExtract all pubs from the Point of Interest dataset.\nFor each LSOA within Lambeth, calculate the average walking distance to the nearest pub.\nCreate a map of the results.\n\n\n\n\n\n\n\nUnlike before, LSOAs are now the unit of analysis. This means you will need to input the LSOA centroids into your distance matrix.\n\n\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R.\n\n\n\n\n\n\nThis brings us to the end of the tutorial. You should now have a basic understanding of the concepts behind accessibility analysis, how it can be executed in R, and some of the challenges you may encounter when conducting your own research. With this being said, you have now reached the end of this week’s content. Onwards and upwards!"
  },
  {
    "objectID": "08-network.html#lecture-slides",
    "href": "08-network.html#lecture-slides",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "08-network.html#reading-list",
    "href": "08-network.html#reading-list",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Geurs, K. and Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\nVerduzco Torres, J. R. and McArthur, D.P. 2024. Public transport accessibility indicators to urban and regional services in Great Britain. Scientific Data 11: 53. [Link]\n\n\n\n\n\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]"
  },
  {
    "objectID": "08-network.html#accessibility-in-lambeth",
    "href": "08-network.html#accessibility-in-lambeth",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "This week, we will analyse the accessibility of fast-food outlets in the London Borough of Lambeth. Specifically, we will examine how closely these outlets are located within walking distance of primary and secondary schools, and explore any potential relationships between their proximity and the relative levels of deprivation in the area.\nWe will extract the points of interest that we will use for this analysis from the Point of Interest (POI) data for the United Kingdom, obtained from the Overture Maps Foundation and pre-processed by the Consumer Data Research Centre to provide users with easy access.\nYou can download a subset of the POI dataset via the link provided below. A copy of the 2011 London LSOAs spatial boundaries, the boundaries of the London Boroughs, and the 2019 English Index of Multiple Deprivation. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLambeth Overture Points of Interest 2024\nGeoPackage\nDownload\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon Borough Spatial Boundaries\nGeoPackage\nDownload\n\n\nEngland 2019 Index of Multiple Deprivation\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nYou may have already downloaded some of these datasets in previous weeks, but for completeness, they are all provided here. Only download the datasets you do not already have or did not save.\n\n\n\n\n\n\n\n\n\nTo extract the Lambeth Overture Points of Interest data, a 2-kilometre buffer was applied around the boundaries of Lambeth. This approach ensures that points just outside the study area are included, as locations beyond the borough boundary may still be accessible to residents and could represent the nearest available options.\n\n\n\nOpen a new script within your GEOG0030 project and save this as w08-accessibility-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nNext, we can load the spatial data into R.\n\n\n\nR code\n\n# read poi data\npoi24 &lt;- st_read(\"data/spatial/Lambeth-POI-2024.gpkg\")\n\n\nReading layer `Lambeth-POI-2024' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/Lambeth-POI-2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 65060 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526556.6 ymin: 167827 xmax: 535640.4 ymax: 182673.8\nProjected CRS: OSGB36 / British National Grid\n\n# read lsoa dataset\nlsoa11 &lt;- st_read(\"data/spatial/London-LSOA-2011.gpkg\")\n\nReading layer `London-LSOA-2011' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4835 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# read borough dataset\nborough &lt;- st_read(\"data/spatial/London-Boroughs.gpkg\")\n\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n\nNow, carefully examine each individual dataframe to understand how the data is structured and what information it contains.\n\n\n\nR code\n\n# inspect poi data\nhead(poi24)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526913.4 ymin: 169695.2 xmax: 526945.5 ymax: 169970.8\nProjected CRS: OSGB36 / British National Grid\n                                id    primary_name       main_category\n1 08f194ada9716b86030eab41bbd4207e \"Gorgeous Grub\" \"burger_restaurant\"\n2 08f194ada9715a1903d73f4aef170602    \"TLC Direct\"   \"wholesale_store\"\n3 08f194ada944cba203fa613de4f5e6d5     \"JD Sports\"       \"sports_wear\"\n4 08f194ada9449a8a0345a466a0a6ece9       \"Lidl GB\"       \"supermarket\"\n                    alternate_category                                 address\n1   eat_and_drink|fast_food_restaurant                 \"1 Prince Georges Road\"\n2 professional_services|lighting_store                      \"280 Western Road\"\n3            sporting_goods|shoe_store \"Unit 2 Tandem Centre Top Of Church Rd\"\n4          retail|fast_food_restaurant                         \"Colliers Wood\"\n         locality   postcode region country source   source_record_id\n1        \"London\"   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\"  \"232538816864698\"\n2        \"London\" \"SW19 2QA\"  \"ENG\"    \"GB\" \"meta\" \"1959707454355017\"\n3 \"Colliers Wood\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"644899945690935\"\n4        \"London\" \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\"  \"111430837210163\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526921.1 16969...\n3 MULTIPOINT ((526915.7 16997...\n4 MULTIPOINT ((526922.2 16988...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect lsoa dataset\nhead(lsoa11)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531948.3 ymin: 180733.9 xmax: 545296.2 ymax: 184700.6\nProjected CRS: OSGB36 / British National Grid\n   lsoa11cd            lsoa11nm           lsoa11nmw  bng_e  bng_n     long\n1 E01000001 City of London 001A City of London 001A 532129 181625 -0.09706\n2 E01000002 City of London 001B City of London 001B 532480 181699 -0.09197\n3 E01000003 City of London 001C City of London 001C 532245 182036 -0.09523\n4 E01000005 City of London 001E City of London 001E 533581 181265 -0.07628\n       lat                               globalid          lsoa11_name pop2011\n1 51.51810 {283B0EAD-F8FC-40B6-9A79-1DDD7E5C0758}  City of London 001A    1465\n2 51.51868 {DDCE266B-7825-428C-9E0A-DF66B0179A55}  City of London 001B    1436\n3 51.52176 {C45E358E-A794-485A-BF76-D96E5D458EA4}  City of London 001C    1346\n4 51.51452 {4DDAF5E4-E47F-4312-89A0-923FFEC028A6}  City of London 001E     985\n                            geom\n1 MULTIPOLYGON (((532105.1 18...\n2 MULTIPOLYGON (((532634.5 18...\n3 MULTIPOLYGON (((532135.1 18...\n4 MULTIPOLYGON (((533808 1807...\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n# inspect borough dataset\nhead(borough)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 507007.4 ymin: 155850.8 xmax: 561957.5 ymax: 194889.3\nProjected CRS: OSGB36 / British National Grid\n  objectid                 name  gss_code  hectares nonld_area ons_inner\n1        1 Kingston upon Thames E09000021  3726.117      0.000         F\n2        2              Croydon E09000008  8649.441      0.000         F\n3        3              Bromley E09000006 15013.487      0.000         F\n4        4             Hounslow E09000018  5658.541     60.755         F\n5        5               Ealing E09000009  5554.428      0.000         F\n6        6             Havering E09000016 11445.735    210.763         F\n  sub_2011                           geom\n1    South POLYGON ((516401.6 160201.8...\n2    South POLYGON ((535009.2 159504.7...\n3    South POLYGON ((540373.6 157530.4...\n4     West POLYGON ((509703.4 175356.6...\n5     West POLYGON ((515647.2 178787.8...\n6     East POLYGON ((553564 179127.1, ...\n\n\n\n\nThe inspection shows that the POI dataset contains a wide variety of location types, with each point tagged under a main and alternative category, as provided by the Overture Maps Foundation via Meta and Microsoft. However, these tags may not be consistent across the dataset, so we will need to identify specific keywords to filter the main_category and alternate_category columns.\nWe will start by filtering out all POIs where the word school features in the main_category column:\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"school\"))\n\n# inspect\nhead(unique(poi_schools$main_category), n = 50)\n\n\n [1] \"\\\"day_care_preschool\\\"\"              \"\\\"driving_school\\\"\"                 \n [3] \"\\\"elementary_school\\\"\"               \"\\\"school\\\"\"                         \n [5] \"\\\"language_school\\\"\"                 \"\\\"music_school\\\"\"                   \n [7] \"\\\"specialty_school\\\"\"                \"\\\"preschool\\\"\"                      \n [9] \"\\\"dance_school\\\"\"                    \"\\\"high_school\\\"\"                    \n[11] \"\\\"drama_school\\\"\"                    \"\\\"cooking_school\\\"\"                 \n[13] \"\\\"middle_school\\\"\"                   \"\\\"vocational_and_technical_school\\\"\"\n[15] \"\\\"art_school\\\"\"                      \"\\\"private_school\\\"\"                 \n[17] \"\\\"religious_school\\\"\"                \"\\\"nursing_school\\\"\"                 \n[19] \"\\\"montessori_school\\\"\"               \"\\\"public_school\\\"\"                  \n[21] \"\\\"cosmetology_school\\\"\"              \"\\\"medical_school\\\"\"                 \n[23] \"\\\"engineering_schools\\\"\"             \"\\\"massage_school\\\"\"                 \n[25] \"\\\"business_schools\\\"\"                \"\\\"law_schools\\\"\"                    \n[27] \"\\\"medical_sciences_schools\\\"\"        \"\\\"sports_school\\\"\"                  \n[29] \"\\\"flight_school\\\"\"                  \n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nThis is still a very large list, and looking at the categories not all POIs containing the string school should be included. However, this initial selection has given us a more manageable list from which we can choose the relevant tags. We can now further filter the dataset as well as clip the dataset to the administrative boundaries of Lambeth.\n\n\n\nR code\n\n# remove quotes for easier processing\npoi_schools &lt;- poi_schools |&gt;\n    mutate(main_category = str_replace_all(main_category, \"\\\"\", \"\"))\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(main_category == \"elementary_school\" | main_category == \"high_school\" |\n        main_category == \"middle_school\" | main_category == \"private_school\" | main_category ==\n        \"public_school\" | main_category == \"school\")\n\n# filter school poi data\nlambeth &lt;- borough |&gt;\n    filter(name == \"Lambeth\")\n\npoi_schools &lt;- poi_schools |&gt;\n    st_intersection(lambeth) |&gt;\n    select(1:11)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 141 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 169846.4 xmax: 533065.9 ymax: 180398\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                 id\n6  08f194ad1a394235035f3ab7c2e4721d\n7  08f194ad1a8da734035945d69c357ddd\n8  08f194ad1abb648603defd9d76b4c314\n27 08f194ad130f0cd303c1c9f9b42438f8\n                                               primary_name     main_category\n6                         \"Woodmansterne Children's Centre\" elementary_school\n7   \"Immanuel & St Andrew Church of England Primary School\"            school\n8  \"Monkey Puzzle Day Nursery & Preschool Streatham Common\"            school\n27                                     \"Campsbourne School\"            school\n                        alternate_category                   address locality\n6                         school|education            \"Stockport Rd\"     &lt;NA&gt;\n7              elementary_school|education           \"Northanger Rd\"     &lt;NA&gt;\n8  education|public_service_and_government \"496 Streatham High Road\" \"London\"\n27                               education                      &lt;NA&gt; \"London\"\n     postcode region country source   source_record_id\n6  \"SW16 5XE\"   &lt;NA&gt;    \"GB\" \"meta\"  \"114577088601307\"\n7  \"SW16 5SL\"   &lt;NA&gt;    \"GB\" \"meta\"  \"128479257200832\"\n8  \"SW16 3QB\"  \"ENG\"    \"GB\" \"meta\" \"1092187950854118\"\n27       &lt;NA&gt;   &lt;NA&gt;    \"GB\" \"meta\"  \"114411542481619\"\n                        geom\n6  POINT (529701.5 169846.4)\n7  POINT (530016.4 170574.1)\n8  POINT (530208.6 170587.9)\n27 POINT (528819.8 174228.7)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nThis is still a rather long list and likely inaccurate. According to Lambeth Council Education Statistics, there should be 80 primary and secondary schools across the borough. We can use the alternate_category column to further narrow down our results.\n\n\n\n\n\n\nYou can inspect the different tags and their frequencies easily by creating a frequency table: table(poi_schools$alternate_category).\n\n\n\n\n\n\nR code\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(str_detect(alternate_category, \"elementary_school\") | str_detect(alternate_category,\n        \"high_school\") | str_detect(alternate_category, \"middle_school\") | str_detect(alternate_category,\n        \"private_school\") | str_detect(alternate_category, \"public_school\"))\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 58 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id\n1 08f194ad1a8da734035945d69c357ddd\n2 08f194ad1a70460d037da737c256001b\n3 08f194ad1c2dc81c032e9e0aa296a8d1\n4 08f194ad1e4cec5903fafb7496a2d2f3\n                                             primary_name     main_category\n1 \"Immanuel & St Andrew Church of England Primary School\"            school\n2                                \"Granton Primary School\" elementary_school\n3                 \"Kingswood Primary School (Upper Site)\" elementary_school\n4                              \"Battersea Grammar School\"            school\n           alternate_category          address locality   postcode region\n1 elementary_school|education  \"Northanger Rd\"     &lt;NA&gt; \"SW16 5SL\"   &lt;NA&gt;\n2        school|public_school     \"Granton Rd\"     &lt;NA&gt; \"SW16 5AN\"   &lt;NA&gt;\n3          school|high_school \"193 Gipsy Road\" \"London\"   \"SE27 9\"  \"ENG\"\n4       high_school|education             &lt;NA&gt; \"London\"       &lt;NA&gt;   &lt;NA&gt;\n  country source  source_record_id                      geom\n1    \"GB\" \"meta\" \"128479257200832\" POINT (530016.4 170574.1)\n2    \"GB\" \"meta\" \"235737420093504\" POINT (529299.7 170025.3)\n3    \"GB\" \"meta\" \"110066125723254\" POINT (532897.2 171498.4)\n4    \"GB\" \"meta\" \"103107239728950\" POINT (529523.9 172310.9)\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nSince the POI dataset is compiled from various open sources, the data quality is not guaranteed. Some schools may be missing, while others could be duplicated, perhaps under slightly different names or because different buildings have been assigned separate point locations. However, it is unlikely that more than one school would share the same postcode. Therefore, we will use postcode information (where available) to finalise our school selection and remove any likely duplicates.\n\n\n\nR code\n\n# identify duplicate postcodes\npoi_schools &lt;- poi_schools |&gt;\n    group_by(postcode) |&gt;\n    mutate(rank = rank(primary_name)) |&gt;\n    ungroup()\n\n# filter school poi data\npoi_schools &lt;- poi_schools |&gt;\n    filter(is.na(postcode) | rank == 1) |&gt;\n    select(-rank)\n\n# inspect\npoi_schools\n\n\nSimple feature collection with 54 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528635.7 ymin: 170025.3 xmax: 532897.2 ymax: 179678.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 54 × 12\n   id    primary_name main_category alternate_category address locality postcode\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1 08f1… \"\\\"Immanuel… school        elementary_school… \"\\\"Nor…  &lt;NA&gt;    \"\\\"SW16…\n 2 08f1… \"\\\"Granton … elementary_s… school|public_sch… \"\\\"Gra…  &lt;NA&gt;    \"\\\"SW16…\n 3 08f1… \"\\\"Kingswoo… elementary_s… school|high_school \"\\\"193… \"\\\"Lond… \"\\\"SE27…\n 4 08f1… \"\\\"Batterse… school        high_school|educa…  &lt;NA&gt;   \"\\\"Lond…  &lt;NA&gt;   \n 5 08f1… \"\\\"St Bede'… school        elementary_school… \"\\\"St …  &lt;NA&gt;    \"\\\"SW12…\n 6 08f1… \"\\\"St Leona… school        elementary_school… \"\\\"42 … \"\\\"Lond… \"\\\"SW16…\n 7 08f1… \"\\\"Richard … elementary_s… college_universit… \"\\\"New…  &lt;NA&gt;    \"\\\"SW2 …\n 8 08f1… \"\\\"Henry Ca… school        high_school|eleme… \"\\\"Hyd…  &lt;NA&gt;    \"\\\"SW12…\n 9 08f1… \"\\\"South Ba… school        high_school|b2b_s… \"\\\"56 … \"\\\"Lond… \"\\\"SW2 …\n10 08f1… \"\\\"Glenbroo… elementary_s… school|public_sch… \"\\\"Cla…  &lt;NA&gt;    \"\\\"SW4 …\n# ℹ 44 more rows\n# ℹ 5 more variables: region &lt;chr&gt;, country &lt;chr&gt;, source &lt;chr&gt;,\n#   source_record_id &lt;chr&gt;, geom &lt;POINT [m]&gt;\n\n\nAlthough we now have fewer schools than we had expected, either due to overly restrictive filtering of tags or because some school locations are not recorded in the dataset, we will proceed with the current data.\n\n\n\n\n\n\nVariable preparation can be a time-consuming process that often necessitates a more extensive exploratory analysis to ensure sufficient data quality. This may involve sourcing additional data to supplement your existing dataset.\n\n\n\nWe can use a similar approach to approximate the locations of fast food outlets in the Borough.\n\n\n\nR code\n\n# filter fast food poi data\npoi_fastfood &lt;- poi24 |&gt;\n    filter(str_detect(main_category, \"fast_food_restaurant\") | str_detect(alternate_category,\n        \"fast_food_restaurant\") | str_detect(alternate_category, \"chicken_restaurant\") |\n        str_detect(alternate_category, \"burger_restaurant\"))\n\n# inspect\npoi_fastfood\n\n\nSimple feature collection with 1444 features and 11 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 526666.3 ymin: 168272.9 xmax: 535546.9 ymax: 182554\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n                                id     primary_name          main_category\n1 08f194ada9716b86030eab41bbd4207e  \"Gorgeous Grub\"    \"burger_restaurant\"\n2 08f194ada9449a8a0345a466a0a6ece9        \"Lidl GB\"          \"supermarket\"\n3 08f194ada944daa80328c6604dab3503     \"Moss Bros.\" \"men's_clothing_store\"\n4 08f194ada932ad8603db11bbb7f953a7 \"Livi's Cuisine\"   \"african_restaurant\"\n                  alternate_category                          address  locality\n1 eat_and_drink|fast_food_restaurant          \"1 Prince Georges Road\"  \"London\"\n2        retail|fast_food_restaurant                  \"Colliers Wood\"  \"London\"\n3               fast_food_restaurant \"Unit 5, Tandem Shopping Centre\"  \"London\"\n4       caterer|fast_food_restaurant                   \"1 Locks Lane\" \"Mitcham\"\n    postcode region country source  source_record_id\n1   \"SW19 2\"  \"ENG\"    \"GB\" \"meta\" \"232538816864698\"\n2 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"111430837210163\"\n3 \"SW19 2TY\"   &lt;NA&gt;    \"GB\" \"meta\" \"478090646011341\"\n4    \"CR4 2\"  \"ENG\"    \"GB\" \"meta\" \"231745500530140\"\n                            geom\n1 MULTIPOINT ((526913.4 16984...\n2 MULTIPOINT ((526922.2 16988...\n3 MULTIPOINT ((526945.5 16992...\n4 MULTIPOINT ((527970.3 16955...\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nLet’s map both datasets to get an idea of how the data look like:\n\n\n\nR code\n\n# combine for mapping\npoi_schools &lt;- poi_schools |&gt;\n  mutate(type = \"School\")\npoi_fastfood &lt;- poi_fastfood |&gt;\n  mutate(type = \"Fast food\")\npoi_lambeth &lt;- rbind(poi_schools, poi_fastfood)\n\n# shape, polygon\ntm_shape(lambeth) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(poi_lambeth) +\n\n  # specify column, colours\n  tm_dots(\n    col = \"type\",\n    size = 0.05,\n    palette = c(\"#beaed4\", \"#fdc086\"),\n    title = \"\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Extracted school and fast food locations for Lambeth.\n\n\n\n\n\n\n\nIn addition to the locations of interest, we need network data to assess the accessibility of schools in relation to fast food outlets. We will use OpenStreetMap to extract road segment data. Similar to the POI dataset, OSM uses key and value tags to categorise the features within its dataset.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world, but its coverage is uneven globally. However, the accuracy and quality of the data can at times be questionable, with details such as road types and speed limits missing. The OpenStreetMap Wiki provides more details on the tagging system.\n\n\n\nTo download the Lambeth road network dataset, we first need to define our bounding box coordinates. We will then use these coordinates in our OSM query to extract specific types of road segments within the defined search area. Our focus will be on selecting all OSM features with the highway tag that are likely to be used by pedestrians (e.g. excluding motorways).\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_lambeth &lt;- poi24 |&gt;\n    st_transform(4326) |&gt;\n    st_bbox()\n\n# osm query\nosm_network &lt;- opq(bbox = bbox_lambeth) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Roads.RData')\n\n\n\nThe returned osm_network object contains a variety of elements with the specified tags. Our next step is to extract the spatial data from this object to create our road network dataset. Specifically, we will extract the edges of the network, which represent the lines of the roads, as well as the nodes, which represent the points where the roads start, end, or intersect.\n\n\n\nR code\n\n# extract the nodes, with their osm_id\nosm_network_nodes &lt;- osm_network$osm_points[, \"osm_id\"]\n\n# extract the edges, with their osm_id and relevant columns\nosm_network_edges &lt;- osm_network$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n# inspect\nhead(osm_network_nodes)\n\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1541499 ymin: 51.52434 xmax: -0.1457924 ymax: 51.52698\nGeodetic CRS:  WGS 84\n      osm_id                    geometry\n78112  78112 POINT (-0.1457924 51.52698)\n99878  99878 POINT (-0.1529787 51.52434)\n99879  99879 POINT (-0.1532934 51.52482)\n99880  99880 POINT (-0.1535802 51.52508)\n99882  99882 POINT (-0.1541499 51.52567)\n99883  99883 POINT (-0.1541362 51.52598)\n\n# inspect\nhead(osm_network_edges)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -0.1398347 ymin: 51.50608 xmax: -0.0821093 ymax: 51.5246\nGeodetic CRS:  WGS 84\n         osm_id                 name     highway maxspeed oneway\n31030     31030          Grafton Way     primary   20 mph    yes\n31039     31039 Tottenham Court Road     primary   20 mph   &lt;NA&gt;\n31959     31959     Cleveland Street residential   20 mph    yes\n554369   554369  King William Street    tertiary   20 mph    yes\n554526   554526     Fenchurch Street    tertiary   20 mph   &lt;NA&gt;\n1530592 1530592  Borough High Street     primary   30 mph    yes\n                              geometry\n31030   LINESTRING (-0.1349153 51.5...\n31039   LINESTRING (-0.1303693 51.5...\n31959   LINESTRING (-0.139512 51.52...\n554369  LINESTRING (-0.08745 51.511...\n554526  LINESTRING (-0.085135 51.51...\n1530592 LINESTRING (-0.0882957 51.5...\n\n\nWe can quickly map the network edges to see how the road network looks like:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(osm_network_edges) +\n\n  # specify column, classes\n  tm_lines(\n    col = \"#bdbdbd\",\n    lwd = 0.2,\n  ) +\n\n  # shape, polygon\n  tm_shape(lambeth) +\n\n  # specify column, classes\n  tm_borders(\n    col = \"#252525\",\n    lwd = 2\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Road segments\",\n    col = \"#bdbdbd\"\n  ) +\n\n  tm_add_legend(\n    type = \"line\",\n    labels = \"Outline Lambeth\",\n    col = \"#252525\"\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.outside = TRUE,\n    legend.position = c(\"right\", \"bottom\"),\n    legend.text.size = 1\n  )\n\n\n\n\n\nFigure 2: Extracted OpenStreetMap road network data for Lambeth.\n\n\n\n\n\n\n\nSince our focus is on schoolchildren and walking distances, we will overwrite the oneway variable to assume that none of the road segments are restricted to one-way traffic. This adjustment will ensure our analysis is not skewed by such restrictions and will help maintain a more accurate representation of the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nosm_network_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics with our prepared point of interest data.\nIn any network analysis, the primary data structure is a graph composed of nodes and edges. The dodgr library utilises weighting profiles to assign weights based on road types, tailored to the mode of transport that each profile is designed to model. In this instance, we will use the foot weighting profile, as our focus is on modelling walking accessibility. To prevent errors related to the weighting profile, we will replace any NA values in the highway tag with the value unclassified.\n\n\n\nR code\n\n# replace missing highway tags with unclassified\nosm_network_edges &lt;- osm_network_edges |&gt;\n    mutate(highway = if_else(is.na(highway), \"unclassified\", highway))\n\n# create network graph\nosm_network_graph &lt;- weight_streetnet(osm_network_edges, wt_profile = \"foot\")\n\n\nOnce we have constructed our graph, we can use it to calculate network distances between our points of interest. One important consideration is that not all individual components in the extracted network may be connected. This can occur, for example, if the bounding box cuts off access to the road of a cul-de-sac. To ensure that our entire extracted network is connected, we will therefore extract the largest connected component of the graph.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always representing the largest component. It is essential to inspect the resulting subgraph to ensure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\nnetx_connected &lt;- osm_network_graph[osm_network_graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(netx_connected)\n\n\n[1] 440474\n\n\n\n\n\n\n\n\nOpenStreetMap is a dynamic dataset, meaning that changes are made on a continuous basis. As a result, it is quite possible that the number of remaining road segments, as shown above, may differ slightly when you run this analysis.\n\n\n\n\n\n\nNow that we have our connected subgraph, we can use the dodgr_distances() function to calculate the network distances between every possible origin (i.e. school) and destination (i.e. fast food outlet). For all combinations, the function will map the point of interest locations to the nearest point on the network and return the corresponding shortest-path distances.\n\n\n\n\n\n\nThe dodgr package requires data to be projected in WGS84, so we need to reproject our point of interest data accordingly.\n\n\n\n\n\n\nR code\n\n# reproject\npoi_schools &lt;- poi_schools |&gt;\n    st_transform(4326)\npoi_fastfood &lt;- poi_fastfood |&gt;\n    st_transform(4326)\n\n# distance matrix\ndistance_matrix &lt;- dodgr_distances(netx_connected, from = st_coordinates(poi_schools),\n    to = st_coordinates(poi_fastfood), shortest = FALSE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance matrix that contains the network distances between all origins (i.e. schools) and all destinations (i.e. fast-food outlets):\n\n\n\nR code\n\n# inspect\ndistance_matrix[1:5, 1:5]\n\n\n            6807494201 7110321980 7110321980 11371586827 33148215\n8796433764    4660.831   4661.009   4661.009    3128.948 3087.031\n8820889464    3611.758   3753.383   3753.383    1957.011 1915.094\n11479633279   8497.581   8497.760   8497.760    6940.464 6898.547\n292521291     4917.554   4917.732   4917.732    4222.538 4287.953\n12331531180   6270.840   6271.018   6271.018    5575.824 5641.240\n\n\n\n\n\n\n\n\nThe above output displays the distance (in metres) between the first five schools and the first five fast-food outlets. The row and column IDs refer to the nearest nodes on the OSM network to which the schools and fast-food outlets were mapped.\n\n\n\nNow that we have the distance matrix, we can aggregate the data and perform accessibility analysis. For example, we can count the number of fast-food outlets within 500 or 1,000 metres walking distance from each school:\n\n\n\nR code\n\n# fast-food outlets within 500m\npoi_schools$fastfood_500m &lt;- rowSums(distance_matrix &lt;= 500)\n\n# fast-food outlets within 1000m\npoi_schools$fastfood_1000m &lt;- rowSums(distance_matrix &lt;= 1000)\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nIn the final step, we can investigate whether there is a relationship between the proximity of fast-food outlets and the relative levels of deprivation in the area. One approach is to calculate the average number of fast-food outlets within 1,000 metres of a school for each LSOA, and then compare these figures to their corresponding IMD deciles.\n\n\n\nR code\n\n# read imd dataset\nimd19 &lt;- read_csv(\"data/attributes/England-IMD-2019.csv\")\n\n\nRows: 32844 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lsoa11cd\ndbl (2): imd_rank, imd_dec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# join imd\nlsoa11 &lt;- lsoa11 |&gt;\n  left_join(imd19, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n# join schools to their parent lsoa\npoi_schools &lt;- poi_schools |&gt;\n  st_transform(27700) |&gt;\n  st_join(lsoa11)\n\nWe can use this approach to derive the average number of fast-food by IMD decile:\n\n\n\nR code\n\n# average counts by imd decile\nfastfood_imd &lt;- poi_schools |&gt;\n    group_by(imd_dec) |&gt;\n    mutate(avg_cnt = mean(fastfood_1000m)) |&gt;\n    distinct(imd_dec, avg_cnt) |&gt;\n    arrange(imd_dec)\n\n# inspect\nfastfood_imd\n\n\n# A tibble: 7 × 2\n# Groups:   imd_dec [7]\n  imd_dec avg_cnt\n    &lt;dbl&gt;   &lt;dbl&gt;\n1       2   20.1 \n2       3   14.3 \n3       4   17.5 \n4       5    9.83\n5       6    3   \n6       7    8.4 \n7       8   23.5 \n\n\nThere appears to be a weak relationship, with schools in more deprived areas having, on average, a higher number of fast-food outlets within a 1,000-metre walking distance. However, this trend is not consistent, as schools in the least deprived areas of Lambeth show the highest accessibility on average."
  },
  {
    "objectID": "08-network.html#assignment",
    "href": "08-network.html#assignment",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "Accessibility analysis involves evaluating how easily people can reach essential services, destinations, or opportunities, such as schools, healthcare facilities, or workplaces, from a given location. The CDRC Access to Healthy Assets & Hazards (AHAH) dataset, for instance, uses accessibility analysis to quantify how easy it is to reach ‘unhealthy’ places, such as pubs and gambling outlets, for each neighbourhood in Great Britain.\nHaving run through all the steps during the tutorial, we can recreate this analysis ourselves. Using Lambeth as a case study, try to complete the following tasks:\n\nExtract all pubs from the Point of Interest dataset.\nFor each LSOA within Lambeth, calculate the average walking distance to the nearest pub.\nCreate a map of the results.\n\n\n\n\n\n\n\nUnlike before, LSOAs are now the unit of analysis. This means you will need to input the LSOA centroids into your distance matrix.\n\n\n\n\n\n\n\n\n\nIf you want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R."
  },
  {
    "objectID": "08-network.html#before-you-leave",
    "href": "08-network.html#before-you-leave",
    "title": "1 Accessibility Analysis",
    "section": "",
    "text": "This brings us to the end of the tutorial. You should now have a basic understanding of the concepts behind accessibility analysis, how it can be executed in R, and some of the challenges you may encounter when conducting your own research. With this being said, you have now reached the end of this week’s content. Onwards and upwards!"
  },
  {
    "objectID": "03-point-pattern.html",
    "href": "03-point-pattern.html",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "This week, we will be focusing on point pattern analysis (PPA), which aims to detect clusters or patterns within a set of points. Through this analysis, we can measure density, dispersion, and homogeneity in point structures. Various methods exist for calculating and identifying these clusters, and today we will explore several of these techniques using our bike theft dataset from last week.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nArribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]\n\n\n\n\n\nThis week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the bicycle theft dataset that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Bicycle Theft 2023\nGeoPackage\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w03-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAs always, we will start by loading loading our files into memory:\n\n\n\nR code\n\n# load msoa dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load bicycle theft dataset\ntheft_bike &lt;- st_read(\"data/spatial/London-BicycleTheft-2023.gpkg\")\n\nReading layer `London-BicycleTheft-2023' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-BicycleTheft-2023.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16019 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(theft_bike)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n                                                          crime_id   month\n1 62b0f525fc471c062463ec87469c71e133cdd1c39a09e2bc5ec50c0cbbdd650a 2023-01\n2 9a078d630cf67c37c9e47b5904149d553697931d920eec993f358a637fbf6186 2023-01\n3 f175a32ef7f90c67a5cb83c268ad793e4ce7e0ae19e52b57d213805e65651bdd 2023-01\n4 137ec1201fd64b57898d3558fe3b8000182442e957efc28246797ea61065c86b 2023-01\n5 4c3b467755a98afa3d3c82b526307750ddad9ec13598aaa68130b76863e112cb 2023-01\n6 13b5eb5ca0aef09a222221fbab8da799d55b5a65716f1bbb7cc96e36aebdc816 2023-01\n            reported_by          falls_within                   location\n1 City of London Police City of London Police                 On or near\n2 City of London Police City of London Police                 On or near\n3 City of London Police City of London Police  On or near Lombard Street\n4 City of London Police City of London Police    On or near Mitre Street\n5 City of London Police City of London Police   On or near Temple Avenue\n6 City of London Police City of London Police On or near Montague Street\n  lsoa_code           lsoa_name    crime_type\n1 E01000002 City of London 001B Bicycle theft\n2 E01032739 City of London 001F Bicycle theft\n3 E01032739 City of London 001F Bicycle theft\n4 E01032739 City of London 001F Bicycle theft\n5 E01032740 City of London 001G Bicycle theft\n6 E01032740 City of London 001G Bicycle theft\n                          last_outcome_category context\n1 Investigation complete; no suspect identified      NA\n2 Investigation complete; no suspect identified      NA\n3 Investigation complete; no suspect identified      NA\n4 Investigation complete; no suspect identified      NA\n5 Investigation complete; no suspect identified      NA\n6 Investigation complete; no suspect identified      NA\n                       geom\n1 POINT (532390.8 181781.7)\n2 POINT (532157.8 181196.8)\n3 POINT (532720.7 181087.8)\n4 POINT (533333.7 181219.8)\n5 POINT (531274.9 180967.9)\n6 POINT (531956.8 181624.8)\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nOne key advantage of point data is that it is scale-free, allowing aggregation to any geographic level for analysis. Before diving into PPA, we will aggregate the bicycle thefts to the MSOA level to map their distribution by using a point-in-polygon approach.\n\n\n\nR code\n\n# point in polygon\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike, sparse = TRUE)))\n\n\n\n\n\n\n\n\nTo create a point-in-polygon count within sf, we use the st_intersects() function and keep its default sparse = TRUE output, which produces a list of intersecting points by index for each polygon (e.g. MSOA). We then apply the lengths() function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.\n\n\n\nWe can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.\n\n\n\nR code\n\n# msoa area size\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(km_sq = as.numeric(st_area(msoa21))/1e+06)\n\n# theft density\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(thef_km_sq = theft_bike_n/km_sq)\n\n\nLet’s put this onto a map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"thef_km_sq\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Thefts / Square kilometre\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n  )\n\n\n\n\n\nFigure 1: Number of reported bicycle thefts by square kilometre.\n\n\n\n\n\n\n\nFigure 1 shows that the number of bicycle thefts is clearly concentrated in parts of Central London. While this map may provide helpful insights, its representation depends on the classification and aggregation of the underlying data. Alternatively, we can directly analyse the point events themselves. For this, we will use the spatstat library, the primary library for point pattern analysis in R. To use spatstat, we need to convert our data into a ppp object.\n\n\n\n\n\n\nThe ppp format is specific to spatstat but is also used in some other spatial analysis libraries. A ppp object represents a two-dimensional point dataset within a defined area, called the window of observation (owin in spatstat). We can either create a ppp object directly from a list of coordinates (with a specified window of observation) or convert it from another data type.\n\n\n\nWe can turn our theft_bike dataframe into a ppp object as follows:\n\n\n\nR code\n\n# london outline\noutline &lt;- msoa21 |&gt;\n    st_union()\n\n# clip\ntheft_bike &lt;- theft_bike |&gt;\n    st_intersection(outline)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# sf to ppp\nwindow = as.owin(msoa21)\ntheft_bike_ppp &lt;- ppp(st_coordinates(theft_bike)[, 1], st_coordinates(theft_bike)[,\n    2], window = window)\n\nWarning: data contain duplicated points\n\n# inspect\npar(mar = c(1, 1, 1, 1))\nplot(theft_bike_ppp, main = \"\")\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nSome statistical procedures require point events to be unique. In our bicycle theft data, duplicates are likely due to the police snapping points to protect anonymity and privacy. This can pose an issue for spatial point pattern analysis, where each theft and its location must be distinct. We can check whether we have any duplicated points as follows:\n\n\n\nR code\n\n# check for duplicates\nanyDuplicated(theft_bike_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_ppp) &gt; 1)\n\n[1] 10003\n\n\nTo address this, we have three options:\n\nRemove duplicates if the number of duplicated points is small or the exact location is less important than the overall distribution.\nAssign weights to points, where each has an attribute indicating the number of events at that location rather than being recorded as separate event.\nAdd jitter by slightly offsetting the points randomly, which can be useful if precise location is not crucial for the analysis.\n\nEach approach has its own trade-offs, depending on the analysis. In our case, we will use the jitter approach to retain all bike theft events. Since the locations are already approximated, adding a small offset (~5 metre) will not impact the analysis.\n\n\n\nR code\n\n# add ajitter\ntheft_bike_jitter &lt;- rjitter(theft_bike_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for duplicates\nanyDuplicated(theft_bike_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_jitter) &gt; 1)\n\n[1] 0\n\n\nThis seemed to have worked, so we can move forward.\n\n\nInstead of visualising the distribution of bike thefts at a specific geographical level, we can use Kernel Density Estimation (KDE) to display the distribution of these incidents. KDE is a statistical method that creates a smooth, continuous distribution to represent the density of the underlying pattern between data points.\n\n\n\n\n\n\nKernel Density Estimation (KDE) generates a raster surface that shows the estimated density of event points across space. Each cell represents the local density, highlighting areas of high or low concentration. KDE uses overlapping moving windows (defined by a kernel) and a bandwidth parameter, which controls the size of the window, influencing the smoothness of the resulting density surface. The kernel function can assign equal or weighted values to points, producing a grid of density values based on these local calculations.\n\n\n\nLet’s go ahead and create a simple KDE of bike theft with our bandwidth set to 500 metres:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 500), main = \"\")\n\n\n\n\n\nFigure 3: Kernel density estimation - bandwidth 500m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around Central London. We can go ahead and increase the bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 1000), main = \"\")\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 1000m.\n\n\n\n\nBy increasing the bandwidth, our clusters appear larger and brighter than with the 500-metre bandwidth. A larger bandwidth considers more points, resulting in a smoother surface. However, this can lead to oversmoothing, where clusters become less defined, potentially overestimating areas of high bike theft. Smaller bandwidths offer more precision and sharper clusters but risk undersmoothing, which can cause irregularities.\n\n\n\n\n\n\nWhile automated methods (e.g. maximum-likelihood estimation) can assist in selecting an optimal bandwidth, the choice is subjective and depends on the specific characteristics of your dataset.\n\n\n\n\n\n\n\n\n\nAlthough bandwidth has a greater impact on density estimation than the kernel type, the choice of kernel can still influence the results by altering how points are weighted within the window. We will explore kernel types a little further when we discuss spatial models in a few weeks time.\n\n\n\nOnce we are satisfied with our KDE visualisation, we can create a proper map by converting the KDE output into raster format.\n\n\n\nR code\n\n# to raster\ntheft_bike_raster &lt;- density.ppp(theft_bike_jitter, sigma = 1000) |&gt;\n    rast()\n\n\nWe now have a standalone raster that we can use with any function in the tmap library. However, one issue is that the resulting raster lacks a Coordinate Reference System (CRS), so we need to manually assign this information to the raster object:\n\n\n\nR code\n\n# set CRS\ncrs(theft_bike_raster) &lt;- \"epsg:27700\"\n\n\nNow we can map the KDE values.\n\n\n\nR code\n\n# shape, polygon\ntm_shape(theft_bike_raster) +\n\n  # specify column, colours\n  tm_raster(\n    col = \"lyr.1\",\n    palette = \"Blues\",\n    title = \"Density\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Kernel Density Estimate of bicycle thefts in London.\n\n\n\n\n\n\n\n\n\n\nThe values of the KDE output are stored in the raster grid as lyr.1.\n\n\n\n\n\n\nKernel Density Estimation is a useful exploratory technique for identifying spatial clusters in point data, but it does not provide precise boundaries for these clusters. To more accurately delineate clusters, we can use an algorithm called DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which takes both distance and density into account. DBSCAN is effective at discovering distinct clusters by grouping together points that are close to one another while marking points that don’t belong to any cluster as noise.\nDBSCAN requires two parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nepsilon\nThe maximum distance for points to be considered in the same cluster.\n\n\nminPts\nThe minimum number of points for a cluster.\n\n\n\nThe algorithm groups nearby points based on these parameters and marks low-density points as outliers. DBSCAN is useful for uncovering patterns that are difficult to detect visually, but it works best when clusters have consistent densities.\nLet us try this with an epsilon of 200 metres and minPts of 20 bicycle thefts:\n\n\n\nR code\n\n# dbscan\nbike_theft_dbscan &lt;- theft_bike |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function accepts a data matrix or dataframe of points, not a spatial dataframe. That is why, in the code above, we use the st_coordinates() function to extract the projected coordinates from the spatial dataframe.\n\n\n\nThe DBSCAN output includes three objects, one of which is a vector detailing the cluster each bike theft observation has been assigned to. To work with this output effectively, we need to add the cluster labels back to the original point dataset. Since DBSCAN does not alter the order of points, we can simply add the cluster output to the theft_bike spatial dataframe.\n\n\n\nR code\n\n# add cluster numbers\ntheft_bike &lt;- theft_bike |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow that each bike theft point in London is associated with a specific cluster, where appropriate, we can generate a polygon representing these clusters. To do this, we will use the st_convex_hull() function from the sf package, which creates a polygon that covers the minimum bounding area of a collection of points. We will apply this function to each cluster using a for loop, which allows us to repeat the process for each group of points and create a polygon representing the geometry of each cluster.\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(theft_bike$dbcluster))\n\n# create a counter to keep track\ncounter &lt;- 0\n\n# begin loop\nfor (cluster_index in seq(0, max(theft_bike$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    theft_bike_subset &lt;- theft_bike |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- theft_bike_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[counter] &lt;- (cluster_polygon)\n\n    # update the counter\n    counter &lt;- counter + 1\n}\n\n# combine the list\ntheft_bike_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\n\n\n\n\n\n\nWhile loops in R should generally be avoided for large datasets due to inefficiency, they remain a useful tool for automating repetitive tasks and reducing the risk of errors. For smaller datasets or tasks that cannot easily be vectorised, loops can still be effective and simplify the code.\n\n\n\nWe now have a spatial dataframe that contains the bike theft clusters in London, as defined by the DBSCAN clustering algorithm. Let’s quickly map these clusters:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify colours\n  tm_dots(\n    col = \"#636363\",\n    size = 0.01,\n  ) +\n\n  # shape, polygon\n  tm_shape(theft_bike_clusters) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#fdc086\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 6: DBSCAN-identified clusters of reported bicycle theft in London.\n\n\n\n\n\n\n\n\n\nNow that we know how to work with point locatoin data, we can again apply a similar analysis to road crashes in London in 2022 that we used last week. This time we will use this dataset to assess whether road crashes cluster in specific areas. Try the following:\n\nCreate a Kernel Density Estimation (KDE) of all road crashes that occurred in London in 2022.\nUsing DBSCAN output, create a cluster map of serious and fatal road crashes in London\n\nIf you no longer have a copy of the 2022 London STATS19 Road Collision dataset, you can download it using the link provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\n\n\n\n\nWith access to point event data, geographers aim to identify underlying patterns. This week, we explored several techniques that help us analyse and interpret such data. That is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "03-point-pattern.html#lecture-slides",
    "href": "03-point-pattern.html#lecture-slides",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "03-point-pattern.html#reading-list",
    "href": "03-point-pattern.html#reading-list",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]"
  },
  {
    "objectID": "03-point-pattern.html#bike-theft-in-london-ii",
    "href": "03-point-pattern.html#bike-theft-in-london-ii",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "This week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the bicycle theft dataset that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Bicycle Theft 2023\nGeoPackage\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w03-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAs always, we will start by loading loading our files into memory:\n\n\n\nR code\n\n# load msoa dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load bicycle theft dataset\ntheft_bike &lt;- st_read(\"data/spatial/London-BicycleTheft-2023.gpkg\")\n\nReading layer `London-BicycleTheft-2023' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-BicycleTheft-2023.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16019 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(theft_bike)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n                                                          crime_id   month\n1 62b0f525fc471c062463ec87469c71e133cdd1c39a09e2bc5ec50c0cbbdd650a 2023-01\n2 9a078d630cf67c37c9e47b5904149d553697931d920eec993f358a637fbf6186 2023-01\n3 f175a32ef7f90c67a5cb83c268ad793e4ce7e0ae19e52b57d213805e65651bdd 2023-01\n4 137ec1201fd64b57898d3558fe3b8000182442e957efc28246797ea61065c86b 2023-01\n5 4c3b467755a98afa3d3c82b526307750ddad9ec13598aaa68130b76863e112cb 2023-01\n6 13b5eb5ca0aef09a222221fbab8da799d55b5a65716f1bbb7cc96e36aebdc816 2023-01\n            reported_by          falls_within                   location\n1 City of London Police City of London Police                 On or near\n2 City of London Police City of London Police                 On or near\n3 City of London Police City of London Police  On or near Lombard Street\n4 City of London Police City of London Police    On or near Mitre Street\n5 City of London Police City of London Police   On or near Temple Avenue\n6 City of London Police City of London Police On or near Montague Street\n  lsoa_code           lsoa_name    crime_type\n1 E01000002 City of London 001B Bicycle theft\n2 E01032739 City of London 001F Bicycle theft\n3 E01032739 City of London 001F Bicycle theft\n4 E01032739 City of London 001F Bicycle theft\n5 E01032740 City of London 001G Bicycle theft\n6 E01032740 City of London 001G Bicycle theft\n                          last_outcome_category context\n1 Investigation complete; no suspect identified      NA\n2 Investigation complete; no suspect identified      NA\n3 Investigation complete; no suspect identified      NA\n4 Investigation complete; no suspect identified      NA\n5 Investigation complete; no suspect identified      NA\n6 Investigation complete; no suspect identified      NA\n                       geom\n1 POINT (532390.8 181781.7)\n2 POINT (532157.8 181196.8)\n3 POINT (532720.7 181087.8)\n4 POINT (533333.7 181219.8)\n5 POINT (531274.9 180967.9)\n6 POINT (531956.8 181624.8)\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nOne key advantage of point data is that it is scale-free, allowing aggregation to any geographic level for analysis. Before diving into PPA, we will aggregate the bicycle thefts to the MSOA level to map their distribution by using a point-in-polygon approach.\n\n\n\nR code\n\n# point in polygon\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike, sparse = TRUE)))\n\n\n\n\n\n\n\n\nTo create a point-in-polygon count within sf, we use the st_intersects() function and keep its default sparse = TRUE output, which produces a list of intersecting points by index for each polygon (e.g. MSOA). We then apply the lengths() function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.\n\n\n\nWe can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.\n\n\n\nR code\n\n# msoa area size\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(km_sq = as.numeric(st_area(msoa21))/1e+06)\n\n# theft density\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(thef_km_sq = theft_bike_n/km_sq)\n\n\nLet’s put this onto a map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"thef_km_sq\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Thefts / Square kilometre\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n  )\n\n\n\n\n\nFigure 1: Number of reported bicycle thefts by square kilometre.\n\n\n\n\n\n\n\nFigure 1 shows that the number of bicycle thefts is clearly concentrated in parts of Central London. While this map may provide helpful insights, its representation depends on the classification and aggregation of the underlying data. Alternatively, we can directly analyse the point events themselves. For this, we will use the spatstat library, the primary library for point pattern analysis in R. To use spatstat, we need to convert our data into a ppp object.\n\n\n\n\n\n\nThe ppp format is specific to spatstat but is also used in some other spatial analysis libraries. A ppp object represents a two-dimensional point dataset within a defined area, called the window of observation (owin in spatstat). We can either create a ppp object directly from a list of coordinates (with a specified window of observation) or convert it from another data type.\n\n\n\nWe can turn our theft_bike dataframe into a ppp object as follows:\n\n\n\nR code\n\n# london outline\noutline &lt;- msoa21 |&gt;\n    st_union()\n\n# clip\ntheft_bike &lt;- theft_bike |&gt;\n    st_intersection(outline)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# sf to ppp\nwindow = as.owin(msoa21)\ntheft_bike_ppp &lt;- ppp(st_coordinates(theft_bike)[, 1], st_coordinates(theft_bike)[,\n    2], window = window)\n\nWarning: data contain duplicated points\n\n# inspect\npar(mar = c(1, 1, 1, 1))\nplot(theft_bike_ppp, main = \"\")\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nSome statistical procedures require point events to be unique. In our bicycle theft data, duplicates are likely due to the police snapping points to protect anonymity and privacy. This can pose an issue for spatial point pattern analysis, where each theft and its location must be distinct. We can check whether we have any duplicated points as follows:\n\n\n\nR code\n\n# check for duplicates\nanyDuplicated(theft_bike_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_ppp) &gt; 1)\n\n[1] 10003\n\n\nTo address this, we have three options:\n\nRemove duplicates if the number of duplicated points is small or the exact location is less important than the overall distribution.\nAssign weights to points, where each has an attribute indicating the number of events at that location rather than being recorded as separate event.\nAdd jitter by slightly offsetting the points randomly, which can be useful if precise location is not crucial for the analysis.\n\nEach approach has its own trade-offs, depending on the analysis. In our case, we will use the jitter approach to retain all bike theft events. Since the locations are already approximated, adding a small offset (~5 metre) will not impact the analysis.\n\n\n\nR code\n\n# add ajitter\ntheft_bike_jitter &lt;- rjitter(theft_bike_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for duplicates\nanyDuplicated(theft_bike_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_jitter) &gt; 1)\n\n[1] 0\n\n\nThis seemed to have worked, so we can move forward.\n\n\nInstead of visualising the distribution of bike thefts at a specific geographical level, we can use Kernel Density Estimation (KDE) to display the distribution of these incidents. KDE is a statistical method that creates a smooth, continuous distribution to represent the density of the underlying pattern between data points.\n\n\n\n\n\n\nKernel Density Estimation (KDE) generates a raster surface that shows the estimated density of event points across space. Each cell represents the local density, highlighting areas of high or low concentration. KDE uses overlapping moving windows (defined by a kernel) and a bandwidth parameter, which controls the size of the window, influencing the smoothness of the resulting density surface. The kernel function can assign equal or weighted values to points, producing a grid of density values based on these local calculations.\n\n\n\nLet’s go ahead and create a simple KDE of bike theft with our bandwidth set to 500 metres:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 500), main = \"\")\n\n\n\n\n\nFigure 3: Kernel density estimation - bandwidth 500m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around Central London. We can go ahead and increase the bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 1000), main = \"\")\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 1000m.\n\n\n\n\nBy increasing the bandwidth, our clusters appear larger and brighter than with the 500-metre bandwidth. A larger bandwidth considers more points, resulting in a smoother surface. However, this can lead to oversmoothing, where clusters become less defined, potentially overestimating areas of high bike theft. Smaller bandwidths offer more precision and sharper clusters but risk undersmoothing, which can cause irregularities.\n\n\n\n\n\n\nWhile automated methods (e.g. maximum-likelihood estimation) can assist in selecting an optimal bandwidth, the choice is subjective and depends on the specific characteristics of your dataset.\n\n\n\n\n\n\n\n\n\nAlthough bandwidth has a greater impact on density estimation than the kernel type, the choice of kernel can still influence the results by altering how points are weighted within the window. We will explore kernel types a little further when we discuss spatial models in a few weeks time.\n\n\n\nOnce we are satisfied with our KDE visualisation, we can create a proper map by converting the KDE output into raster format.\n\n\n\nR code\n\n# to raster\ntheft_bike_raster &lt;- density.ppp(theft_bike_jitter, sigma = 1000) |&gt;\n    rast()\n\n\nWe now have a standalone raster that we can use with any function in the tmap library. However, one issue is that the resulting raster lacks a Coordinate Reference System (CRS), so we need to manually assign this information to the raster object:\n\n\n\nR code\n\n# set CRS\ncrs(theft_bike_raster) &lt;- \"epsg:27700\"\n\n\nNow we can map the KDE values.\n\n\n\nR code\n\n# shape, polygon\ntm_shape(theft_bike_raster) +\n\n  # specify column, colours\n  tm_raster(\n    col = \"lyr.1\",\n    palette = \"Blues\",\n    title = \"Density\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Kernel Density Estimate of bicycle thefts in London.\n\n\n\n\n\n\n\n\n\n\nThe values of the KDE output are stored in the raster grid as lyr.1.\n\n\n\n\n\n\nKernel Density Estimation is a useful exploratory technique for identifying spatial clusters in point data, but it does not provide precise boundaries for these clusters. To more accurately delineate clusters, we can use an algorithm called DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which takes both distance and density into account. DBSCAN is effective at discovering distinct clusters by grouping together points that are close to one another while marking points that don’t belong to any cluster as noise.\nDBSCAN requires two parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nepsilon\nThe maximum distance for points to be considered in the same cluster.\n\n\nminPts\nThe minimum number of points for a cluster.\n\n\n\nThe algorithm groups nearby points based on these parameters and marks low-density points as outliers. DBSCAN is useful for uncovering patterns that are difficult to detect visually, but it works best when clusters have consistent densities.\nLet us try this with an epsilon of 200 metres and minPts of 20 bicycle thefts:\n\n\n\nR code\n\n# dbscan\nbike_theft_dbscan &lt;- theft_bike |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function accepts a data matrix or dataframe of points, not a spatial dataframe. That is why, in the code above, we use the st_coordinates() function to extract the projected coordinates from the spatial dataframe.\n\n\n\nThe DBSCAN output includes three objects, one of which is a vector detailing the cluster each bike theft observation has been assigned to. To work with this output effectively, we need to add the cluster labels back to the original point dataset. Since DBSCAN does not alter the order of points, we can simply add the cluster output to the theft_bike spatial dataframe.\n\n\n\nR code\n\n# add cluster numbers\ntheft_bike &lt;- theft_bike |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow that each bike theft point in London is associated with a specific cluster, where appropriate, we can generate a polygon representing these clusters. To do this, we will use the st_convex_hull() function from the sf package, which creates a polygon that covers the minimum bounding area of a collection of points. We will apply this function to each cluster using a for loop, which allows us to repeat the process for each group of points and create a polygon representing the geometry of each cluster.\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(theft_bike$dbcluster))\n\n# create a counter to keep track\ncounter &lt;- 0\n\n# begin loop\nfor (cluster_index in seq(0, max(theft_bike$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    theft_bike_subset &lt;- theft_bike |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- theft_bike_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[counter] &lt;- (cluster_polygon)\n\n    # update the counter\n    counter &lt;- counter + 1\n}\n\n# combine the list\ntheft_bike_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\n\n\n\n\n\n\nWhile loops in R should generally be avoided for large datasets due to inefficiency, they remain a useful tool for automating repetitive tasks and reducing the risk of errors. For smaller datasets or tasks that cannot easily be vectorised, loops can still be effective and simplify the code.\n\n\n\nWe now have a spatial dataframe that contains the bike theft clusters in London, as defined by the DBSCAN clustering algorithm. Let’s quickly map these clusters:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify colours\n  tm_dots(\n    col = \"#636363\",\n    size = 0.01,\n  ) +\n\n  # shape, polygon\n  tm_shape(theft_bike_clusters) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#fdc086\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 6: DBSCAN-identified clusters of reported bicycle theft in London."
  },
  {
    "objectID": "03-point-pattern.html#assignment",
    "href": "03-point-pattern.html#assignment",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "Now that we know how to work with point locatoin data, we can again apply a similar analysis to road crashes in London in 2022 that we used last week. This time we will use this dataset to assess whether road crashes cluster in specific areas. Try the following:\n\nCreate a Kernel Density Estimation (KDE) of all road crashes that occurred in London in 2022.\nUsing DBSCAN output, create a cluster map of serious and fatal road crashes in London\n\nIf you no longer have a copy of the 2022 London STATS19 Road Collision dataset, you can download it using the link provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload"
  },
  {
    "objectID": "03-point-pattern.html#before-you-leave",
    "href": "03-point-pattern.html#before-you-leave",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "With access to point event data, geographers aim to identify underlying patterns. This week, we explored several techniques that help us analyse and interpret such data. That is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "01-spatial.html",
    "href": "01-spatial.html",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "This week’s lecture offered a comprehensive introduction to the Geocomputation module, highlighting how and why it differs from a traditional GIScience course. In this week’s tutorial, we will introduce you to using R and RStudio for working with spatial data, focusing specifically on how R can be used to make maps.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nBrundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\n\n\n\n\n\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nFranklin, S., Houlden, V., Robinson, C. et al. 2021. Who counts? Gender, Gatekeeping, and Quantitative Human Geography. The Professional Geographer 73(1): 48-61. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]\n\n\n\n\n\nIn RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths and allows us to keep track of our installed packages through renv.\n\n\n\n\n\n\nPackage management in R involves handling the installation, updating, and tracking of external libraries needed for your code. This ensures that your R scripts can run smoothly without issues related to missing or incompatible packages. Within an RStudio project, you can use renv to create a reproducible environment by capturing the specific package versions used in the project. This means that anyone working on or revisiting the project will have access to the same package setup, preventing problems caused by package updates or changes.\n\n\n\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0030, and select the location on your computer where you want to save this project by clicking on Browse….\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0030 project. For example, you might use your Geocomputation folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nTick the checkbox for Use renv with this project and click on Create Project. You should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0030.\nWith our GEOG0030 project ready to go, in this first tutorial we will look at the distribution of the share of European immigrants across London. The data covers the number of people residing in London that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the Middle Layer Super Output Area (MSOA) level.\n\n\n\n\n\n\nAn MSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 5,000 to 15,000 people and is designed to ensure consistent data reporting. MSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the file in your project folder under data/attributes. Along with this dataset, we also have access to a GeoPackage that contains the MSOA boundaries. Save this file under data/spatial, respectively.\n\n\n\n\n\n\nYou will to have created a folder named data within your RStudio Project directory, inside which you will have to have a folder named attributes and a folder named spatial.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA Census 2021 European Population\ncsv\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w01-european-population-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, the installation of the sf library may fail because additional (non-R) libraries are required which are automatically installed for Windows users. If you encounter installation issues,, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load attribute dataset\nmsoa_eur &lt;- read_csv(\"data/attributes/London-MSOA-European.csv\")\n\nRows: 1002 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): msoa21cd\ndbl (2): eur21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(msoa_eur)\n\n# A tibble: 6 × 3\n  msoa21cd  eur21 pop21\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E02000001  1926  8582\n2 E02000002  1102  8280\n3 E02000003  1930 11542\n4 E02000004   808  6640\n5 E02000005  1541 11082\n6 E02000007  1365 10159\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nThe first thing we want to do when we load spatial data is to plot the data to check whether everything is in order. To do this, we can simply use the base R plot() function\n\n\n\nR code\n\n# plot data\nplot(msoa21, max.plot = 1, main = \"\")\n\n\n\n\n\nFigure 1: Quick plot to inspect the MSOA spatial data.\n\n\n\n\nYou should see your msoa21 plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the attributes of the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 9\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n\nWe can further establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(msoa21)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want.\n\n\n\nNow we have our dataset containing London’s European born population and the MSOA spatial boundaries loaded, we can join these together using an Attribute Join. Before proceeding with the join, we need to verify that a matching unique identifier exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(msoa21)\n\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n# inspect column names\nnames(msoa_eur)\n\n[1] \"msoa21cd\" \"eur21\"    \"pop21\"   \n\n\nThe msoa21cd columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect spatial dataset\nhead(sort(msoa21$msoa21cd))\n\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n# inspect attribute dataset\nhead(sort(msoa_eur$msoa21cd))\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\nmsoa21 &lt;- msoa21 |&gt; \n  left_join(msoa_eur, by = c('msoa21cd' = 'msoa21cd'))\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 11\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid eur21 pop21\n1 {71249043-B176-4306-BA6C-D1A993B1B741}  1926  8582\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E}  1102  8280\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B}  1930 11542\n4 {511181CD-E71F-4C63-81EE-E8E76744A627}   808  6640\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87}  1541 11082\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1}  1365 10159\n                            geom\n1 MULTIPOLYGON (((532135.1 18...\n2 MULTIPOLYGON (((548881.6 19...\n3 MULTIPOLYGON (((549102.4 18...\n4 MULTIPOLYGON (((551550.1 18...\n5 MULTIPOLYGON (((549099.6 18...\n6 MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n [1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n [7] \"long\"      \"globalid\"  \"eur21\"     \"pop21\"     \"geom\"     \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA.\nWe can also compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(msoa21)\n\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$eur21))\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$pop21))\n\n[1] 1002\n\n\nNo missing values. In this case we did not expect any missing values, so this confirms that all our full attribute dataset has been linked to the spatial dataset.\nWe are almost ready to map the data. Only thing that is left is for us to calculate the share of European-born immigrants within each MSOA:\n\n\n\nR code\n\n# calculate proportion\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(prop_eur21 = eur21/pop21)\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and proportional symbol maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the MSOA borders to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X”, where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our msoa21 spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are coloured based on the proportion of European immigrants. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red or #fc9272).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our prop_eur21 variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n  # specify column\n  tm_polygons(\n    col = \"prop_eur21\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nn\nSpecifies the number of classification breaks.\n\n\nstyle\nDefines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\n\n\nbreaks\nAllows you to set specific numeric breaks when using the fixed style.\n\n\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n  # specify column, classes\n  tm_polygons(\n    col = \"prop_eur21\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\n\n\n\n\nStyling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntm_layout()\nCustomise titles, fonts, legends, and other layout elements.\n\n\ntm_compass()\nAdd and style a North arrow or compass.\n\n\ntm_scale_bar()\nAdd and style a scale bar.\n\n\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"prop_eur21\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population born in Europe\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as separate map layer:\n\n\n\nR code\n\n# map labels\nlab &lt;- msoa21 |&gt;\n  filter(msoa21cd == \"E02000642\" | msoa21cd == \"E02000180\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\nlon_eurpop &lt;-\n  # shape, polygons\n  tm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"prop_eur21\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # label centroids\n  tm_shape(lab) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"msoa21nm\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population born in Europe\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2021, Office for National Statistics\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\nlon_eurpop\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = lon_eurpop, filename = \"london-european-population.jpg\", width = 15,\n    height = 15, units = c(\"cm\"))\n\n\n\n\n\nNow that we have prepared our dataset and created our initial maps in R, we can also try and map the distribution of the proportion of European immigrants across Wales and experiment with different mapping parameters. Follow these steps:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of people residing in Wales that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the MSOA level.\nA GeoPackage file containing the 2021 MSOA spatial boundaries for England and Wales.\n\nLoad both datasets and merge them together. Make sure to only retain those MSOAs that belong to Wales.\nCreate a map that shows the proportion of the population residing in Wales that is born in Europe.\nExperiment by adjusting various map parameters, such as the colour scheme, map labels, and data classification method.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWales MSOA Census 2021 European Population\ncsv\nDownload\n\n\nEngland and Wales MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\nAnd that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week."
  },
  {
    "objectID": "01-spatial.html#lecture-slides",
    "href": "01-spatial.html#lecture-slides",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "01-spatial.html#reading-list",
    "href": "01-spatial.html#reading-list",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Brundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\n\n\n\n\n\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nFranklin, S., Houlden, V., Robinson, C. et al. 2021. Who counts? Gender, Gatekeeping, and Quantitative Human Geography. The Professional Geographer 73(1): 48-61. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]"
  },
  {
    "objectID": "01-spatial.html#europeans-in-london",
    "href": "01-spatial.html#europeans-in-london",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "In RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths and allows us to keep track of our installed packages through renv.\n\n\n\n\n\n\nPackage management in R involves handling the installation, updating, and tracking of external libraries needed for your code. This ensures that your R scripts can run smoothly without issues related to missing or incompatible packages. Within an RStudio project, you can use renv to create a reproducible environment by capturing the specific package versions used in the project. This means that anyone working on or revisiting the project will have access to the same package setup, preventing problems caused by package updates or changes.\n\n\n\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0030, and select the location on your computer where you want to save this project by clicking on Browse….\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0030 project. For example, you might use your Geocomputation folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nTick the checkbox for Use renv with this project and click on Create Project. You should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0030.\nWith our GEOG0030 project ready to go, in this first tutorial we will look at the distribution of the share of European immigrants across London. The data covers the number of people residing in London that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the Middle Layer Super Output Area (MSOA) level.\n\n\n\n\n\n\nAn MSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 5,000 to 15,000 people and is designed to ensure consistent data reporting. MSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the file in your project folder under data/attributes. Along with this dataset, we also have access to a GeoPackage that contains the MSOA boundaries. Save this file under data/spatial, respectively.\n\n\n\n\n\n\nYou will to have created a folder named data within your RStudio Project directory, inside which you will have to have a folder named attributes and a folder named spatial.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA Census 2021 European Population\ncsv\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w01-european-population-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, the installation of the sf library may fail because additional (non-R) libraries are required which are automatically installed for Windows users. If you encounter installation issues,, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load attribute dataset\nmsoa_eur &lt;- read_csv(\"data/attributes/London-MSOA-European.csv\")\n\nRows: 1002 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): msoa21cd\ndbl (2): eur21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(msoa_eur)\n\n# A tibble: 6 × 3\n  msoa21cd  eur21 pop21\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E02000001  1926  8582\n2 E02000002  1102  8280\n3 E02000003  1930 11542\n4 E02000004   808  6640\n5 E02000005  1541 11082\n6 E02000007  1365 10159\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nThe first thing we want to do when we load spatial data is to plot the data to check whether everything is in order. To do this, we can simply use the base R plot() function\n\n\n\nR code\n\n# plot data\nplot(msoa21, max.plot = 1, main = \"\")\n\n\n\n\n\nFigure 1: Quick plot to inspect the MSOA spatial data.\n\n\n\n\nYou should see your msoa21 plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the attributes of the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 9\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n\nWe can further establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(msoa21)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want.\n\n\n\nNow we have our dataset containing London’s European born population and the MSOA spatial boundaries loaded, we can join these together using an Attribute Join. Before proceeding with the join, we need to verify that a matching unique identifier exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(msoa21)\n\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n# inspect column names\nnames(msoa_eur)\n\n[1] \"msoa21cd\" \"eur21\"    \"pop21\"   \n\n\nThe msoa21cd columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect spatial dataset\nhead(sort(msoa21$msoa21cd))\n\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n# inspect attribute dataset\nhead(sort(msoa_eur$msoa21cd))\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\nmsoa21 &lt;- msoa21 |&gt; \n  left_join(msoa_eur, by = c('msoa21cd' = 'msoa21cd'))\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 11\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid eur21 pop21\n1 {71249043-B176-4306-BA6C-D1A993B1B741}  1926  8582\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E}  1102  8280\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B}  1930 11542\n4 {511181CD-E71F-4C63-81EE-E8E76744A627}   808  6640\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87}  1541 11082\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1}  1365 10159\n                            geom\n1 MULTIPOLYGON (((532135.1 18...\n2 MULTIPOLYGON (((548881.6 19...\n3 MULTIPOLYGON (((549102.4 18...\n4 MULTIPOLYGON (((551550.1 18...\n5 MULTIPOLYGON (((549099.6 18...\n6 MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n [1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n [7] \"long\"      \"globalid\"  \"eur21\"     \"pop21\"     \"geom\"     \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA.\nWe can also compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(msoa21)\n\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$eur21))\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$pop21))\n\n[1] 1002\n\n\nNo missing values. In this case we did not expect any missing values, so this confirms that all our full attribute dataset has been linked to the spatial dataset.\nWe are almost ready to map the data. Only thing that is left is for us to calculate the share of European-born immigrants within each MSOA:\n\n\n\nR code\n\n# calculate proportion\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(prop_eur21 = eur21/pop21)\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and proportional symbol maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the MSOA borders to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X”, where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our msoa21 spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are coloured based on the proportion of European immigrants. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red or #fc9272).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our prop_eur21 variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n  # specify column\n  tm_polygons(\n    col = \"prop_eur21\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nn\nSpecifies the number of classification breaks.\n\n\nstyle\nDefines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\n\n\nbreaks\nAllows you to set specific numeric breaks when using the fixed style.\n\n\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n  # specify column, classes\n  tm_polygons(\n    col = \"prop_eur21\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer."
  },
  {
    "objectID": "01-spatial.html#styling-spatial-data",
    "href": "01-spatial.html#styling-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Styling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntm_layout()\nCustomise titles, fonts, legends, and other layout elements.\n\n\ntm_compass()\nAdd and style a North arrow or compass.\n\n\ntm_scale_bar()\nAdd and style a scale bar.\n\n\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"prop_eur21\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population born in Europe\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as separate map layer:\n\n\n\nR code\n\n# map labels\nlab &lt;- msoa21 |&gt;\n  filter(msoa21cd == \"E02000642\" | msoa21cd == \"E02000180\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\nlon_eurpop &lt;-\n  # shape, polygons\n  tm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"prop_eur21\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # label centroids\n  tm_shape(lab) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"msoa21nm\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population born in Europe\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2021, Office for National Statistics\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\nlon_eurpop\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = lon_eurpop, filename = \"london-european-population.jpg\", width = 15,\n    height = 15, units = c(\"cm\"))"
  },
  {
    "objectID": "01-spatial.html#assignment",
    "href": "01-spatial.html#assignment",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Now that we have prepared our dataset and created our initial maps in R, we can also try and map the distribution of the proportion of European immigrants across Wales and experiment with different mapping parameters. Follow these steps:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of people residing in Wales that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the MSOA level.\nA GeoPackage file containing the 2021 MSOA spatial boundaries for England and Wales.\n\nLoad both datasets and merge them together. Make sure to only retain those MSOAs that belong to Wales.\nCreate a map that shows the proportion of the population residing in Wales that is born in Europe.\nExperiment by adjusting various map parameters, such as the colour scheme, map labels, and data classification method.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWales MSOA Census 2021 European Population\ncsv\nDownload\n\n\nEngland and Wales MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload"
  },
  {
    "objectID": "01-spatial.html#before-you-leave",
    "href": "01-spatial.html#before-you-leave",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "And that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week."
  },
  {
    "objectID": "09-maps.html",
    "href": "09-maps.html",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "So far, we have primarily created univariate choropleth maps to visualise data across defined spatial areas, such as LSOAs. This week, we will expand on this by exploring bivariate maps, which illustrate the relationship between two variables within a single visualisation. We will also introduce you to the ggplot2 library and pay some attention to creating functions to automate and repeat analyses for different spatial units, allowing for more efficient and consistent workflows\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 1-32. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2021. Atlas of the invisible: maps and graphics that will change how you see the world. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 19: Functions. [Link]\n\n\n\n\n\nThis week, we will look at the change in unemployment across London between 2011 and 2021. Specifically, we will try to reconcile 2011 Census data with 2021 Census data and present the results on a bivariate map. The data cover all usual residents, as recorded in the 2011 and 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are periodically updated to reflect changes in population and other factors, resulting in occasional boundary adjustments. Consequently, it is essential to use the 2011 LSOA boundaries when mapping 2011 Census data and the 2021 LSOA boundaries for 2021 Census data. To facilitate mapping changes over time, we have access to a csv file containing a best-fit lookup table. This table provides a correspondence between 2011 LSOAs and their equivalent 2021 LSOAs, enabling consistent comparison across census periods.\n\n\n\nYou can download three files below and save them in your project folder under data/attributes. Along with these dataset, we also have access to a GeoPackage that contains the 2021 LSOA boundaries.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2011 Unemployment\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Unemployment\ncsv\nDownload\n\n\nEngland and Wales LSOA 2011-2021 Lookup\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w09-unemployment-change.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(biscale)\nlibrary(cowplot)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load all three files into memory:\n\n\n\nR code\n\n# read 2011 data\nlsoa11 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2011.csv\")\n\n\nRows: 4835 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa11cd, lsoa11nm\ndbl (2): eco_active_unemployed11, pop11\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read 2021 data\nlsoa21 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2021.csv\")\n\nRows: 4994 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, lsoa21nm\ndbl (2): eco_active_unemployed21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read lookup data\nlookup &lt;- read_csv(\"data/attributes/England-Wales-LSOA-2011-2021.csv\")\n\nRows: 35796 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): lsoa11cd, lsoa11nm, lsoa21cd, lsoa21nm, chgind\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa11)\n\n# A tibble: 6 × 4\n  lsoa11cd  lsoa11nm                  eco_active_unemployed11 pop11\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            34  1221\n2 E01000002 City of London 001B                            16  1196\n3 E01000003 City of London 001C                            39  1102\n4 E01000005 City of London 001E                            46   773\n5 E01000006 Barking and Dagenham 016A                      83  1251\n6 E01000007 Barking and Dagenham 015A                      87  1034\n\n# inspect\nhead(lsoa21)\n\n# A tibble: 6 × 4\n  lsoa21cd  lsoa21nm                  eco_active_unemployed21 pop21\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            32  1478\n2 E01000002 City of London 001B                            30  1383\n3 E01000003 City of London 001C                            68  1614\n4 E01000005 City of London 001E                            60  1099\n5 E01000006 Barking and Dagenham 016A                      57  1844\n6 E01000007 Barking and Dagenham 015A                     154  2908\n\n# inspect\nhead(lookup)\n\n# A tibble: 6 × 5\n  lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm                  chgind\n  &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt; \n1 E01000001 City of London 001A       E01000001 City of London 001A       U     \n2 E01000002 City of London 001B       E01000002 City of London 001B       U     \n3 E01000003 City of London 001C       E01000003 City of London 001C       U     \n4 E01000005 City of London 001E       E01000005 City of London 001E       U     \n5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A U     \n6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A U     \n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nTo analyse changes in unemployment over time, we need to combine the 2011 and 2021 unemployment data. Previously, we have joined datasets using a unique identifier found in both, assuming the identifiers match exactly and represent the same geographies. However, when comparing the unique identifiers from (lsoa11cd and lsoa21cd) these datasets, we can see some clear differences:\n\n\n\nR code\n\n# inspect\nlength(unique(lsoa11$lsoa11cd))\n\n\n[1] 4835\n\n# inspect\nlength(unique(lsoa21$lsoa21cd))\n\n[1] 4994\n\n\nThe number of LSOAs increased between the 2011 and 2021 Census due to boundary changes. Specifically, some 2011 LSOAs have been split into multiple 2021 LSOAs, while others have been merged into a single 2021 LSOA polygon. The relationship between 2011 and 2021 LSOAs is captured in the chgind column of the lookup table, which flags the type of change for each case.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nUnchanged: The LSOA boundaries remain the same from 2011 to 2021, allowing direct comparisons between data for these years.\n\n\nS\nSplit: A 2011 LSOA has been divided into two or more 2021 LSOAs. Each split 2021 LSOA will have a corresponding record in the table, enabling comparisons by aggregating the 2021 LSOA data back to the 2011 boundary.\n\n\nM\nMerged: Two or more 2011 LSOAs have been combined into a single 2021 LSOA. Comparisons can be made by aggregating the 2011 LSOA data to match the new 2021 boundary.\n\n\nX\nIrregular/Fragmented: The relationship between 2011 and 2021 LSOAs is complex due to redesigns from local authority boundary changes or efforts to improve social homogeneity. These cases do not allow straightforward comparisons between 2011 and 2021 data.\n\n\n\nAlthough there are different approaches to handling this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across the corresponding 2021 LSOAs.\nCombine the total crimes for 2011 LSOAs that have been merged into a single 2021 LSOA.\n\n\n\n\n\n\n\nThe LSOA boundary changes in London between 2011 and 2021 did not result in any irregular or fragmented boundaries. Therefore, we only need to address the merged and split LSOAs.\n\n\n\nThis means we will apply weightings to the values based on their relationships. We can prepare these weightings as follows:\n\n\n\nR code\n\n# for unchanged LSOAs keep weighting the same\nlsoa_lookup_same &lt;- lookup |&gt;\n    filter(chgind == \"U\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weighting the same\nlsoa_lookup_merge &lt;- lookup |&gt;\n    filter(chgind == \"M\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh proportionally to the number of 2021 LSOAs\nlsoa_lookup_split &lt;- lookup |&gt;\n    filter(chgind == \"S\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 6\n# Groups:   lsoa11cd [34,747]\n   lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm           chgind     n\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;  &lt;dbl&gt;\n 1 E01000001 City of London 001A       E01000001 City of London 00… U          1\n 2 E01000002 City of London 001B       E01000002 City of London 00… U          1\n 3 E01000003 City of London 001C       E01000003 City of London 00… U          1\n 4 E01000005 City of London 001E       E01000005 City of London 00… U          1\n 5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagen… U          1\n 6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagen… U          1\n 7 E01000008 Barking and Dagenham 015B E01000008 Barking and Dagen… U          1\n 8 E01000009 Barking and Dagenham 016B E01000009 Barking and Dagen… U          1\n 9 E01000011 Barking and Dagenham 016C E01000011 Barking and Dagen… U          1\n10 E01000012 Barking and Dagenham 015D E01000012 Barking and Dagen… U          1\n# ℹ 35,776 more rows\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nWe can now join the lookup table on the 2011 LSOA data:\n\n\n\nR code\n\n# join to lsoa data\nlsoa11_21 &lt;- lsoa11 |&gt;\n  select(-lsoa11nm) |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n\nIf we now compare the number of records in our lsoa11_21 dataset with the original 2011 and 2021 LSOA datasets, we notice some differences:\n\n\n\nR code\n\n# lsoa 2011\nnrow(lsoa11)\n\n\n[1] 4835\n\n# lsoa 2021\nnrow(lsoa21)\n\n[1] 4994\n\n# lookup\nnrow(lsoa11_21)\n\n[1] 5016\n\n\nSomehow, the number of our LSOAs seem to have increased. However, this is not an actual increase in LSOAs; rather, the change in the number of LSOAs is due to our one-to-many relationships. A single 2011 LSOA can correspond to multiple 2021 LSOAs, which causes the data for that 2011 LSOA to be duplicated in the join operation. Fortunately, we anticipated this and have already created the necessary weightings. We can now apply these weightings to assign our 2011 population estimates to the 2021 LSOA boundaries as follows:\n\n\n\nR code\n\n# weigh data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(eco_active_unemployed11 = eco_active_unemployed11 * n) |&gt;\n    mutate(pop11 = pop11 * n)\n\n# assign 2011 to 2021\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    group_by(lsoa21cd) |&gt;\n    mutate(eco_active_unemployed11_lsoa21 = sum(eco_active_unemployed11)) |&gt;\n    mutate(pop11_lsoa21 = sum(pop11)) |&gt;\n    distinct(lsoa21cd, eco_active_unemployed11_lsoa21, pop11_lsoa21)\n\n\nWe should now be left with all 2021 LSOAs, each containing the corresponding 2011 values, adjusted according to the merged and split LSOA relationships. We can quickly check this by comparing the original values with the re-assigned values:\n\n\n\nR code\n\n# inspect number\nnrow(lsoa21)\n\n\n[1] 4994\n\n# inspect number\nnrow(lsoa11_21)\n\n[1] 4994\n\n# inspect count original data\nsum(lsoa11$pop11)\n\n[1] 6117482\n\n# inspect count re-assigned data\nsum(lsoa11_21$pop11_lsoa21)\n\n[1] 6117482\n\n\nWe can now join the 2011 and 2021 population data together:\n\n\n\nR code\n\n# join 2011 data with 2021 data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n  left_join(lsoa21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n\n\n\n\nBivariate maps are visualisations that represent two different variables simultaneously on a single map, using combinations of colours, patterns, or symbols to convey relationships between them. They are commonly used to explore spatial correlations or patterns, such as comparing population density with income levels across a region. We will use a bivariate map to illustrate changes in unemployment between 2011 and 2021 in London.\nWe will start by calculating unemployment rates for both years and classifing them into categories using the biscale library:\n\n\n\nR code\n\n# unemployment rates\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(unemp11 = eco_active_unemployed11_lsoa21/pop11_lsoa21) |&gt;\n    mutate(unemp21 = eco_active_unemployed21/pop21) |&gt;\n    select(-lsoa21nm)\n\n# add classes\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# inspect\nhead(lsoa11_21$bi_class)\n\n\n[1] \"1-1\" \"1-1\" \"3-1\" \"3-2\" \"2-3\" \"3-3\"\n\n\n\n\n\n\n\n\nThe dim argument is used to control the extent of the legend. For instance, dim = 2 will produce a two-by-two map where dim = 3 will produce a three-by-three map.\n\n\n\nInstead of using tmap to create our map, we will need to use the ggplot2 library. Like tmap, ggplot2 is based on the grammar of graphics, allowing you to build a graphic step by step by layering components such as data, aesthetics, and geometries. While we will explore ggplot2 in more detail next week, for now, we will use it to create a bivariate map by adding the necessary layers one at a time.\n\n\n\n\n\n\nBivariate maps are not supported in Version 3 of tmap. However, Version 4, which is currently under development, will include functionality for creating bivariate maps. This new version is expected to be released on CRAN soon\n\n\n\nOnce breaks are created, we can use bi_scale_fill() as part of our ggplot() call:\n\n\n\nR code\n\n# load spatial data\nlsoa21_sf &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join unemployment data\nlsoa21_sf &lt;- lsoa21_sf |&gt;\n  left_join(lsoa11_21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# bivariate map using ggplot\nggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n\n\n\nFigure 1: Bivariate map change of unemployment rates in London 2011-2021.\n\n\n\n\nShades closer to grey indicate areas with relative low unemployment rates in both years, while shades closer to blue represent areas with high unemployment rates in both years. Mixed tones suggest areas where unemployment rates have changed between 2011 and 2021, with the specific colour intensity reflecting the degree and direction of this change.\nWe have set show.legend = FALSE to allow us to manually add our own bivariate legend. The palette and dimensions should align with those used in bi_class() for dimensions and bi_scale_fill() for both dimensions and palette to ensure consistency. We can create a legend and combine it with a map object as follows:\n\n\n\nR code\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0, 0, .3, 0.3)\n\n\n\n\n\nFigure 2: Bivariate map of relative changes in unemployment rates in London 2011-2021.\n\n\n\n\n\n\n\n\n\n\nThe values in the draw_plot() function specify the relative location and size of each map object on the canvas. Adjusting these values often requires some trial and error to achieve the desired positioning, as they control the x and y coordinates for placement and the width and height proportions of each object.\n\n\n\nWe have used LSOA data to create a bivariate map illustrating changes in unemployment rates. However, with nearly 5,000 LSOAs in London, this map can be challenging to interpret due to the high level of detail. Let’s zoom in to Lambeth:\n\n\n\nR code\n\n# select lambeth\nlsoa21_lambeth &lt;- lsoa21_sf |&gt;\n  filter(str_detect(lsoa21nm, \"Lambeth\"))\n\n# add classes\nlsoa21_lambeth &lt;- lsoa21_lambeth |&gt;\n  bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_lambeth,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nbivmap &lt;- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n# plot\nbivmap\n\n\n\n\n\nFigure 3: Bivariate map of relative changes in unemployment rates in Lambeth 2011-2021.\n\n\n\n\n\n\n\nSo we have now created a map of Lambeth. But what if we need to create a map for every borough in London? In R, you can create a basic function using the function() keyword, which allows you to encapsulate reusable code. A function can take arguments (inputs), perform operations, and return a result. A simple example of a function that adds two values together:\n\n\n\nR code\n\n# define function to add two numbers\nadd_numbers &lt;- function(a, b) {\n    result &lt;- a + b\n    return(result)\n}\n\n# use function\nadd_numbers(5, 3)\n\n\n[1] 8\n\n\nWe can use the same logic to construct a basic function that takes a spatial dataframe and the name of a borough as input and subsequently creates a bivarate map as output:\n\n\n\nR code\n\n# define function to create bivariate unemployment maps\ncreate_bivariate_map &lt;- function(spatial_df, borough_name) {\n  # select borough\n  spatial_df_filter &lt;- spatial_df |&gt;\n    filter(str_detect(lsoa21nm, borough_name))\n\n  # add classes\n  spatial_df_filter &lt;- spatial_df_filter |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n  # bivariate map object\n  map &lt;- ggplot() +\n    geom_sf(\n      data = spatial_df_filter,\n      mapping = aes(fill = bi_class),\n      color = NA,\n      show.legend = FALSE\n    ) +\n    bi_scale_fill(\n      pal = \"DkBlue2\",\n      dim = 3\n    ) +\n    bi_theme()\n\n  # legend object\n  legend &lt;- bi_legend(\n    pal = \"DkBlue2\",\n    dim = 3,\n    xlab = \"Higher Unemployment 2021\",\n    ylab = \"Higher Unemployment 2011\",\n    size = 6\n  )\n\n  # combine\n  bivariate_map &lt;- ggdraw() +\n    draw_plot(map, 0, 0, 1, 1) +\n    draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n  # return value\n  return(bivariate_map)\n}\n\n\nWe can now use this function to quickly recreate maps for individual boroughs. Let’s try it for the London Borough of Hammersmith:\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Hammersmith\")\n\n\n\n\n\nFigure 4: Bivariate map of relative changes in unemployment rates in Hammersmith 2011-2021.\n\n\n\n\nWhat about Kensington and Chelsea? Or Wandsworth?\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Kensington and Chelsea\")\n\n\n\n\n\nFigure 5: Bivariate map of relative changes in unemployment rates in Kensington and Chelsea 2011-2021.\n\n\n\n\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Wandsworth\")\n\n\n\n\n\nFigure 6: Bivariate map of relative changes in unemployment rates in Wandsworth 2011-2021.\n\n\n\n\n\n\n\n\nWhen the same action, such as mapping a particular variable, needs to be repeated across different datasets or regions, a function ensures that the process is consistent and can be applied easily without rewriting code. This not only saves time but also reduces the risk of errors, as you can simply call the function with different inputs, ensuring the same analysis steps are followed each time.\nHaving created a function to generate bivariate maps, we can now create a function for univariate maps. Using the dataset from the tutorial, try to:\n\nWrite a function with two parameters that uses the standard tmap library to map unemployment rates at the LSOA-level for a specified borough.\nAdd a third parameter that specifies which variable should be mapped (e.g., unemployment rates in 2011 or 2021).\nAdd a fourth parameter to define the colour palette to be used for the map.\n\n\n\n\n\n\n\nIf you would like a more comprehensive introduction to writing your own functions in R, refer to Chapter 19: Functions in R for Data Science. This chapter provides a detailed explanation of how to create and use functions, along with best practices for making your code more efficient and reusable.\n\n\n\n\n\n\nThat is it for today. You should now be able to use lookup tables, create bivariate maps with the ggplot2 library, and build basic reproducible functions. Next week, we will dive deeper into the ggplot2 library, but for now that is this week’s Geocompuation done!"
  },
  {
    "objectID": "09-maps.html#lecture-slides",
    "href": "09-maps.html#lecture-slides",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "09-maps.html#reading-list",
    "href": "09-maps.html#reading-list",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 1-32. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2021. Atlas of the invisible: maps and graphics that will change how you see the world. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 19: Functions. [Link]"
  },
  {
    "objectID": "09-maps.html#unemployment-in-london",
    "href": "09-maps.html#unemployment-in-london",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "This week, we will look at the change in unemployment across London between 2011 and 2021. Specifically, we will try to reconcile 2011 Census data with 2021 Census data and present the results on a bivariate map. The data cover all usual residents, as recorded in the 2011 and 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are periodically updated to reflect changes in population and other factors, resulting in occasional boundary adjustments. Consequently, it is essential to use the 2011 LSOA boundaries when mapping 2011 Census data and the 2021 LSOA boundaries for 2021 Census data. To facilitate mapping changes over time, we have access to a csv file containing a best-fit lookup table. This table provides a correspondence between 2011 LSOAs and their equivalent 2021 LSOAs, enabling consistent comparison across census periods.\n\n\n\nYou can download three files below and save them in your project folder under data/attributes. Along with these dataset, we also have access to a GeoPackage that contains the 2021 LSOA boundaries.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2011 Unemployment\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Unemployment\ncsv\nDownload\n\n\nEngland and Wales LSOA 2011-2021 Lookup\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w09-unemployment-change.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(biscale)\nlibrary(cowplot)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load all three files into memory:\n\n\n\nR code\n\n# read 2011 data\nlsoa11 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2011.csv\")\n\n\nRows: 4835 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa11cd, lsoa11nm\ndbl (2): eco_active_unemployed11, pop11\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read 2021 data\nlsoa21 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2021.csv\")\n\nRows: 4994 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, lsoa21nm\ndbl (2): eco_active_unemployed21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read lookup data\nlookup &lt;- read_csv(\"data/attributes/England-Wales-LSOA-2011-2021.csv\")\n\nRows: 35796 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): lsoa11cd, lsoa11nm, lsoa21cd, lsoa21nm, chgind\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa11)\n\n# A tibble: 6 × 4\n  lsoa11cd  lsoa11nm                  eco_active_unemployed11 pop11\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            34  1221\n2 E01000002 City of London 001B                            16  1196\n3 E01000003 City of London 001C                            39  1102\n4 E01000005 City of London 001E                            46   773\n5 E01000006 Barking and Dagenham 016A                      83  1251\n6 E01000007 Barking and Dagenham 015A                      87  1034\n\n# inspect\nhead(lsoa21)\n\n# A tibble: 6 × 4\n  lsoa21cd  lsoa21nm                  eco_active_unemployed21 pop21\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            32  1478\n2 E01000002 City of London 001B                            30  1383\n3 E01000003 City of London 001C                            68  1614\n4 E01000005 City of London 001E                            60  1099\n5 E01000006 Barking and Dagenham 016A                      57  1844\n6 E01000007 Barking and Dagenham 015A                     154  2908\n\n# inspect\nhead(lookup)\n\n# A tibble: 6 × 5\n  lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm                  chgind\n  &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt; \n1 E01000001 City of London 001A       E01000001 City of London 001A       U     \n2 E01000002 City of London 001B       E01000002 City of London 001B       U     \n3 E01000003 City of London 001C       E01000003 City of London 001C       U     \n4 E01000005 City of London 001E       E01000005 City of London 001E       U     \n5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A U     \n6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A U     \n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nTo analyse changes in unemployment over time, we need to combine the 2011 and 2021 unemployment data. Previously, we have joined datasets using a unique identifier found in both, assuming the identifiers match exactly and represent the same geographies. However, when comparing the unique identifiers from (lsoa11cd and lsoa21cd) these datasets, we can see some clear differences:\n\n\n\nR code\n\n# inspect\nlength(unique(lsoa11$lsoa11cd))\n\n\n[1] 4835\n\n# inspect\nlength(unique(lsoa21$lsoa21cd))\n\n[1] 4994\n\n\nThe number of LSOAs increased between the 2011 and 2021 Census due to boundary changes. Specifically, some 2011 LSOAs have been split into multiple 2021 LSOAs, while others have been merged into a single 2021 LSOA polygon. The relationship between 2011 and 2021 LSOAs is captured in the chgind column of the lookup table, which flags the type of change for each case.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nUnchanged: The LSOA boundaries remain the same from 2011 to 2021, allowing direct comparisons between data for these years.\n\n\nS\nSplit: A 2011 LSOA has been divided into two or more 2021 LSOAs. Each split 2021 LSOA will have a corresponding record in the table, enabling comparisons by aggregating the 2021 LSOA data back to the 2011 boundary.\n\n\nM\nMerged: Two or more 2011 LSOAs have been combined into a single 2021 LSOA. Comparisons can be made by aggregating the 2011 LSOA data to match the new 2021 boundary.\n\n\nX\nIrregular/Fragmented: The relationship between 2011 and 2021 LSOAs is complex due to redesigns from local authority boundary changes or efforts to improve social homogeneity. These cases do not allow straightforward comparisons between 2011 and 2021 data.\n\n\n\nAlthough there are different approaches to handling this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across the corresponding 2021 LSOAs.\nCombine the total crimes for 2011 LSOAs that have been merged into a single 2021 LSOA.\n\n\n\n\n\n\n\nThe LSOA boundary changes in London between 2011 and 2021 did not result in any irregular or fragmented boundaries. Therefore, we only need to address the merged and split LSOAs.\n\n\n\nThis means we will apply weightings to the values based on their relationships. We can prepare these weightings as follows:\n\n\n\nR code\n\n# for unchanged LSOAs keep weighting the same\nlsoa_lookup_same &lt;- lookup |&gt;\n    filter(chgind == \"U\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weighting the same\nlsoa_lookup_merge &lt;- lookup |&gt;\n    filter(chgind == \"M\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh proportionally to the number of 2021 LSOAs\nlsoa_lookup_split &lt;- lookup |&gt;\n    filter(chgind == \"S\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 6\n# Groups:   lsoa11cd [34,747]\n   lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm           chgind     n\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;  &lt;dbl&gt;\n 1 E01000001 City of London 001A       E01000001 City of London 00… U          1\n 2 E01000002 City of London 001B       E01000002 City of London 00… U          1\n 3 E01000003 City of London 001C       E01000003 City of London 00… U          1\n 4 E01000005 City of London 001E       E01000005 City of London 00… U          1\n 5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagen… U          1\n 6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagen… U          1\n 7 E01000008 Barking and Dagenham 015B E01000008 Barking and Dagen… U          1\n 8 E01000009 Barking and Dagenham 016B E01000009 Barking and Dagen… U          1\n 9 E01000011 Barking and Dagenham 016C E01000011 Barking and Dagen… U          1\n10 E01000012 Barking and Dagenham 015D E01000012 Barking and Dagen… U          1\n# ℹ 35,776 more rows\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nWe can now join the lookup table on the 2011 LSOA data:\n\n\n\nR code\n\n# join to lsoa data\nlsoa11_21 &lt;- lsoa11 |&gt;\n  select(-lsoa11nm) |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n\nIf we now compare the number of records in our lsoa11_21 dataset with the original 2011 and 2021 LSOA datasets, we notice some differences:\n\n\n\nR code\n\n# lsoa 2011\nnrow(lsoa11)\n\n\n[1] 4835\n\n# lsoa 2021\nnrow(lsoa21)\n\n[1] 4994\n\n# lookup\nnrow(lsoa11_21)\n\n[1] 5016\n\n\nSomehow, the number of our LSOAs seem to have increased. However, this is not an actual increase in LSOAs; rather, the change in the number of LSOAs is due to our one-to-many relationships. A single 2011 LSOA can correspond to multiple 2021 LSOAs, which causes the data for that 2011 LSOA to be duplicated in the join operation. Fortunately, we anticipated this and have already created the necessary weightings. We can now apply these weightings to assign our 2011 population estimates to the 2021 LSOA boundaries as follows:\n\n\n\nR code\n\n# weigh data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(eco_active_unemployed11 = eco_active_unemployed11 * n) |&gt;\n    mutate(pop11 = pop11 * n)\n\n# assign 2011 to 2021\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    group_by(lsoa21cd) |&gt;\n    mutate(eco_active_unemployed11_lsoa21 = sum(eco_active_unemployed11)) |&gt;\n    mutate(pop11_lsoa21 = sum(pop11)) |&gt;\n    distinct(lsoa21cd, eco_active_unemployed11_lsoa21, pop11_lsoa21)\n\n\nWe should now be left with all 2021 LSOAs, each containing the corresponding 2011 values, adjusted according to the merged and split LSOA relationships. We can quickly check this by comparing the original values with the re-assigned values:\n\n\n\nR code\n\n# inspect number\nnrow(lsoa21)\n\n\n[1] 4994\n\n# inspect number\nnrow(lsoa11_21)\n\n[1] 4994\n\n# inspect count original data\nsum(lsoa11$pop11)\n\n[1] 6117482\n\n# inspect count re-assigned data\nsum(lsoa11_21$pop11_lsoa21)\n\n[1] 6117482\n\n\nWe can now join the 2011 and 2021 population data together:\n\n\n\nR code\n\n# join 2011 data with 2021 data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n  left_join(lsoa21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n\n\n\n\nBivariate maps are visualisations that represent two different variables simultaneously on a single map, using combinations of colours, patterns, or symbols to convey relationships between them. They are commonly used to explore spatial correlations or patterns, such as comparing population density with income levels across a region. We will use a bivariate map to illustrate changes in unemployment between 2011 and 2021 in London.\nWe will start by calculating unemployment rates for both years and classifing them into categories using the biscale library:\n\n\n\nR code\n\n# unemployment rates\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(unemp11 = eco_active_unemployed11_lsoa21/pop11_lsoa21) |&gt;\n    mutate(unemp21 = eco_active_unemployed21/pop21) |&gt;\n    select(-lsoa21nm)\n\n# add classes\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# inspect\nhead(lsoa11_21$bi_class)\n\n\n[1] \"1-1\" \"1-1\" \"3-1\" \"3-2\" \"2-3\" \"3-3\"\n\n\n\n\n\n\n\n\nThe dim argument is used to control the extent of the legend. For instance, dim = 2 will produce a two-by-two map where dim = 3 will produce a three-by-three map.\n\n\n\nInstead of using tmap to create our map, we will need to use the ggplot2 library. Like tmap, ggplot2 is based on the grammar of graphics, allowing you to build a graphic step by step by layering components such as data, aesthetics, and geometries. While we will explore ggplot2 in more detail next week, for now, we will use it to create a bivariate map by adding the necessary layers one at a time.\n\n\n\n\n\n\nBivariate maps are not supported in Version 3 of tmap. However, Version 4, which is currently under development, will include functionality for creating bivariate maps. This new version is expected to be released on CRAN soon\n\n\n\nOnce breaks are created, we can use bi_scale_fill() as part of our ggplot() call:\n\n\n\nR code\n\n# load spatial data\nlsoa21_sf &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join unemployment data\nlsoa21_sf &lt;- lsoa21_sf |&gt;\n  left_join(lsoa11_21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# bivariate map using ggplot\nggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n\n\n\nFigure 1: Bivariate map change of unemployment rates in London 2011-2021.\n\n\n\n\nShades closer to grey indicate areas with relative low unemployment rates in both years, while shades closer to blue represent areas with high unemployment rates in both years. Mixed tones suggest areas where unemployment rates have changed between 2011 and 2021, with the specific colour intensity reflecting the degree and direction of this change.\nWe have set show.legend = FALSE to allow us to manually add our own bivariate legend. The palette and dimensions should align with those used in bi_class() for dimensions and bi_scale_fill() for both dimensions and palette to ensure consistency. We can create a legend and combine it with a map object as follows:\n\n\n\nR code\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0, 0, .3, 0.3)\n\n\n\n\n\nFigure 2: Bivariate map of relative changes in unemployment rates in London 2011-2021.\n\n\n\n\n\n\n\n\n\n\nThe values in the draw_plot() function specify the relative location and size of each map object on the canvas. Adjusting these values often requires some trial and error to achieve the desired positioning, as they control the x and y coordinates for placement and the width and height proportions of each object.\n\n\n\nWe have used LSOA data to create a bivariate map illustrating changes in unemployment rates. However, with nearly 5,000 LSOAs in London, this map can be challenging to interpret due to the high level of detail. Let’s zoom in to Lambeth:\n\n\n\nR code\n\n# select lambeth\nlsoa21_lambeth &lt;- lsoa21_sf |&gt;\n  filter(str_detect(lsoa21nm, \"Lambeth\"))\n\n# add classes\nlsoa21_lambeth &lt;- lsoa21_lambeth |&gt;\n  bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_lambeth,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nbivmap &lt;- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n# plot\nbivmap\n\n\n\n\n\nFigure 3: Bivariate map of relative changes in unemployment rates in Lambeth 2011-2021.\n\n\n\n\n\n\n\nSo we have now created a map of Lambeth. But what if we need to create a map for every borough in London? In R, you can create a basic function using the function() keyword, which allows you to encapsulate reusable code. A function can take arguments (inputs), perform operations, and return a result. A simple example of a function that adds two values together:\n\n\n\nR code\n\n# define function to add two numbers\nadd_numbers &lt;- function(a, b) {\n    result &lt;- a + b\n    return(result)\n}\n\n# use function\nadd_numbers(5, 3)\n\n\n[1] 8\n\n\nWe can use the same logic to construct a basic function that takes a spatial dataframe and the name of a borough as input and subsequently creates a bivarate map as output:\n\n\n\nR code\n\n# define function to create bivariate unemployment maps\ncreate_bivariate_map &lt;- function(spatial_df, borough_name) {\n  # select borough\n  spatial_df_filter &lt;- spatial_df |&gt;\n    filter(str_detect(lsoa21nm, borough_name))\n\n  # add classes\n  spatial_df_filter &lt;- spatial_df_filter |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n  # bivariate map object\n  map &lt;- ggplot() +\n    geom_sf(\n      data = spatial_df_filter,\n      mapping = aes(fill = bi_class),\n      color = NA,\n      show.legend = FALSE\n    ) +\n    bi_scale_fill(\n      pal = \"DkBlue2\",\n      dim = 3\n    ) +\n    bi_theme()\n\n  # legend object\n  legend &lt;- bi_legend(\n    pal = \"DkBlue2\",\n    dim = 3,\n    xlab = \"Higher Unemployment 2021\",\n    ylab = \"Higher Unemployment 2011\",\n    size = 6\n  )\n\n  # combine\n  bivariate_map &lt;- ggdraw() +\n    draw_plot(map, 0, 0, 1, 1) +\n    draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n  # return value\n  return(bivariate_map)\n}\n\n\nWe can now use this function to quickly recreate maps for individual boroughs. Let’s try it for the London Borough of Hammersmith:\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Hammersmith\")\n\n\n\n\n\nFigure 4: Bivariate map of relative changes in unemployment rates in Hammersmith 2011-2021.\n\n\n\n\nWhat about Kensington and Chelsea? Or Wandsworth?\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Kensington and Chelsea\")\n\n\n\n\n\nFigure 5: Bivariate map of relative changes in unemployment rates in Kensington and Chelsea 2011-2021.\n\n\n\n\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Wandsworth\")\n\n\n\n\n\nFigure 6: Bivariate map of relative changes in unemployment rates in Wandsworth 2011-2021."
  },
  {
    "objectID": "09-maps.html#assignment",
    "href": "09-maps.html#assignment",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "When the same action, such as mapping a particular variable, needs to be repeated across different datasets or regions, a function ensures that the process is consistent and can be applied easily without rewriting code. This not only saves time but also reduces the risk of errors, as you can simply call the function with different inputs, ensuring the same analysis steps are followed each time.\nHaving created a function to generate bivariate maps, we can now create a function for univariate maps. Using the dataset from the tutorial, try to:\n\nWrite a function with two parameters that uses the standard tmap library to map unemployment rates at the LSOA-level for a specified borough.\nAdd a third parameter that specifies which variable should be mapped (e.g., unemployment rates in 2011 or 2021).\nAdd a fourth parameter to define the colour palette to be used for the map.\n\n\n\n\n\n\n\nIf you would like a more comprehensive introduction to writing your own functions in R, refer to Chapter 19: Functions in R for Data Science. This chapter provides a detailed explanation of how to create and use functions, along with best practices for making your code more efficient and reusable."
  },
  {
    "objectID": "09-maps.html#before-you-leave",
    "href": "09-maps.html#before-you-leave",
    "title": "1 Bivariate Maps",
    "section": "",
    "text": "That is it for today. You should now be able to use lookup tables, create bivariate maps with the ggplot2 library, and build basic reproducible functions. Next week, we will dive deeper into the ggplot2 library, but for now that is this week’s Geocompuation done!"
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis and data visualisation while providing a thorough introduction to reproducible research. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, cleaning, analysing and presenting spatial, demographic, and socioeconomic datasets.\n\n\n\nPlease be aware that for this module you are expected to have access to a working R v4.4 installation and have a basic level of proficiency in programming with R. This includes skills such as installing libraries, loading data, calculating variables, and reshaping data. For installation instructions and a refresher, please refer to the Getting started and R for Data Analysis tutorials in the GEOG0018: Methods in Human Geography workbook.\n\n\n\nMoodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial.\n\n\n\nThe topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nR for Spatial Analysis\n\n\n2\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n3\nCore Spatial Analysis\nPoint Pattern Analysis\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nBivariate Maps\n\n\n10\nData Visualisation\nComplex Visualisations\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2024-2025 academic year. The content for 2023-2024 has been archived and can be found here: [Link]\n\n\n\n\n\n\nFor specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook into the Academic Support and Feedback hours.\n\n\n\n\n\n\n\n\n\n\nThis year’s module material features the following major updates:\n\nSecond full rewrite of the workbook using Quarto.\nImproved alignment with GEOG0018 Methods in Human Geography.\nNew material covering geographically weighted regression, geodemographic classification, and data visualisation.\nFully updated material on spatial queries and geometric operations, point pattern analysis, spatial autocorrelation, and accessibility analysis.\nIntroduction of package management using renv.\n\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis and data visualisation while providing a thorough introduction to reproducible research. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, cleaning, analysing and presenting spatial, demographic, and socioeconomic datasets."
  },
  {
    "objectID": "00-index.html#prerequisites",
    "href": "00-index.html#prerequisites",
    "title": "Geocomputation",
    "section": "",
    "text": "Please be aware that for this module you are expected to have access to a working R v4.4 installation and have a basic level of proficiency in programming with R. This includes skills such as installing libraries, loading data, calculating variables, and reshaping data. For installation instructions and a refresher, please refer to the Getting started and R for Data Analysis tutorials in the GEOG0018: Methods in Human Geography workbook."
  },
  {
    "objectID": "00-index.html#moodle",
    "href": "00-index.html#moodle",
    "title": "Geocomputation",
    "section": "",
    "text": "Moodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial."
  },
  {
    "objectID": "00-index.html#module-overview",
    "href": "00-index.html#module-overview",
    "title": "Geocomputation",
    "section": "",
    "text": "The topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nR for Spatial Analysis\n\n\n2\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n3\nCore Spatial Analysis\nPoint Pattern Analysis\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nBivariate Maps\n\n\n10\nData Visualisation\nComplex Visualisations\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2024-2025 academic year. The content for 2023-2024 has been archived and can be found here: [Link]"
  },
  {
    "objectID": "00-index.html#troubleshooting",
    "href": "00-index.html#troubleshooting",
    "title": "Geocomputation",
    "section": "",
    "text": "For specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook into the Academic Support and Feedback hours."
  },
  {
    "objectID": "00-index.html#major-updates",
    "href": "00-index.html#major-updates",
    "title": "Geocomputation",
    "section": "",
    "text": "This year’s module material features the following major updates:\n\nSecond full rewrite of the workbook using Quarto.\nImproved alignment with GEOG0018 Methods in Human Geography.\nNew material covering geographically weighted regression, geodemographic classification, and data visualisation.\nFully updated material on spatial queries and geometric operations, point pattern analysis, spatial autocorrelation, and accessibility analysis.\nIntroduction of package management using renv."
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "Geocomputation",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "11-data.html",
    "href": "11-data.html",
    "title": "1 Data Sources",
    "section": "",
    "text": "Below is a list of resources that you may find helpful when sourcing data for your coursework or dissertation. This list is not exhaustive but includes some recommended websites to get you started.\n\n\nThe following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop\n\n\n\n\nUndergraduate students can also apply for Safeguarded datasets held by the Consumer Data Research Centre. Accessing these datasets requires following a specific process, which is outlined on the CDRC website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nSince the application process for Safeguarded CDRC datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  },
  {
    "objectID": "11-data.html#open-data",
    "href": "11-data.html#open-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "The following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop"
  },
  {
    "objectID": "11-data.html#safeguarded-data",
    "href": "11-data.html#safeguarded-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "Undergraduate students can also apply for Safeguarded datasets held by the Consumer Data Research Centre. Accessing these datasets requires following a specific process, which is outlined on the CDRC website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nSince the application process for Safeguarded CDRC datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  }
]