[
  {
    "objectID": "11-data.html",
    "href": "11-data.html",
    "title": "1 Data Sources",
    "section": "",
    "text": "Below is a list of resources that you may find helpful when sourcing data for your coursework or dissertation. This list is not exhaustive but includes some recommended websites to get you started.\n\n\nThe following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop\n\n\n\n\nUndergraduate students can also apply for Safeguarded datasets held by the Geographic Data Service. Accessing these datasets requires following a specific process, which is outlined on the GeoDS website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the GeoDS that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nModelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSalad Money Daily Transaction Volumes and Values\n\n\n\n\n\n\n\nSince the application process for Safeguarded GeoDS datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  },
  {
    "objectID": "11-data.html#open-data",
    "href": "11-data.html#open-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "The following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGeolytix Supermarket Retail Points\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nOverture Point of Interest data for the United Kingdom\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop"
  },
  {
    "objectID": "11-data.html#safeguarded-data",
    "href": "11-data.html#safeguarded-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "Undergraduate students can also apply for Safeguarded datasets held by the Geographic Data Service. Accessing these datasets requires following a specific process, which is outlined on the GeoDS website. When applying, you will need to explain why you require the specific dataset and describe how you intend to use it. Additionally, consider the ethical implications of using the data, as this will be an important part of your application. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nSome of the datasets held by the GeoDS that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nModelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSalad Money Daily Transaction Volumes and Values\n\n\n\n\n\n\n\nSince the application process for Safeguarded GeoDS datasets can take several weeks, these datasets may be more suitable for your undergraduate dissertation rather than the GEOG0030 coursework assignment. However, CDRC datasets labeled as Open Data do not require an application process. You can download these datasets directly after registering on the website."
  },
  {
    "objectID": "02-operations.html",
    "href": "02-operations.html",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "This week, we look at geometric operations and spatial queries: the fundamental building blocks when it comes to spatial data processing and analysis. This includes operations such as aggregating point data, calculating the distances separating one or more spatial objects, running a buffer analysis, and intersecting different spatial layers.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 13: Spatial Data Analysis, pp. 290-318. [Link]\n\n\n\n\n\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]\n\n\n\n\n\nThis week, we will examine to what extent reported bicycle theft in London cluster around train and underground stations. We will be using open data from data.police.uk on reported crimes alongside OpenStreetMap data for this analysis. We will use R to directly download the necessary data from OpenStreetMap, but the crime data will need to be manually downloaded from the data portal. We further have access to a GeoPackage that contains the London 2021 MSOA boundaries that we can use as reference layer. If you do not already have it on your computer, save this file in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\nThe UK Police Data Portal allows you to access and generate tabular data for crime recorded in the UK across the different police forces. To download recorded crime data for London:\n\nNavigate to data.police.uk and click on Downloads.\nUnder the data range select January 2023 to December 2023.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave the other settings unchanged and click on Generate file.\n\n\n\n\n\n\nFigure 1: Downloading data on reported crimes through data.police.uk\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the dataset.\nAfter downloading, unzip the file. You will find that the zip file contains 12 folders, one for each month of 2023. Each folder includes two files: one for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named London-Crime within your data/attributes directory, and copy all 12 folders with the data into this new folder.\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w02-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAlthough we could read each individual crime file into R one by one and then combine them, we can actually accomplish this in a single step:\n\n\n\nR code\n\n# list all csv files\ncrime_df &lt;- list.files(path = \"data/attributes/London-Crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # read individual csv files\n  lapply(read_csv) |&gt;\n  # bind together into one\n  bind_rows()\n\n# inspect\nhead(crime_df)\n\n\n# A tibble: 6 × 12\n  `Crime ID`      Month `Reported by` `Falls within` Longitude Latitude Location\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 4a14d4745da0a2… 2023… City of Lond… City of Londo…    -0.106     51.5 On or n…\n2 e6e32581c99c5b… 2023… City of Lond… City of Londo…    -0.107     51.5 On or n…\n3 7b7cb8e7debe8b… 2023… City of Lond… City of Londo…    -0.110     51.5 On or n…\n4 f7fc44e1e76332… 2023… City of Lond… City of Londo…    -0.108     51.5 On or n…\n5 8083dafd1770af… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n6 4587239a45f0e8… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n# ℹ 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime type` &lt;chr&gt;,\n#   `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;\n\n\n\n\n\n\n\n\nDepending on your computer, processing this data may take some time due to the large volume involved. Once completed, you should have a dataframe containing 1,144,329 observations.\n\n\n\n\n\n\n\n\n\nYou can further inspect the object using the View() function.\n\n\n\nThe column names contain spaces and are therefore not easily referenced. We can easily clean this up using the janitor package:\n\n\n\nR code\n\n# clean names\ncrime_df &lt;- crime_df |&gt;\n    clean_names()\n\n# inspect\nnames(crime_df)\n\n\n [1] \"crime_id\"              \"month\"                 \"reported_by\"          \n [4] \"falls_within\"          \"longitude\"             \"latitude\"             \n [7] \"location\"              \"lsoa_code\"             \"lsoa_name\"            \n[10] \"crime_type\"            \"last_outcome_category\" \"context\"              \n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nFor our analysis, we are currently only interested in reported bicycle thefts, so we need to filter our data based on the crime_type column. We can start by examining the unique values in this column and then subset the data accordingly:\n\n\n\nR code\n\n# unique types\nunique(crime_df$crime_type)\n\n\n [1] \"Other theft\"                  \"Other crime\"                 \n [3] \"Theft from the person\"        \"Public order\"                \n [5] \"Anti-social behaviour\"        \"Burglary\"                    \n [7] \"Criminal damage and arson\"    \"Drugs\"                       \n [9] \"Shoplifting\"                  \"Vehicle crime\"               \n[11] \"Violence and sexual offences\" \"Bicycle theft\"               \n[13] \"Robbery\"                      \"Possession of weapons\"       \n\n# filter\ntheft_bike &lt;- crime_df |&gt;\n    filter(crime_type == \"Bicycle theft\")\n\n# inspect\nhead(theft_bike)\n\n# A tibble: 6 × 12\n  crime_id  month reported_by falls_within longitude latitude location lsoa_code\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1 62b0f525… 2023… City of Lo… City of Lon…   -0.0916     51.5 On or n… E01000002\n2 9a078d63… 2023… City of Lo… City of Lon…   -0.0952     51.5 On or n… E01032739\n3 f175a32e… 2023… City of Lo… City of Lon…   -0.0872     51.5 On or n… E01032739\n4 137ec120… 2023… City of Lo… City of Lon…   -0.0783     51.5 On or n… E01032739\n5 4c3b4677… 2023… City of Lo… City of Lon…   -0.108      51.5 On or n… E01032740\n6 13b5eb5c… 2023… City of Lo… City of Lon…   -0.0980     51.5 On or n… E01032740\n# ℹ 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;,\n#   last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;\n\n\nNow that we have filtered the data to only include reported bicycle thefts, we need to convert our dataframe into a spatial dataframe that maps the locations of the crimes using the recorded latitude and longitude coordinates. We can then project this spatial dataframe into the British National Grid (EPSG:27700).\n\n\n\nR code\n\n# to spatial data\ntheft_bike &lt;- theft_bike |&gt;\n    filter(!is.na(longitude) & !is.na(latitude)) |&gt;\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n    st_transform(27700)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531388 ymin: 180914 xmax: 533447 ymax: 181727.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 4 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# london outline\noutline &lt;- msoa21 |&gt;\n  st_union()\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    size = 0.10,\n    fill = \"#fdc086\",\n    col = \"#fdc086\",\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\nFigure 2: Reported bicycle thefts in London.\n\n\n\n\nWe can save the prepared dataset as a GeoPackage so that we can use it some other time:\n\n\n\nR code\n\n# write\nst_write(theft_bike, \"data/spatial/London-BicycleTheft-2023.gpkg\")\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world. Each map element (whether a point, line, or polygon) in OSM is tagged with various attribute data. To download the station data we need, we must use the appropriate tags, represented as key and value pairs, to query the OSM database. In our case, we are looking for train stations, which fall under the Public Transport key, with a value of station. To limit our search to London, we can use the spatial extent of the 2021 MSOA boundaries as the bounding box for data extraction.\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_london &lt;- msoa21 |&gt;\n  st_transform(4326) |&gt;\n  st_bbox()\n\n# osm query\nosm_stations &lt;- opq(bbox = bbox_london) |&gt;\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Stations.RData')\n\n\n\nThe OSM query returns all data types, including lines and polygons tagged as stations. For our analysis, we only want to retain the point locations. In addition, we want to clip the results to the outline of London to exclude points that fall within the bounding box but outside the boundaries of Greater London.\n\n\n\nR code\n\n# extract points\nosm_stations &lt;- osm_stations$osm_points |&gt;\n    st_set_crs(4326) |&gt;\n    st_transform(27700) |&gt;\n    st_intersection(outline) |&gt;\n    select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nhead(osm_stations)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 506148.8 ymin: 168292.6 xmax: 546593.8 ymax: 191714.7\nProjected CRS: OSGB36 / British National Grid\n           osm_id                   name                      network\n780856     780856 Shepherd's Bush Market           London Underground\n1256794   1256794           West Drayton National Rail;Elizabeth line\n2013971   2013971       Finchley Central           London Underground\n9780241   9780241           St Mary Cray                National Rail\n13330343 13330343               Woodford           London Underground\n13790683 13790683               Mile End           London Underground\n                   operator public_transport railway                  geometry\n780856   London Underground          station station POINT (523195.9 180061.9)\n1256794      Elizabeth line          station station POINT (506148.8 180085.4)\n2013971  London Underground          station station   POINT (525294.7 190658)\n9780241        Southeastern          station station POINT (546593.8 168292.6)\n13330343 London Underground    stop_position    stop POINT (540946.1 191714.7)\n13790683 London Underground    stop_position    stop   POINT (536524.6 182545)\n\n# inspect\nnrow(osm_stations)\n\n[1] 4056\n\n\nThe total number of data points seems rather high. In fact, looking at the railway variable, several points are not tagged as station or do not have a value at all:\n\n\n\nR code\n\n# inspect values\ncount(osm_stations, railway)\n\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 504982.3 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n                 railway    n                       geometry\n1                station  614 MULTIPOINT ((505078.2 17673...\n2                   stop   17 MULTIPOINT ((528197.7 18571...\n3        subway_entrance   38 MULTIPOINT ((525226.4 18745...\n4 train_station_entrance    1      POINT (538196.2 184826.5)\n5                   &lt;NA&gt; 3386 MULTIPOINT ((504982.3 17581...\n\n\nThe number of points tagged as station in the railway field are most likely the only points in our dataset that represent actual stations, so we will only retain those points.\n\n\n\nR code\n\n# extract train and underground stations\nosm_stations &lt;- osm_stations |&gt;\n    filter(railway == \"station\")\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(osm_stations) +\n\n  # map data\n  tm_symbols(\n    size = 0.2,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 3: Train and underground stations in London.\n\n\n\n\nNow we have our data prepared, we can move on to analyse the extent to which bicycle theft in London cluster around stations. We can use both spatial queries and geometric operations to complete this analysis.\n\n\n\n\n\n\nWhen using osmdata to retrieve train stations in London, be aware that OpenStreetMap data may be incomplete or inconsistent due to different tagging practices by contributors, missing or outdated entries, and variations in how stations are classified. While verifying every detail is beyond the scope of this practical, it is essential to exercise due diligence when working with real-world data. Where possible triangulate with other authoritative sources such as official transport datasets and at the very least perform sanity checks to identify obvious anomalies.\n\n\n\n\n\n\nA spatial query is used to retrieve data based on its geographic location or spatial relationships. It uses spatial information from one or more layers to find features that meet specific criteria, such as proximity, intersection, or containment. For instance, we can use a spatial query to count all the bicycle thefts that have occurred within 500 metres of a train or underground station:\n\n\n\nR code\n\n# create a single station geometry\nosm_stations_comb &lt;- osm_stations |&gt;\n    st_union()\n\n# spatial query\ntheft_bike$d500 &lt;- theft_bike |&gt;\n    st_is_within_distance(osm_stations_comb, dist = 500, sparse = FALSE)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531388 ymin: 180914 xmax: 533447 ymax: 181727.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 12\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 5 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;, d500 &lt;lgl[,1]&gt;\n\n\n\n\n\n\n\n\nThe above code converts the stations dataframe into a single geometry. This step is essential for sf to ensure that each point in the dataset is compared to every point in the stations dataframe. Without this conversion, the comparison would be done one station point at a time, storing only the last result rather than considering all station points simultaneously.\n\n\n\nWe can use the count() function to find out just how many thefts fall in each of these categories:\n\n\n\nR code\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384131 ymin: 101512 xmax: 612925 ymax: 398061.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500[,1]     n                                                        geometry\n* &lt;lgl&gt;    &lt;int&gt;                                                &lt;MULTIPOINT [m]&gt;\n1 FALSE     5485 ((384131 398061.1), (409823 101512), (494953 212260), (495917 …\n2 TRUE     10534 ((505376 184282.9), (505424 184212), (505433 184416), (505558 …\n\n\nMore than two-thirds of all reported bicycle thefts in London occur within 500 metres of a train or underground station. Of course, we can map the results for a visual inspection:\n\n\n\nR code\n\n# classify distances\ntheft_bike &lt;- theft_bike |&gt;\n  mutate(dist_class = if_else(d500 == TRUE, \"&gt; 500 m\", \"&lt; 500 m\")) |&gt;\n  mutate(dist_class = factor(dist_class, levels = c(\"&gt; 500 m\", \"&lt; 500 m\")))\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    # map data\n    col = \"dist_class\",\n    size = 0.1,\n    col.scale = tm_scale_categorical(\n      values = c(\n        \"&gt; 500 m\" = \"#fdc086\",\n        \"&lt; 500 m\" = \"#998ec3\"\n      ),\n    ),\n\n    # legend\n    col.legend = tm_legend(\n      title = \"\",\n      frame = FALSE,\n    )\n  ) +\n\n  # shape\n  tm_shape(osm_stations) +\n\n  # map data\n  tm_symbols(\n    size = 0.2,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # legend\n  tm_add_legend(\n    type = \"symbols\",\n    labels = \"Station\",\n    size = 0.2,\n    fill = \"#636363\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.0, 0.2),\n    legend.text.size = 0.8,\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 4: Reported bicycle thefts in London within 500 metres from a train or underground station.\n\n\n\n\n\n\n\nGeometric operations are used to manipulate and analyse the shapes and spatial properties of geometric objects, such as points, lines, and polygons. These operations include tasks like calculating intersections, buffering, and determining the distance between shapes. In this case, we can create 500-metre buffers around each station and then count how many bicycle thefts fall within these buffers.\n\n\n\nR code\n\n# buffer\nosm_stations_buffer &lt;- osm_stations |&gt;\n    st_buffer(dist = 500) |&gt;\n    st_union()\n\n# inspect\nhead(osm_stations_buffer)\n\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 504578.2 ymin: 158527.2 xmax: 556685.7 ymax: 200638.6\nProjected CRS: OSGB36 / British National Grid\n\n\nMULTIPOLYGON (((517709.6 169311.1, 517686.6 169...\n\n\n\n\n\n\n\n\nWhen performing buffer analysis, the buffer sizes are determined by the units of the coordinate reference system (CRS) used. For instance, with the British National Grid, where the CRS is in metres, the buffer distance must be specified in metres.\n\n\n\nWe can map the results for a visual inspection:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(osm_stations_buffer) +\n\n  # specify colours\n  tm_polygons(\n    fill = \"#beaed4\",\n  ) +\n\n  # layout\n  tm_layout(\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Train and underground stations in London with a 500 metres buffer.\n\n\n\n\nWe can now use the st_intersects function to find out which reported bicycle thefts have occurred within 500 metres of a train or underground station.\n\n\n\nR code\n\n# intersect buffer with bicycle thefts\ntheft_bike$d500_buffer &lt;- theft_bike |&gt;\n    st_intersects(osm_stations_buffer, sparse = FALSE)\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500_buffer)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384131 ymin: 101512 xmax: 612925 ymax: 398061.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500_buffer[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE            5491 ((384131 398061.1), (409823 101512), (494953 212260), (…\n2 TRUE            10528 ((505376 184282.9), (505424 184212), (505433 184416), (…\n\n\n\n\n\n\n\n\nThe results are almost identical, with a small difference due to how the two methods define within and handle spatial relationships and boundaries. For instance, a point on the buffer’s edge will be included in the intersect method, but may not meet the distance threshold required by st_within_distance().\n\n\n\n\n\n\n\nNow that we are familiar with basic spatial queries and geometric operations, we can conduct a similar analysis on the number of serious and fatal road crashed in London in 2022 and determine how many occurred on or near a main road. Try to do the following:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of road crashes that occurred in London in 2022, extracted from the UK’s official road traffic casualty database using the stats19 R library.\nA GeoPackage file that contains main roads in London, extracted from the Ordnance Survey Open Roads dataset.\n\nCalculate the number of serious and fatal road crashes that occurred within 100 metres and 500 metres of a main road.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\nLondon OS Open Roads - Main Roads\nGeoPackage\nDownload\n\n\n\n\n\n\nBoom. That is how you can conduct basic spatial queries and geometric operations and using R and sf. Yet more RGIS coming over the next couple of weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "02-operations.html#lecture-slides",
    "href": "02-operations.html#lecture-slides",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "02-operations.html#reading-list",
    "href": "02-operations.html#reading-list",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 13: Spatial Data Analysis, pp. 290-318. [Link]\n\n\n\n\n\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]"
  },
  {
    "objectID": "02-operations.html#bike-theft-in-london-i",
    "href": "02-operations.html#bike-theft-in-london-i",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "This week, we will examine to what extent reported bicycle theft in London cluster around train and underground stations. We will be using open data from data.police.uk on reported crimes alongside OpenStreetMap data for this analysis. We will use R to directly download the necessary data from OpenStreetMap, but the crime data will need to be manually downloaded from the data portal. We further have access to a GeoPackage that contains the London 2021 MSOA boundaries that we can use as reference layer. If you do not already have it on your computer, save this file in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\nThe UK Police Data Portal allows you to access and generate tabular data for crime recorded in the UK across the different police forces. To download recorded crime data for London:\n\nNavigate to data.police.uk and click on Downloads.\nUnder the data range select January 2023 to December 2023.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave the other settings unchanged and click on Generate file.\n\n\n\n\n\n\nFigure 1: Downloading data on reported crimes through data.police.uk\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the dataset.\nAfter downloading, unzip the file. You will find that the zip file contains 12 folders, one for each month of 2023. Each folder includes two files: one for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named London-Crime within your data/attributes directory, and copy all 12 folders with the data into this new folder.\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w02-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAlthough we could read each individual crime file into R one by one and then combine them, we can actually accomplish this in a single step:\n\n\n\nR code\n\n# list all csv files\ncrime_df &lt;- list.files(path = \"data/attributes/London-Crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # read individual csv files\n  lapply(read_csv) |&gt;\n  # bind together into one\n  bind_rows()\n\n# inspect\nhead(crime_df)\n\n\n# A tibble: 6 × 12\n  `Crime ID`      Month `Reported by` `Falls within` Longitude Latitude Location\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 4a14d4745da0a2… 2023… City of Lond… City of Londo…    -0.106     51.5 On or n…\n2 e6e32581c99c5b… 2023… City of Lond… City of Londo…    -0.107     51.5 On or n…\n3 7b7cb8e7debe8b… 2023… City of Lond… City of Londo…    -0.110     51.5 On or n…\n4 f7fc44e1e76332… 2023… City of Lond… City of Londo…    -0.108     51.5 On or n…\n5 8083dafd1770af… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n6 4587239a45f0e8… 2023… City of Lond… City of Londo…    -0.112     51.5 On or n…\n# ℹ 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime type` &lt;chr&gt;,\n#   `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;\n\n\n\n\n\n\n\n\nDepending on your computer, processing this data may take some time due to the large volume involved. Once completed, you should have a dataframe containing 1,144,329 observations.\n\n\n\n\n\n\n\n\n\nYou can further inspect the object using the View() function.\n\n\n\nThe column names contain spaces and are therefore not easily referenced. We can easily clean this up using the janitor package:\n\n\n\nR code\n\n# clean names\ncrime_df &lt;- crime_df |&gt;\n    clean_names()\n\n# inspect\nnames(crime_df)\n\n\n [1] \"crime_id\"              \"month\"                 \"reported_by\"          \n [4] \"falls_within\"          \"longitude\"             \"latitude\"             \n [7] \"location\"              \"lsoa_code\"             \"lsoa_name\"            \n[10] \"crime_type\"            \"last_outcome_category\" \"context\"              \n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nFor our analysis, we are currently only interested in reported bicycle thefts, so we need to filter our data based on the crime_type column. We can start by examining the unique values in this column and then subset the data accordingly:\n\n\n\nR code\n\n# unique types\nunique(crime_df$crime_type)\n\n\n [1] \"Other theft\"                  \"Other crime\"                 \n [3] \"Theft from the person\"        \"Public order\"                \n [5] \"Anti-social behaviour\"        \"Burglary\"                    \n [7] \"Criminal damage and arson\"    \"Drugs\"                       \n [9] \"Shoplifting\"                  \"Vehicle crime\"               \n[11] \"Violence and sexual offences\" \"Bicycle theft\"               \n[13] \"Robbery\"                      \"Possession of weapons\"       \n\n# filter\ntheft_bike &lt;- crime_df |&gt;\n    filter(crime_type == \"Bicycle theft\")\n\n# inspect\nhead(theft_bike)\n\n# A tibble: 6 × 12\n  crime_id  month reported_by falls_within longitude latitude location lsoa_code\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1 62b0f525… 2023… City of Lo… City of Lon…   -0.0916     51.5 On or n… E01000002\n2 9a078d63… 2023… City of Lo… City of Lon…   -0.0952     51.5 On or n… E01032739\n3 f175a32e… 2023… City of Lo… City of Lon…   -0.0872     51.5 On or n… E01032739\n4 137ec120… 2023… City of Lo… City of Lon…   -0.0783     51.5 On or n… E01032739\n5 4c3b4677… 2023… City of Lo… City of Lon…   -0.108      51.5 On or n… E01032740\n6 13b5eb5c… 2023… City of Lo… City of Lon…   -0.0980     51.5 On or n… E01032740\n# ℹ 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;,\n#   last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;\n\n\nNow that we have filtered the data to only include reported bicycle thefts, we need to convert our dataframe into a spatial dataframe that maps the locations of the crimes using the recorded latitude and longitude coordinates. We can then project this spatial dataframe into the British National Grid (EPSG:27700).\n\n\n\nR code\n\n# to spatial data\ntheft_bike &lt;- theft_bike |&gt;\n    filter(!is.na(longitude) & !is.na(latitude)) |&gt;\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n    st_transform(27700)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531388 ymin: 180914 xmax: 533447 ymax: 181727.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 4 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# london outline\noutline &lt;- msoa21 |&gt;\n  st_union()\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    size = 0.10,\n    fill = \"#fdc086\",\n    col = \"#fdc086\",\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\nFigure 2: Reported bicycle thefts in London.\n\n\n\n\nWe can save the prepared dataset as a GeoPackage so that we can use it some other time:\n\n\n\nR code\n\n# write\nst_write(theft_bike, \"data/spatial/London-BicycleTheft-2023.gpkg\")\n\n\n\n\n\nOpenStreetMap (OSM) is a free, editable map of the world. Each map element (whether a point, line, or polygon) in OSM is tagged with various attribute data. To download the station data we need, we must use the appropriate tags, represented as key and value pairs, to query the OSM database. In our case, we are looking for train stations, which fall under the Public Transport key, with a value of station. To limit our search to London, we can use the spatial extent of the 2021 MSOA boundaries as the bounding box for data extraction.\n\n\n\nR code\n\n# define our bbox coordinates, use WGS84\nbbox_london &lt;- msoa21 |&gt;\n  st_transform(4326) |&gt;\n  st_bbox()\n\n# osm query\nosm_stations &lt;- opq(bbox = bbox_london) |&gt;\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some cases, the OSM query may return an error, particularly when multiple users from the same location are executing the exact same query. If so, you can download a prepared copy of the data here: [Download]. You can load this copy into R through load('data/spatial/London-OSM-Stations.RData')\n\n\n\nThe OSM query returns all data types, including lines and polygons tagged as stations. For our analysis, we only want to retain the point locations. In addition, we want to clip the results to the outline of London to exclude points that fall within the bounding box but outside the boundaries of Greater London.\n\n\n\nR code\n\n# extract points\nosm_stations &lt;- osm_stations$osm_points |&gt;\n    st_set_crs(4326) |&gt;\n    st_transform(27700) |&gt;\n    st_intersection(outline) |&gt;\n    select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# inspect\nhead(osm_stations)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 506148.8 ymin: 168292.6 xmax: 546593.8 ymax: 191714.7\nProjected CRS: OSGB36 / British National Grid\n           osm_id                   name                      network\n780856     780856 Shepherd's Bush Market           London Underground\n1256794   1256794           West Drayton National Rail;Elizabeth line\n2013971   2013971       Finchley Central           London Underground\n9780241   9780241           St Mary Cray                National Rail\n13330343 13330343               Woodford           London Underground\n13790683 13790683               Mile End           London Underground\n                   operator public_transport railway                  geometry\n780856   London Underground          station station POINT (523195.9 180061.9)\n1256794      Elizabeth line          station station POINT (506148.8 180085.4)\n2013971  London Underground          station station   POINT (525294.7 190658)\n9780241        Southeastern          station station POINT (546593.8 168292.6)\n13330343 London Underground    stop_position    stop POINT (540946.1 191714.7)\n13790683 London Underground    stop_position    stop   POINT (536524.6 182545)\n\n# inspect\nnrow(osm_stations)\n\n[1] 4056\n\n\nThe total number of data points seems rather high. In fact, looking at the railway variable, several points are not tagged as station or do not have a value at all:\n\n\n\nR code\n\n# inspect values\ncount(osm_stations, railway)\n\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 504982.3 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n                 railway    n                       geometry\n1                station  614 MULTIPOINT ((505078.2 17673...\n2                   stop   17 MULTIPOINT ((528197.7 18571...\n3        subway_entrance   38 MULTIPOINT ((525226.4 18745...\n4 train_station_entrance    1      POINT (538196.2 184826.5)\n5                   &lt;NA&gt; 3386 MULTIPOINT ((504982.3 17581...\n\n\nThe number of points tagged as station in the railway field are most likely the only points in our dataset that represent actual stations, so we will only retain those points.\n\n\n\nR code\n\n# extract train and underground stations\nosm_stations &lt;- osm_stations |&gt;\n    filter(railway == \"station\")\n\n\nLet’s map the dataset to get an idea of how the data looks like, using the outline of London as background:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(osm_stations) +\n\n  # map data\n  tm_symbols(\n    size = 0.2,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 3: Train and underground stations in London.\n\n\n\n\nNow we have our data prepared, we can move on to analyse the extent to which bicycle theft in London cluster around stations. We can use both spatial queries and geometric operations to complete this analysis.\n\n\n\n\n\n\nWhen using osmdata to retrieve train stations in London, be aware that OpenStreetMap data may be incomplete or inconsistent due to different tagging practices by contributors, missing or outdated entries, and variations in how stations are classified. While verifying every detail is beyond the scope of this practical, it is essential to exercise due diligence when working with real-world data. Where possible triangulate with other authoritative sources such as official transport datasets and at the very least perform sanity checks to identify obvious anomalies.\n\n\n\n\n\n\nA spatial query is used to retrieve data based on its geographic location or spatial relationships. It uses spatial information from one or more layers to find features that meet specific criteria, such as proximity, intersection, or containment. For instance, we can use a spatial query to count all the bicycle thefts that have occurred within 500 metres of a train or underground station:\n\n\n\nR code\n\n# create a single station geometry\nosm_stations_comb &lt;- osm_stations |&gt;\n    st_union()\n\n# spatial query\ntheft_bike$d500 &lt;- theft_bike |&gt;\n    st_is_within_distance(osm_stations_comb, dist = 500, sparse = FALSE)\n\n# inspect\nhead(theft_bike)\n\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531388 ymin: 180914 xmax: 533447 ymax: 181727.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 12\n  crime_id           month reported_by falls_within location lsoa_code lsoa_name\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    \n1 62b0f525fc471c062… 2023… City of Lo… City of Lon… On or n… E01000002 City of …\n2 9a078d630cf67c37c… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n3 f175a32ef7f90c67a… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n4 137ec1201fd64b578… 2023… City of Lo… City of Lon… On or n… E01032739 City of …\n5 4c3b467755a98afa3… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n6 13b5eb5ca0aef09a2… 2023… City of Lo… City of Lon… On or n… E01032740 City of …\n# ℹ 5 more variables: crime_type &lt;chr&gt;, last_outcome_category &lt;chr&gt;,\n#   context &lt;lgl&gt;, geometry &lt;POINT [m]&gt;, d500 &lt;lgl[,1]&gt;\n\n\n\n\n\n\n\n\nThe above code converts the stations dataframe into a single geometry. This step is essential for sf to ensure that each point in the dataset is compared to every point in the stations dataframe. Without this conversion, the comparison would be done one station point at a time, storing only the last result rather than considering all station points simultaneously.\n\n\n\nWe can use the count() function to find out just how many thefts fall in each of these categories:\n\n\n\nR code\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384131 ymin: 101512 xmax: 612925 ymax: 398061.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500[,1]     n                                                        geometry\n* &lt;lgl&gt;    &lt;int&gt;                                                &lt;MULTIPOINT [m]&gt;\n1 FALSE     5485 ((384131 398061.1), (409823 101512), (494953 212260), (495917 …\n2 TRUE     10534 ((505376 184282.9), (505424 184212), (505433 184416), (505558 …\n\n\nMore than two-thirds of all reported bicycle thefts in London occur within 500 metres of a train or underground station. Of course, we can map the results for a visual inspection:\n\n\n\nR code\n\n# classify distances\ntheft_bike &lt;- theft_bike |&gt;\n  mutate(dist_class = if_else(d500 == TRUE, \"&gt; 500 m\", \"&lt; 500 m\")) |&gt;\n  mutate(dist_class = factor(dist_class, levels = c(\"&gt; 500 m\", \"&lt; 500 m\")))\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    # map data\n    col = \"dist_class\",\n    size = 0.1,\n    col.scale = tm_scale_categorical(\n      values = c(\n        \"&gt; 500 m\" = \"#fdc086\",\n        \"&lt; 500 m\" = \"#998ec3\"\n      ),\n    ),\n\n    # legend\n    col.legend = tm_legend(\n      title = \"\",\n      frame = FALSE,\n    )\n  ) +\n\n  # shape\n  tm_shape(osm_stations) +\n\n  # map data\n  tm_symbols(\n    size = 0.2,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # legend\n  tm_add_legend(\n    type = \"symbols\",\n    labels = \"Station\",\n    size = 0.2,\n    fill = \"#636363\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.0, 0.2),\n    legend.text.size = 0.8,\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 4: Reported bicycle thefts in London within 500 metres from a train or underground station.\n\n\n\n\n\n\n\nGeometric operations are used to manipulate and analyse the shapes and spatial properties of geometric objects, such as points, lines, and polygons. These operations include tasks like calculating intersections, buffering, and determining the distance between shapes. In this case, we can create 500-metre buffers around each station and then count how many bicycle thefts fall within these buffers.\n\n\n\nR code\n\n# buffer\nosm_stations_buffer &lt;- osm_stations |&gt;\n    st_buffer(dist = 500) |&gt;\n    st_union()\n\n# inspect\nhead(osm_stations_buffer)\n\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 504578.2 ymin: 158527.2 xmax: 556685.7 ymax: 200638.6\nProjected CRS: OSGB36 / British National Grid\n\n\nMULTIPOLYGON (((517709.6 169311.1, 517686.6 169...\n\n\n\n\n\n\n\n\nWhen performing buffer analysis, the buffer sizes are determined by the units of the coordinate reference system (CRS) used. For instance, with the British National Grid, where the CRS is in metres, the buffer distance must be specified in metres.\n\n\n\nWe can map the results for a visual inspection:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(osm_stations_buffer) +\n\n  # specify colours\n  tm_polygons(\n    fill = \"#beaed4\",\n  ) +\n\n  # layout\n  tm_layout(\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Train and underground stations in London with a 500 metres buffer.\n\n\n\n\nWe can now use the st_intersects function to find out which reported bicycle thefts have occurred within 500 metres of a train or underground station.\n\n\n\nR code\n\n# intersect buffer with bicycle thefts\ntheft_bike$d500_buffer &lt;- theft_bike |&gt;\n    st_intersects(osm_stations_buffer, sparse = FALSE)\n\n# number of bicycle thefts within 500m of a station\ncount(theft_bike, d500_buffer)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 384131 ymin: 101512 xmax: 612925 ymax: 398061.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d500_buffer[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE            5491 ((384131 398061.1), (409823 101512), (494953 212260), (…\n2 TRUE            10528 ((505376 184282.9), (505424 184212), (505433 184416), (…\n\n\n\n\n\n\n\n\nThe results are almost identical, with a small difference due to how the two methods define within and handle spatial relationships and boundaries. For instance, a point on the buffer’s edge will be included in the intersect method, but may not meet the distance threshold required by st_within_distance()."
  },
  {
    "objectID": "02-operations.html#assignment",
    "href": "02-operations.html#assignment",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Now that we are familiar with basic spatial queries and geometric operations, we can conduct a similar analysis on the number of serious and fatal road crashed in London in 2022 and determine how many occurred on or near a main road. Try to do the following:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of road crashes that occurred in London in 2022, extracted from the UK’s official road traffic casualty database using the stats19 R library.\nA GeoPackage file that contains main roads in London, extracted from the Ordnance Survey Open Roads dataset.\n\nCalculate the number of serious and fatal road crashes that occurred within 100 metres and 500 metres of a main road.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\nLondon OS Open Roads - Main Roads\nGeoPackage\nDownload"
  },
  {
    "objectID": "02-operations.html#before-you-leave",
    "href": "02-operations.html#before-you-leave",
    "title": "1 Spatial Queries and Geometric Operations",
    "section": "",
    "text": "Boom. That is how you can conduct basic spatial queries and geometric operations and using R and sf. Yet more RGIS coming over the next couple of weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "10-datavis.html",
    "href": "10-datavis.html",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Most of the visualisations we have created over the past weeks have been maps. However, you will often need to use other types of visualisations for your data, such as histograms, scatterplots, dendrograms, and boxplots. While base R can be used for simple visualisations, it is best suited for quick data inspections. For publication-worthy and more complex visualisations, the ggplot2 library, which we used last week to create bivariate maps, offers a unified and effective approach to data visualisation based on the grammar of graphics.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nWickham, H. 2010. A layered grammar of graphics. Journal of Computational and Graphical Statistics 19(1): 3-28. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2014. London, The Information Capital: 100 Maps & Graphics That Will Change How You View the City. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 3: Data visualisation. [Link]\n\n\n\n\n\nToday, we will use the same dataset that we used in Week 8 on self-identified ethnicity. We will visualise the distribution of the self-identified White-British population across the 12 Inner London Boroughs. The LSOA data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. A copy of the 2021 London LSOAs spatial boundaries is also available. If you do not already have it on your computer, save these file in your data/attribues and data/spatial folders.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w10-ethnicity-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(treemapify)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load the files in the usual fashion:\n\n\n\nR code\n\n# load attribute dataset\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\") |&gt;\n    st_drop_geometry()\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect\nhead(lsoa21)\n\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\nWe will start by pivoting the data and transforming the raw counts into proportions:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2)\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 4,994 × 21\n# Rowwise: \n   lower_layer_super_output_area…¹ asian_asian_british_…² asian_asian_british_…³\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 E01000001                                      0.00271                0.0448 \n 2 E01000002                                      0.00505                0.0736 \n 3 E01000003                                      0.00682                0.0323 \n 4 E01000005                                      0.239                  0.0318 \n 5 E01000006                                      0.116                  0.00596\n 6 E01000007                                      0.113                  0.0148 \n 7 E01000008                                      0.110                  0.00445\n 8 E01000009                                      0.119                  0.0122 \n 9 E01000011                                      0.146                  0.00469\n10 E01000012                                      0.122                  0.00298\n# ℹ 4,984 more rows\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 18 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe column names are rather long, so let’s rename these manually:\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth)[2:20] &lt;- c(\"Asian - Bangladeshi\", \"Asian - Chinese\", \"Asian - Indian\",\n    \"Asian - Pakistani\", \"Asian - Other\", \"Black - African\", \"Black - Caribbean\",\n    \"Black - Other\", \"Mixed - Asian\", \"Mixed - Black African\", \"Mixed - Black Carribean\",\n    \"Mixed - Other\", \"White - British\", \"White - Irish\", \"White - Traveller\", \"White - Roma\",\n    \"White - Other\", \"Arab - Other\", \"Any Other Group\")\n\n\nThe last thing we need to do is extract the LSOAs that fall within the 12 Inner London Boroughs. We can do this by using the LSOA names that are inside the spatial dataframe:\n\n\n\nR code\n\n# boroughs\ninner_boroughs &lt;- c(\"Camden\", \"Greenwich\", \"Hackney\", \"Hammersmith and Fulham\", \"Islington\", \"Kensington and Chelsea\", \"Lambeth\", \"Lewisham\", \"Southwark\", \"Tower Hamlets\", \"Wandsworth\", \"Westminster\")\n\n# filter spatial data\nlsoa21_inner &lt;- lsoa21 |&gt;\n  filter(str_detect(lsoa21nm, paste(inner_boroughs, collapse = \"|\")))\n\n# filter attribute data, add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  filter(lower_layer_super_output_areas_code %in% lsoa21_inner$lsoa21cd)\n\n# add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  left_join(lsoa21[1:2], by = c(\"lower_layer_super_output_areas_code\" = \"lsoa21cd\"))\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 1,792 × 22\n# Rowwise: \n   lower_layer_super_output_areas_code `Asian - Bangladeshi` `Asian - Chinese`\n   &lt;chr&gt;                                               &lt;dbl&gt;             &lt;dbl&gt;\n 1 E01000842                                         0.00209            0.0265\n 2 E01000843                                         0.00167            0.0301\n 3 E01000844                                         0.00614            0.0258\n 4 E01000845                                         0.0193             0.0273\n 5 E01000846                                         0.0586             0.0179\n 6 E01000847                                         0.0506             0.0312\n 7 E01000848                                         0.00269            0.0249\n 8 E01000849                                         0.0125             0.0249\n 9 E01000850                                         0.0218             0.113 \n10 E01000851                                         0.0216             0.0824\n# ℹ 1,782 more rows\n# ℹ 19 more variables: `Asian - Indian` &lt;dbl&gt;, `Asian - Pakistani` &lt;dbl&gt;,\n#   `Asian - Other` &lt;dbl&gt;, `Black - African` &lt;dbl&gt;, `Black - Caribbean` &lt;dbl&gt;,\n#   `Black - Other` &lt;dbl&gt;, `Mixed - Asian` &lt;dbl&gt;,\n#   `Mixed - Black African` &lt;dbl&gt;, `Mixed - Black Carribean` &lt;dbl&gt;,\n#   `Mixed - Other` &lt;dbl&gt;, `White - British` &lt;dbl&gt;, `White - Irish` &lt;dbl&gt;,\n#   `White - Traveller` &lt;dbl&gt;, `White - Roma` &lt;dbl&gt;, `White - Other` &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf you want to know what the paste(inner_boroughs, collapse = '|') code does, you can run it separately in the console to find out.\n\n\n\n\n\nThe ggplot2 library is built on the layered grammar of graphics, which provides a structured approach to creating visualisations. This means that plots are constructed by adding layers, such as data, aesthetic mappings (e.g., axes, colours, sizes), geometric shapes (e.g., points, lines, bars), and optional elements like themes or statistical transformations. This modular design allows users to build complex and customisable plots step by step, ensuring flexibility and clarity in the visualisation process.\nLet’s try to use this approach by making a boxplot on the distribution of people that self-identify as White British across all Inner London Boroughs. With ggplot2, every plot begins with the ggplot() function, which creates a coordinate system to which layers can be added. The first argument of ggplot() specifies the dataset to use:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth)\n\n\nTo build your graph, you add one or more layers to ggplot(). For instance, geom_point() adds a layer of points to create a scatterplot. ggplot2 provides many geom functions, each adding a different type of layer to your plot. To create a boxplot, you add the geom_boxplot() layer. For boxplots, the mapping argument defines how dataset variables are linked to visual properties, such as the grouping or value axes. The mapping is paired with aes(), where y specifies the numeric variable:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot()\n\n\n\n\n\nFigure 1: Basic boxplot using ggplot2.\n\n\n\n\n\n\n\n\n\n\nAn aesthetic is a visual property of the elements in your plot. Aesthetics include attributes like size, shape, or colour of points. By modifying the values of these aesthetic properties, you can display a point in various ways, allowing for greater customisation and clarity in your visualisation.\n\n\n\nJust like with tmap, we can customise the basic plot by styling the boxplot, adding labels, and adjusting its overall appearancs:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 2: Stylised boxplot using ggplot2.\n\n\n\n\nBut what if we wwant to create a boxplot for all Inner London Boroughs? We can do this by adding a grouping variable:\n\n\n\nR code\n\n# add borough names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name = substr(lsoa21nm, 1, nchar(lsoa21nm) - 5))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    utlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\nWarning in geom_boxplot(fill = \"#f0f0f0\", color = \"#252525\", outlier.color =\n\"#ef3b2c\", : Ignoring unknown parameters: `utlier.shape`\n\n\n\n\n\nFigure 3: Stylised boxplot using ggplot2.\n\n\n\n\nThe boroughs are drawn in alphabetical order by default. To change this we need to adjust the order by creating a factor. For instance, we can sort the boroughs by their median values.\n\n\n\n\n\n\nIn R, a factor is a data structure used to represent categorical variables with a specific order or grouping. Factors allow you to define and manipulate the order of categories, which is especially useful for plotting or analysis.\n\n\n\n\n\n\nR code\n\n# median values\nlsoa_med &lt;- lsoa_eth |&gt;\n  group_by(borough_name) |&gt;\n  summarise(median = median(`White - British`))\n\n# create factor\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name_factor = factor(borough_name, levels = lsoa_med$borough_name[order(lsoa_med$median, decreasing = TRUE)]))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name_factor)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 4: Stylised boxplot using ggplot2.\n\n\n\n\n\n\n\nBoxplots are effective for visualising distributions, but histograms offer another way to explore the same data by showing the frequency of values. While histograms cannot be displayed alongside boxplots in the same image, we can create a series of histograms, each displayed in a separate panel. These panels can show the distributions for different groups, such as individual boroughs.\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_histogram() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Number of LSOAs\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\nFigure 5: Histograms presented in individual panels.\n\n\n\n\nWe could use the same approach to create a series of scatterplots to show the relationship between two variables:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = `White - Other`)) +\n  # add geometry\n  geom_point() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n\n\n\nFigure 6: Scatterplots presented in individual panels.\n\n\n\n\n\n\n\n\n\n\nTo export a ggplot, first assign your plot to an object. Then, use the ggsave() function to save the plot to a file, specifying the desired filename and format (e.g. .png or .pdf). You can specify the dimensions of the output using the width and height arguments.\n\n\n\n\n\n\nThe flexibility of ggplot2 extends beyond traditional plots through additional libraries that expand its functionality, allowing you to create specialised visualisations. For instance, we can use the treemapify library to create a treemap.\n\n\n\n\n\n\nA treemap is a data visualisation that displays hierarchical data as nested rectangles, with each rectangle representing a category or subcategory. The size of each rectangle is proportional to a specific variable, often reflecting values such as frequency or proportion, making it easier to compare the relative sizes of different elements. Treemaps are particularly useful for visualising large datasets with multiple categories or subcategories in a compact, space-efficient layout.\n\n\n\nLet’s try to create a treemap of the mean share of different population groups in the borough of Lambeth. We first need to calculate the mean of each population group in Lambeth and then transform the data from a wide format to a long format so that all proportions are in the same column.\n\n\n\nR code\n\n# mean group values lambeth\nlambeth_mean &lt;- lsoa_eth |&gt;\n    filter(borough_name == \"Lambeth\") |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# wide to long\nlambeth_mean &lt;- lambeth_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n\nWe can now visualise the share of each population group in Lambeth using a treemap:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lambeth_mean, aes(area = proportion, fill = population_group, label = population_group)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 7: Treemap of relative share of population groups in Lambeth.\n\n\n\n\nWe could create facets for these treemaps, but we can also use subgroups to create a nested representation of our data - weighted by the total population in each London borough.\n\n\n\nR code\n\n# mean group values london\nlondon_mean &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# total group values london\nlondon_sum &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(borough_population = sum(eth_pop))\n\n# wide to long,\nlondon_mean &lt;- london_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n# add total population, weigh\nlondon_mean &lt;- london_mean |&gt;\n    left_join(london_sum, by = c(borough_name = \"borough_name\")) |&gt;\n    mutate(proportion_weighted = proportion * borough_population)\n\n\nNow the data have been prepared, we can create a treemap again as follows:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = london_mean, aes(area = proportion_weighted, fill = population_group, label = population_group, subgroup = borough_name)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"#f0f0f0\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8,\n  ) +\n  # add border\n  geom_treemap_subgroup_border(\n    colour = \"#000000\"\n  ) +\n  # add text\n  geom_treemap_subgroup_text(\n    colour = \"#636363\",\n    place = \"bottomleft\",\n    size = 14,\n    fontface = \"bold\",\n    padding.x = grid::unit(2, \"mm\"),\n    padding.y = grid::unit(2, \"mm\"),\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\",\n  )\n\n\n\n\n\nFigure 8: Treemap of relative share of population groups in London, organised by borough\n\n\n\n\n\n\n\n\nThe ggplot2 library supports a wide variety of chart types, all based on the same core principles of layering elements such as data, aesthetics, and geometric shapes. So far, we have worked with boxplots, scatterplots, histograms, and treemaps. However, ggplot2 also offers many other geometries, including spatial geometries, that you can use to create more diverse visualisations.\nUsing the lsoa_eth dataset try to to complete the following tasks:\n\nCreate a violin plot: A violin plot combines aspects of a boxplot and a density plot, offering a compact view of the distribution of continuous data. Use the geom_violin() function to visualise the distribution of the self-identified Asian Bangladeshi population for each of the Inner London boroughs.\nCreate a map: Use the geom_sf() function to map the distribution of the self-identified Black Caribbean population across Greater London.\nCreate a faceted map: Create a faceted map showing the distribution of the self-identified Asian Bangladeshi, Asian Chinese, Black African, and White British populations across London.\n\n\n\n\n\n\n\nTo help you get familiar with ggplot2 and its principles, you can use the esquisse library, which allows you to interactively create plots and generate the corresponding ggplot2 code.\n\n\n\n\n\n\nThat is it for today, and indeed, you have now reached the end of Geocomputation! Over the course of this module, we have explored the fundamental principles of spatial analysis, data visualisation, and reproducible research. It is now inevitable: time for that reading list."
  },
  {
    "objectID": "10-datavis.html#lecture-slides",
    "href": "10-datavis.html#lecture-slides",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "10-datavis.html#reading-list",
    "href": "10-datavis.html#reading-list",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Wickham, H. 2010. A layered grammar of graphics. Journal of Computational and Graphical Statistics 19(1): 3-28. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2014. London, The Information Capital: 100 Maps & Graphics That Will Change How You View the City. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 3: Data visualisation. [Link]"
  },
  {
    "objectID": "10-datavis.html#population-groups-in-london",
    "href": "10-datavis.html#population-groups-in-london",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "Today, we will use the same dataset that we used in Week 8 on self-identified ethnicity. We will visualise the distribution of the self-identified White-British population across the 12 Inner London Boroughs. The LSOA data covers all usual residents, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level. A copy of the 2021 London LSOAs spatial boundaries is also available. If you do not already have it on your computer, save these file in your data/attribues and data/spatial folders.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Ethnicity\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w10-ethnicity-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(treemapify)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load the files in the usual fashion:\n\n\n\nR code\n\n# load attribute dataset\nlsoa_eth &lt;- read_csv(\"data/attributes/London-LSOA-Ethnicity.csv\")\n\n\nRows: 99880 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Ethnic group (20 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load spatial dataset\nlsoa21 &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\") |&gt;\n    st_drop_geometry()\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(lsoa_eth)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Ethnic group (20 cat…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000001                        City of London 001A                         4\n6 E01000001                        City of London 001A                         5\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Ethnic group (20 categories) Code`\n# ℹ 2 more variables: `Ethnic group (20 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect\nhead(lsoa21)\n\n   lsoa21cd                  lsoa21nm  bng_e  bng_n      long      lat\n1 E01000001       City of London 001A 532123 181632 -0.097140 51.51816\n2 E01000002       City of London 001B 532480 181715 -0.091970 51.51882\n3 E01000003       City of London 001C 532239 182033 -0.095320 51.52174\n4 E01000005       City of London 001E 533581 181283 -0.076270 51.51468\n5 E01000006 Barking and Dagenham 016A 544994 184274  0.089317 51.53875\n6 E01000007 Barking and Dagenham 015A 544187 184455  0.077763 51.54058\n                                globalid pop2021\n1 {1A259A13-A525-4858-9CB0-E4952BA01AF6}    1473\n2 {1233E433-0B0D-4807-8117-17A83C23960D}    1384\n3 {5163B7CB-4FFE-4F41-95B9-AA6CFC0508A3}    1613\n4 {2AF8015E-386E-456D-A45A-D0A223C340DF}    1101\n5 {B492B45E-175E-4E77-B0B5-5B2FD6993EF4}    1842\n6 {4A374975-B1D0-40CE-BF6E-6305623E5F7E}    2904\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\nWe will start by pivoting the data and transforming the raw counts into proportions:\n\n\n\nR code\n\n# prepare ethnicity data\nlsoa_eth &lt;- lsoa_eth |&gt;\n    clean_names() |&gt;\n    pivot_wider(id_cols = \"lower_layer_super_output_areas_code\", names_from = \"ethnic_group_20_categories\",\n        values_from = \"observation\") |&gt;\n    clean_names()\n\n# proportions, select columns\nlsoa_eth &lt;- lsoa_eth |&gt;\n    rowwise() |&gt;\n    mutate(eth_pop = sum(across(2:21))) |&gt;\n    mutate(across(2:21, ~./eth_pop)) |&gt;\n    select(-2)\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 4,994 × 21\n# Rowwise: \n   lower_layer_super_output_area…¹ asian_asian_british_…² asian_asian_british_…³\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 E01000001                                      0.00271                0.0448 \n 2 E01000002                                      0.00505                0.0736 \n 3 E01000003                                      0.00682                0.0323 \n 4 E01000005                                      0.239                  0.0318 \n 5 E01000006                                      0.116                  0.00596\n 6 E01000007                                      0.113                  0.0148 \n 7 E01000008                                      0.110                  0.00445\n 8 E01000009                                      0.119                  0.0122 \n 9 E01000011                                      0.146                  0.00469\n10 E01000012                                      0.122                  0.00298\n# ℹ 4,984 more rows\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​asian_asian_british_or_asian_welsh_bangladeshi,\n#   ³​asian_asian_british_or_asian_welsh_chinese\n# ℹ 18 more variables: asian_asian_british_or_asian_welsh_indian &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_pakistani &lt;dbl&gt;,\n#   asian_asian_british_or_asian_welsh_other_asian &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe column names are rather long, so let’s rename these manually:\n\n\n\nR code\n\n# rename columns\nnames(lsoa_eth)[2:20] &lt;- c(\"Asian - Bangladeshi\", \"Asian - Chinese\", \"Asian - Indian\",\n    \"Asian - Pakistani\", \"Asian - Other\", \"Black - African\", \"Black - Caribbean\",\n    \"Black - Other\", \"Mixed - Asian\", \"Mixed - Black African\", \"Mixed - Black Carribean\",\n    \"Mixed - Other\", \"White - British\", \"White - Irish\", \"White - Traveller\", \"White - Roma\",\n    \"White - Other\", \"Arab - Other\", \"Any Other Group\")\n\n\nThe last thing we need to do is extract the LSOAs that fall within the 12 Inner London Boroughs. We can do this by using the LSOA names that are inside the spatial dataframe:\n\n\n\nR code\n\n# boroughs\ninner_boroughs &lt;- c(\"Camden\", \"Greenwich\", \"Hackney\", \"Hammersmith and Fulham\", \"Islington\", \"Kensington and Chelsea\", \"Lambeth\", \"Lewisham\", \"Southwark\", \"Tower Hamlets\", \"Wandsworth\", \"Westminster\")\n\n# filter spatial data\nlsoa21_inner &lt;- lsoa21 |&gt;\n  filter(str_detect(lsoa21nm, paste(inner_boroughs, collapse = \"|\")))\n\n# filter attribute data, add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  filter(lower_layer_super_output_areas_code %in% lsoa21_inner$lsoa21cd)\n\n# add lsoa names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  left_join(lsoa21[1:2], by = c(\"lower_layer_super_output_areas_code\" = \"lsoa21cd\"))\n\n# inspect\nlsoa_eth\n\n\n# A tibble: 1,792 × 22\n# Rowwise: \n   lower_layer_super_output_areas_code `Asian - Bangladeshi` `Asian - Chinese`\n   &lt;chr&gt;                                               &lt;dbl&gt;             &lt;dbl&gt;\n 1 E01000842                                         0.00209            0.0265\n 2 E01000843                                         0.00167            0.0301\n 3 E01000844                                         0.00614            0.0258\n 4 E01000845                                         0.0193             0.0273\n 5 E01000846                                         0.0586             0.0179\n 6 E01000847                                         0.0506             0.0312\n 7 E01000848                                         0.00269            0.0249\n 8 E01000849                                         0.0125             0.0249\n 9 E01000850                                         0.0218             0.113 \n10 E01000851                                         0.0216             0.0824\n# ℹ 1,782 more rows\n# ℹ 19 more variables: `Asian - Indian` &lt;dbl&gt;, `Asian - Pakistani` &lt;dbl&gt;,\n#   `Asian - Other` &lt;dbl&gt;, `Black - African` &lt;dbl&gt;, `Black - Caribbean` &lt;dbl&gt;,\n#   `Black - Other` &lt;dbl&gt;, `Mixed - Asian` &lt;dbl&gt;,\n#   `Mixed - Black African` &lt;dbl&gt;, `Mixed - Black Carribean` &lt;dbl&gt;,\n#   `Mixed - Other` &lt;dbl&gt;, `White - British` &lt;dbl&gt;, `White - Irish` &lt;dbl&gt;,\n#   `White - Traveller` &lt;dbl&gt;, `White - Roma` &lt;dbl&gt;, `White - Other` &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIf you want to know what the paste(inner_boroughs, collapse = '|') code does, you can run it separately in the console to find out.\n\n\n\n\n\nThe ggplot2 library is built on the layered grammar of graphics, which provides a structured approach to creating visualisations. This means that plots are constructed by adding layers, such as data, aesthetic mappings (e.g., axes, colours, sizes), geometric shapes (e.g., points, lines, bars), and optional elements like themes or statistical transformations. This modular design allows users to build complex and customisable plots step by step, ensuring flexibility and clarity in the visualisation process.\nLet’s try to use this approach by making a boxplot on the distribution of people that self-identify as White British across all Inner London Boroughs. With ggplot2, every plot begins with the ggplot() function, which creates a coordinate system to which layers can be added. The first argument of ggplot() specifies the dataset to use:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth)\n\n\nTo build your graph, you add one or more layers to ggplot(). For instance, geom_point() adds a layer of points to create a scatterplot. ggplot2 provides many geom functions, each adding a different type of layer to your plot. To create a boxplot, you add the geom_boxplot() layer. For boxplots, the mapping argument defines how dataset variables are linked to visual properties, such as the grouping or value axes. The mapping is paired with aes(), where y specifies the numeric variable:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot()\n\n\n\n\n\nFigure 1: Basic boxplot using ggplot2.\n\n\n\n\n\n\n\n\n\n\nAn aesthetic is a visual property of the elements in your plot. Aesthetics include attributes like size, shape, or colour of points. By modifying the values of these aesthetic properties, you can display a point in various ways, allowing for greater customisation and clarity in your visualisation.\n\n\n\nJust like with tmap, we can customise the basic plot by styling the boxplot, adding labels, and adjusting its overall appearancs:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 2: Stylised boxplot using ggplot2.\n\n\n\n\nBut what if we wwant to create a boxplot for all Inner London Boroughs? We can do this by adding a grouping variable:\n\n\n\nR code\n\n# add borough names\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name = substr(lsoa21nm, 1, nchar(lsoa21nm) - 5))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    utlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\nWarning in geom_boxplot(fill = \"#f0f0f0\", color = \"#252525\", outlier.color =\n\"#ef3b2c\", : Ignoring unknown parameters: `utlier.shape`\n\n\n\n\n\nFigure 3: Stylised boxplot using ggplot2.\n\n\n\n\nThe boroughs are drawn in alphabetical order by default. To change this we need to adjust the order by creating a factor. For instance, we can sort the boroughs by their median values.\n\n\n\n\n\n\nIn R, a factor is a data structure used to represent categorical variables with a specific order or grouping. Factors allow you to define and manipulate the order of categories, which is especially useful for plotting or analysis.\n\n\n\n\n\n\nR code\n\n# median values\nlsoa_med &lt;- lsoa_eth |&gt;\n  group_by(borough_name) |&gt;\n  summarise(median = median(`White - British`))\n\n# create factor\nlsoa_eth &lt;- lsoa_eth |&gt;\n  mutate(borough_name_factor = factor(borough_name, levels = lsoa_med$borough_name[order(lsoa_med$median, decreasing = TRUE)]))\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = borough_name_factor)) +\n  # add geometry\n  geom_boxplot(\n    fill = \"#f0f0f0\",\n    color = \"#252525\",\n    outlier.color = \"#ef3b2c\",\n    linewidth = 0.5,\n    staplewidth = 0.5,\n    outlier.shape = 16,\n    outlier.size = 2\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Borough\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n    panel.grid.major = element_line(linewidth = 0.5, colour = \"#969696\"),\n    panel.grid.minor = element_line(linewidth = 0.2, colour = \"#d9d9d9\")\n  )\n\n\n\n\n\nFigure 4: Stylised boxplot using ggplot2.\n\n\n\n\n\n\n\nBoxplots are effective for visualising distributions, but histograms offer another way to explore the same data by showing the frequency of values. While histograms cannot be displayed alongside boxplots in the same image, we can create a series of histograms, each displayed in a separate panel. These panels can show the distributions for different groups, such as individual boroughs.\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`)) +\n  # add geometry\n  geom_histogram() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # add labels\n  labs(\n    title = \"Population self-identifying as White British\",\n    y = \"Number of LSOAs\",\n    x = \"\"\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\nFigure 5: Histograms presented in individual panels.\n\n\n\n\nWe could use the same approach to create a series of scatterplots to show the relationship between two variables:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lsoa_eth, aes(x = `White - British`, y = `White - Other`)) +\n  # add geometry\n  geom_point() +\n  # create panels\n  facet_wrap(\n    ~borough_name,\n    ncol = 4,\n    nrow = 3\n  ) +\n  # set basic theme\n  theme_light() +\n  # customise theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title = element_text(size = 12, colour = \"#34495e\"),\n    axis.text = element_text(size = 10, colour = \"#34495e\"),\n  )\n\n\n\n\n\nFigure 6: Scatterplots presented in individual panels.\n\n\n\n\n\n\n\n\n\n\nTo export a ggplot, first assign your plot to an object. Then, use the ggsave() function to save the plot to a file, specifying the desired filename and format (e.g. .png or .pdf). You can specify the dimensions of the output using the width and height arguments.\n\n\n\n\n\n\nThe flexibility of ggplot2 extends beyond traditional plots through additional libraries that expand its functionality, allowing you to create specialised visualisations. For instance, we can use the treemapify library to create a treemap.\n\n\n\n\n\n\nA treemap is a data visualisation that displays hierarchical data as nested rectangles, with each rectangle representing a category or subcategory. The size of each rectangle is proportional to a specific variable, often reflecting values such as frequency or proportion, making it easier to compare the relative sizes of different elements. Treemaps are particularly useful for visualising large datasets with multiple categories or subcategories in a compact, space-efficient layout.\n\n\n\nLet’s try to create a treemap of the mean share of different population groups in the borough of Lambeth. We first need to calculate the mean of each population group in Lambeth and then transform the data from a wide format to a long format so that all proportions are in the same column.\n\n\n\nR code\n\n# mean group values lambeth\nlambeth_mean &lt;- lsoa_eth |&gt;\n    filter(borough_name == \"Lambeth\") |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# wide to long\nlambeth_mean &lt;- lambeth_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n\nWe can now visualise the share of each population group in Lambeth using a treemap:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = lambeth_mean, aes(area = proportion, fill = population_group, label = population_group)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"white\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nFigure 7: Treemap of relative share of population groups in Lambeth.\n\n\n\n\nWe could create facets for these treemaps, but we can also use subgroups to create a nested representation of our data - weighted by the total population in each London borough.\n\n\n\nR code\n\n# mean group values london\nlondon_mean &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(across(2:20, mean))\n\n# total group values london\nlondon_sum &lt;- lsoa_eth |&gt;\n    group_by(borough_name) |&gt;\n    summarise(borough_population = sum(eth_pop))\n\n# wide to long,\nlondon_mean &lt;- london_mean |&gt;\n    pivot_longer(cols = 2:20, names_to = \"population_group\", values_to = \"proportion\")\n\n# add total population, weigh\nlondon_mean &lt;- london_mean |&gt;\n    left_join(london_sum, by = c(borough_name = \"borough_name\")) |&gt;\n    mutate(proportion_weighted = proportion * borough_population)\n\n\nNow the data have been prepared, we can create a treemap again as follows:\n\n\n\nR code\n\n# initiate ggplot\nggplot(data = london_mean, aes(area = proportion_weighted, fill = population_group, label = population_group, subgroup = borough_name)) +\n  # add geometry\n  geom_treemap() +\n  # add text\n  geom_treemap_text(\n    colour = \"#f0f0f0\",\n    place = \"centre\",\n    grow = TRUE,\n    min.size = 8,\n  ) +\n  # add border\n  geom_treemap_subgroup_border(\n    colour = \"#000000\"\n  ) +\n  # add text\n  geom_treemap_subgroup_text(\n    colour = \"#636363\",\n    place = \"bottomleft\",\n    size = 14,\n    fontface = \"bold\",\n    padding.x = grid::unit(2, \"mm\"),\n    padding.y = grid::unit(2, \"mm\"),\n  ) +\n  # set basic theme\n  theme_minimal() +\n  # customise theme\n  theme(\n    legend.position = \"none\",\n  )\n\n\n\n\n\nFigure 8: Treemap of relative share of population groups in London, organised by borough"
  },
  {
    "objectID": "10-datavis.html#assignment",
    "href": "10-datavis.html#assignment",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "The ggplot2 library supports a wide variety of chart types, all based on the same core principles of layering elements such as data, aesthetics, and geometric shapes. So far, we have worked with boxplots, scatterplots, histograms, and treemaps. However, ggplot2 also offers many other geometries, including spatial geometries, that you can use to create more diverse visualisations.\nUsing the lsoa_eth dataset try to to complete the following tasks:\n\nCreate a violin plot: A violin plot combines aspects of a boxplot and a density plot, offering a compact view of the distribution of continuous data. Use the geom_violin() function to visualise the distribution of the self-identified Asian Bangladeshi population for each of the Inner London boroughs.\nCreate a map: Use the geom_sf() function to map the distribution of the self-identified Black Caribbean population across Greater London.\nCreate a faceted map: Create a faceted map showing the distribution of the self-identified Asian Bangladeshi, Asian Chinese, Black African, and White British populations across London.\n\n\n\n\n\n\n\nTo help you get familiar with ggplot2 and its principles, you can use the esquisse library, which allows you to interactively create plots and generate the corresponding ggplot2 code."
  },
  {
    "objectID": "10-datavis.html#before-you-leave",
    "href": "10-datavis.html#before-you-leave",
    "title": "1 Complex Visualisations",
    "section": "",
    "text": "That is it for today, and indeed, you have now reached the end of Geocomputation! Over the course of this module, we have explored the fundamental principles of spatial analysis, data visualisation, and reproducible research. It is now inevitable: time for that reading list."
  },
  {
    "objectID": "03-point-pattern.html",
    "href": "03-point-pattern.html",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "This week, we will be focusing on point pattern analysis (PPA), which aims to detect clusters or patterns within a set of points. Through this analysis, we can measure density, dispersion, and homogeneity in point structures. Various methods exist for calculating and identifying these clusters, and today we will explore several of these techniques using our bike theft dataset from last week.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nArribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]\n\n\n\n\n\nThis week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the bicycle theft dataset that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Bicycle Theft 2023\nGeoPackage\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w03-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAs always, we will start by loading loading our files into memory:\n\n\n\nR code\n\n# load msoa dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load bicycle theft dataset\ntheft_bike &lt;- st_read(\"data/spatial/London-BicycleTheft-2023.gpkg\")\n\nReading layer `London-BicycleTheft-2023' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-BicycleTheft-2023.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16019 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(theft_bike)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n                                                          crime_id   month\n1 62b0f525fc471c062463ec87469c71e133cdd1c39a09e2bc5ec50c0cbbdd650a 2023-01\n2 9a078d630cf67c37c9e47b5904149d553697931d920eec993f358a637fbf6186 2023-01\n3 f175a32ef7f90c67a5cb83c268ad793e4ce7e0ae19e52b57d213805e65651bdd 2023-01\n4 137ec1201fd64b57898d3558fe3b8000182442e957efc28246797ea61065c86b 2023-01\n5 4c3b467755a98afa3d3c82b526307750ddad9ec13598aaa68130b76863e112cb 2023-01\n6 13b5eb5ca0aef09a222221fbab8da799d55b5a65716f1bbb7cc96e36aebdc816 2023-01\n            reported_by          falls_within                   location\n1 City of London Police City of London Police                 On or near\n2 City of London Police City of London Police                 On or near\n3 City of London Police City of London Police  On or near Lombard Street\n4 City of London Police City of London Police    On or near Mitre Street\n5 City of London Police City of London Police   On or near Temple Avenue\n6 City of London Police City of London Police On or near Montague Street\n  lsoa_code           lsoa_name    crime_type\n1 E01000002 City of London 001B Bicycle theft\n2 E01032739 City of London 001F Bicycle theft\n3 E01032739 City of London 001F Bicycle theft\n4 E01032739 City of London 001F Bicycle theft\n5 E01032740 City of London 001G Bicycle theft\n6 E01032740 City of London 001G Bicycle theft\n                          last_outcome_category context\n1 Investigation complete; no suspect identified      NA\n2 Investigation complete; no suspect identified      NA\n3 Investigation complete; no suspect identified      NA\n4 Investigation complete; no suspect identified      NA\n5 Investigation complete; no suspect identified      NA\n6 Investigation complete; no suspect identified      NA\n                       geom\n1 POINT (532390.8 181781.7)\n2 POINT (532157.8 181196.8)\n3 POINT (532720.7 181087.8)\n4 POINT (533333.7 181219.8)\n5 POINT (531274.9 180967.9)\n6 POINT (531956.8 181624.8)\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nOne key advantage of point data is that it is scale-free, allowing aggregation to any geographic level for analysis. Before diving into PPA, we will aggregate the bicycle thefts to the MSOA level to map their distribution by using a point-in-polygon approach.\n\n\n\nR code\n\n# point in polygon\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike, sparse = TRUE)))\n\n\n\n\n\n\n\n\nTo create a point-in-polygon count within sf, we use the st_intersects() function and keep its default sparse = TRUE output, which produces a list of intersecting points by index for each polygon (e.g. MSOA). We then apply the lengths() function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.\n\n\n\nWe can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.\n\n\n\nR code\n\n# msoa area size\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(km_sq = as.numeric(st_area(msoa21))/1e+06)\n\n# theft density\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(thef_km_sq = theft_bike_n/km_sq)\n\n\nLet’s put this onto a map:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"thef_km_sq\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\")\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.title.size = 0.8,\n    legend.text.size = 0.8,\n    legend.position = c(0.8, 0.35),\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Number of reported bicycle thefts by square kilometre.\n\n\n\n\n\n\n\nFigure 1 shows that the number of bicycle thefts is clearly concentrated in parts of Central London. While this map may provide helpful insights, its representation depends on the classification and aggregation of the underlying data. Alternatively, we can directly analyse the point events themselves. For this, we will use the spatstat library, the primary library for point pattern analysis in R. To use spatstat, we need to convert our data into a ppp object.\n\n\n\n\n\n\nThe ppp format is specific to spatstat but is also used in some other spatial analysis libraries. A ppp object represents a two-dimensional point dataset within a defined area, called the window of observation (owin in spatstat). We can either create a ppp object directly from a list of coordinates (with a specified window of observation) or convert it from another data type.\n\n\n\nWe can turn our theft_bike dataframe into a ppp object as follows:\n\n\n\nR code\n\n# london outline\noutline &lt;- msoa21 |&gt;\n    st_union()\n\n# clip\ntheft_bike &lt;- theft_bike |&gt;\n    st_intersection(outline)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# sf to ppp\nwindow = as.owin(msoa21)\ntheft_bike_ppp &lt;- ppp(st_coordinates(theft_bike)[, 1], st_coordinates(theft_bike)[,\n    2], window = window)\n\nWarning: data contain duplicated points\n\n# inspect\npar(mar = c(1, 1, 1, 1))\nplot(theft_bike_ppp, main = \"\")\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nSome statistical procedures require point events to be unique. In our bicycle theft data, duplicates are likely due to the police snapping points to protect anonymity and privacy. This can pose an issue for spatial point pattern analysis, where each theft and its location must be distinct. We can check whether we have any duplicated points as follows:\n\n\n\nR code\n\n# check for duplicates\nanyDuplicated(theft_bike_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_ppp) &gt; 1)\n\n[1] 10003\n\n\nTo address this, we have three options:\n\nRemove duplicates if the number of duplicated points is small or the exact location is less important than the overall distribution.\nAssign weights to points, where each has an attribute indicating the number of events at that location rather than being recorded as separate event.\nAdd jitter by slightly offsetting the points randomly, which can be useful if precise location is not crucial for the analysis.\n\nEach approach has its own trade-offs, depending on the analysis. In our case, we will use the jitter approach to retain all bike theft events. Since the locations are already approximated, adding a small offset (~5 metre) will not impact the analysis.\n\n\n\nR code\n\n# add ajitter\ntheft_bike_jitter &lt;- rjitter(theft_bike_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for duplicates\nanyDuplicated(theft_bike_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_jitter) &gt; 1)\n\n[1] 0\n\n\nThis seemed to have worked, so we can move forward.\n\n\nInstead of visualising the distribution of bike thefts at a specific geographical level, we can use Kernel Density Estimation (KDE) to display the distribution of these incidents. KDE is a statistical method that creates a smooth, continuous distribution to represent the density of the underlying pattern between data points.\n\n\n\n\n\n\nKernel Density Estimation (KDE) generates a raster surface that shows the estimated density of event points across space. Each cell represents the local density, highlighting areas of high or low concentration. KDE uses overlapping moving windows (defined by a kernel) and a bandwidth parameter, which controls the size of the window, influencing the smoothness of the resulting density surface. The kernel function can assign equal or weighted values to points, producing a grid of density values based on these local calculations.\n\n\n\nLet’s go ahead and create a simple KDE of bike theft with our bandwidth set to 500 metres:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 500), main = \"\")\n\n\n\n\n\nFigure 3: Kernel density estimation - bandwidth 500m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around Central London. We can go ahead and increase the bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 1000), main = \"\")\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 1000m.\n\n\n\n\nBy increasing the bandwidth, our clusters appear larger and brighter than with the 500-metre bandwidth. A larger bandwidth considers more points, resulting in a smoother surface. However, this can lead to oversmoothing, where clusters become less defined, potentially overestimating areas of high bike theft. Smaller bandwidths offer more precision and sharper clusters but risk undersmoothing, which can cause irregularities.\n\n\n\n\n\n\nWhile automated methods (e.g. maximum-likelihood estimation) can assist in selecting an optimal bandwidth, the choice is subjective and depends on the specific characteristics of your dataset.\n\n\n\n\n\n\n\n\n\nAlthough bandwidth has a greater impact on density estimation than the kernel type, the choice of kernel can still influence the results by altering how points are weighted within the window. We will explore kernel types a little further when we discuss spatial models in a few weeks time.\n\n\n\nOnce we are satisfied with our KDE visualisation, we can create a proper map by converting the KDE output into raster format.\n\n\n\nR code\n\n# to raster\ntheft_bike_raster &lt;- density.ppp(theft_bike_jitter, sigma = 1000) |&gt;\n    rast()\n\n\nWe now have a standalone raster that we can use with any function in the tmap library. However, one issue is that the resulting raster lacks a Coordinate Reference System (CRS), so we need to manually assign this information to the raster object:\n\n\n\nR code\n\n# set CRS\ncrs(theft_bike_raster) &lt;- \"EPSG:27700\"\n\n\nNow we can map the KDE values.\n\n\n\nR code\n\n# shape\ntm_shape(theft_bike_raster) +\n\n  # map data\n  tm_raster(\n    # map data\n    col = \"lyr.1\",\n    col.scale = tm_scale(\n      values = \"brewer.blues\"\n    ),\n\n    # legend\n    col.legend = tm_legend(\n      title = \"Density\",\n      frame = FALSE,\n    )\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.8, 0.35),\n    legend.text.size = 0.8,\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Kernel Density Estimate of bicycle thefts in London.\n\n\n\n\n\n\n\n\n\n\nThe values of the KDE output are stored in the raster grid as lyr.1.\n\n\n\n\n\n\nKernel Density Estimation is a useful exploratory technique for identifying spatial clusters in point data, but it does not provide precise boundaries for these clusters. To more accurately delineate clusters, we can use an algorithm called DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which takes both distance and density into account. DBSCAN is effective at discovering distinct clusters by grouping together points that are close to one another while marking points that don’t belong to any cluster as noise.\nDBSCAN requires two parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nepsilon\nThe maximum distance for points to be considered in the same cluster.\n\n\nminPts\nThe minimum number of points for a cluster.\n\n\n\nThe algorithm groups nearby points based on these parameters and marks low-density points as outliers. DBSCAN is useful for uncovering patterns that are difficult to detect visually, but it works best when clusters have consistent densities.\nLet us try this with an epsilon of 200 metres and minPts of 20 bicycle thefts:\n\n\n\nR code\n\n# dbscan\nbike_theft_dbscan &lt;- theft_bike |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function accepts a data matrix or dataframe of points, not a spatial dataframe. That is why, in the code above, we use the st_coordinates() function to extract the projected coordinates from the spatial dataframe.\n\n\n\nThe DBSCAN output includes three objects, one of which is a vector detailing the cluster each bike theft observation has been assigned to. To work with this output effectively, we need to add the cluster labels back to the original point dataset. Since DBSCAN does not alter the order of points, we can simply add the cluster output to the theft_bike spatial dataframe.\n\n\n\nR code\n\n# add cluster numbers\ntheft_bike &lt;- theft_bike |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow that each bike theft point in London is associated with a specific cluster, where appropriate, we can generate a polygon representing these clusters. To do this, we will use the st_convex_hull() function from the sf package, which creates a polygon that covers the minimum bounding area of a collection of points. We will apply this function to each cluster using a for loop, which allows us to repeat the process for each group of points and create a polygon representing the geometry of each cluster.\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(theft_bike$dbcluster))\n\n# begin loop\nfor (cluster_index in seq(1, max(theft_bike$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    theft_bike_subset &lt;- theft_bike |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- theft_bike_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[cluster_index] &lt;- (cluster_polygon)\n\n}\n\n# combine the list\ntheft_bike_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\n\n\n\n\n\n\nWhile loops in R should generally be avoided for large datasets due to inefficiency, they remain a useful tool for automating repetitive tasks and reducing the risk of errors. For smaller datasets or tasks that cannot easily be vectorised, loops can still be effective and simplify the code.\n\n\n\nWe now have a spatial dataframe that contains the bike theft clusters in London, as defined by the DBSCAN clustering algorithm. Let’s quickly map these clusters:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    size = 0.10,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # shape\n  tm_shape(theft_bike_clusters) +\n\n  # map data\n  tm_polygons(\n    col = \"#fdc086\",\n    fill = \"#fdc086\",\n    fill_alpha = 0.7\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 6: DBSCAN-identified clusters of reported bicycle theft in London.\n\n\n\n\n\n\n\n\n\nNow that we know how to work with point locatoin data, we can again apply a similar analysis to road crashes in London in 2022 that we used last week. This time we will use this dataset to assess whether road crashes cluster in specific areas. Try the following:\n\nCreate a Kernel Density Estimation (KDE) of all road crashes that occurred in London in 2022.\nUsing DBSCAN output, create a cluster map of serious and fatal road crashes in London\n\nIf you no longer have a copy of the 2022 London STATS19 Road Collision dataset, you can download it using the link provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload\n\n\n\n\n\n\nWith access to point event data, geographers aim to identify underlying patterns. This week, we explored several techniques that help us analyse and interpret such data. That is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "03-point-pattern.html#lecture-slides",
    "href": "03-point-pattern.html#lecture-slides",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "03-point-pattern.html#reading-list",
    "href": "03-point-pattern.html#reading-list",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]"
  },
  {
    "objectID": "03-point-pattern.html#bike-theft-in-london-ii",
    "href": "03-point-pattern.html#bike-theft-in-london-ii",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "This week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the bicycle theft dataset that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon Bicycle Theft 2023\nGeoPackage\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w03-bike-theft.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nAs always, we will start by loading loading our files into memory:\n\n\n\nR code\n\n# load msoa dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load bicycle theft dataset\ntheft_bike &lt;- st_read(\"data/spatial/London-BicycleTheft-2023.gpkg\")\n\nReading layer `London-BicycleTheft-2023' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-BicycleTheft-2023.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16019 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(theft_bike)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n                                                          crime_id   month\n1 62b0f525fc471c062463ec87469c71e133cdd1c39a09e2bc5ec50c0cbbdd650a 2023-01\n2 9a078d630cf67c37c9e47b5904149d553697931d920eec993f358a637fbf6186 2023-01\n3 f175a32ef7f90c67a5cb83c268ad793e4ce7e0ae19e52b57d213805e65651bdd 2023-01\n4 137ec1201fd64b57898d3558fe3b8000182442e957efc28246797ea61065c86b 2023-01\n5 4c3b467755a98afa3d3c82b526307750ddad9ec13598aaa68130b76863e112cb 2023-01\n6 13b5eb5ca0aef09a222221fbab8da799d55b5a65716f1bbb7cc96e36aebdc816 2023-01\n            reported_by          falls_within                   location\n1 City of London Police City of London Police                 On or near\n2 City of London Police City of London Police                 On or near\n3 City of London Police City of London Police  On or near Lombard Street\n4 City of London Police City of London Police    On or near Mitre Street\n5 City of London Police City of London Police   On or near Temple Avenue\n6 City of London Police City of London Police On or near Montague Street\n  lsoa_code           lsoa_name    crime_type\n1 E01000002 City of London 001B Bicycle theft\n2 E01032739 City of London 001F Bicycle theft\n3 E01032739 City of London 001F Bicycle theft\n4 E01032739 City of London 001F Bicycle theft\n5 E01032740 City of London 001G Bicycle theft\n6 E01032740 City of London 001G Bicycle theft\n                          last_outcome_category context\n1 Investigation complete; no suspect identified      NA\n2 Investigation complete; no suspect identified      NA\n3 Investigation complete; no suspect identified      NA\n4 Investigation complete; no suspect identified      NA\n5 Investigation complete; no suspect identified      NA\n6 Investigation complete; no suspect identified      NA\n                       geom\n1 POINT (532390.8 181781.7)\n2 POINT (532157.8 181196.8)\n3 POINT (532720.7 181087.8)\n4 POINT (533333.7 181219.8)\n5 POINT (531274.9 180967.9)\n6 POINT (531956.8 181624.8)\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nOne key advantage of point data is that it is scale-free, allowing aggregation to any geographic level for analysis. Before diving into PPA, we will aggregate the bicycle thefts to the MSOA level to map their distribution by using a point-in-polygon approach.\n\n\n\nR code\n\n# point in polygon\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike, sparse = TRUE)))\n\n\n\n\n\n\n\n\nTo create a point-in-polygon count within sf, we use the st_intersects() function and keep its default sparse = TRUE output, which produces a list of intersecting points by index for each polygon (e.g. MSOA). We then apply the lengths() function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.\n\n\n\nWe can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.\n\n\n\nR code\n\n# msoa area size\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(km_sq = as.numeric(st_area(msoa21))/1e+06)\n\n# theft density\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(thef_km_sq = theft_bike_n/km_sq)\n\n\nLet’s put this onto a map:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"thef_km_sq\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\")\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.title.size = 0.8,\n    legend.text.size = 0.8,\n    legend.position = c(0.8, 0.35),\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 1: Number of reported bicycle thefts by square kilometre.\n\n\n\n\n\n\n\nFigure 1 shows that the number of bicycle thefts is clearly concentrated in parts of Central London. While this map may provide helpful insights, its representation depends on the classification and aggregation of the underlying data. Alternatively, we can directly analyse the point events themselves. For this, we will use the spatstat library, the primary library for point pattern analysis in R. To use spatstat, we need to convert our data into a ppp object.\n\n\n\n\n\n\nThe ppp format is specific to spatstat but is also used in some other spatial analysis libraries. A ppp object represents a two-dimensional point dataset within a defined area, called the window of observation (owin in spatstat). We can either create a ppp object directly from a list of coordinates (with a specified window of observation) or convert it from another data type.\n\n\n\nWe can turn our theft_bike dataframe into a ppp object as follows:\n\n\n\nR code\n\n# london outline\noutline &lt;- msoa21 |&gt;\n    st_union()\n\n# clip\ntheft_bike &lt;- theft_bike |&gt;\n    st_intersection(outline)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# sf to ppp\nwindow = as.owin(msoa21)\ntheft_bike_ppp &lt;- ppp(st_coordinates(theft_bike)[, 1], st_coordinates(theft_bike)[,\n    2], window = window)\n\nWarning: data contain duplicated points\n\n# inspect\npar(mar = c(1, 1, 1, 1))\nplot(theft_bike_ppp, main = \"\")\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nSome statistical procedures require point events to be unique. In our bicycle theft data, duplicates are likely due to the police snapping points to protect anonymity and privacy. This can pose an issue for spatial point pattern analysis, where each theft and its location must be distinct. We can check whether we have any duplicated points as follows:\n\n\n\nR code\n\n# check for duplicates\nanyDuplicated(theft_bike_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_ppp) &gt; 1)\n\n[1] 10003\n\n\nTo address this, we have three options:\n\nRemove duplicates if the number of duplicated points is small or the exact location is less important than the overall distribution.\nAssign weights to points, where each has an attribute indicating the number of events at that location rather than being recorded as separate event.\nAdd jitter by slightly offsetting the points randomly, which can be useful if precise location is not crucial for the analysis.\n\nEach approach has its own trade-offs, depending on the analysis. In our case, we will use the jitter approach to retain all bike theft events. Since the locations are already approximated, adding a small offset (~5 metre) will not impact the analysis.\n\n\n\nR code\n\n# add ajitter\ntheft_bike_jitter &lt;- rjitter(theft_bike_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for duplicates\nanyDuplicated(theft_bike_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(theft_bike_jitter) &gt; 1)\n\n[1] 0\n\n\nThis seemed to have worked, so we can move forward.\n\n\nInstead of visualising the distribution of bike thefts at a specific geographical level, we can use Kernel Density Estimation (KDE) to display the distribution of these incidents. KDE is a statistical method that creates a smooth, continuous distribution to represent the density of the underlying pattern between data points.\n\n\n\n\n\n\nKernel Density Estimation (KDE) generates a raster surface that shows the estimated density of event points across space. Each cell represents the local density, highlighting areas of high or low concentration. KDE uses overlapping moving windows (defined by a kernel) and a bandwidth parameter, which controls the size of the window, influencing the smoothness of the resulting density surface. The kernel function can assign equal or weighted values to points, producing a grid of density values based on these local calculations.\n\n\n\nLet’s go ahead and create a simple KDE of bike theft with our bandwidth set to 500 metres:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 500), main = \"\")\n\n\n\n\n\nFigure 3: Kernel density estimation - bandwidth 500m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around Central London. We can go ahead and increase the bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 1000), main = \"\")\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 1000m.\n\n\n\n\nBy increasing the bandwidth, our clusters appear larger and brighter than with the 500-metre bandwidth. A larger bandwidth considers more points, resulting in a smoother surface. However, this can lead to oversmoothing, where clusters become less defined, potentially overestimating areas of high bike theft. Smaller bandwidths offer more precision and sharper clusters but risk undersmoothing, which can cause irregularities.\n\n\n\n\n\n\nWhile automated methods (e.g. maximum-likelihood estimation) can assist in selecting an optimal bandwidth, the choice is subjective and depends on the specific characteristics of your dataset.\n\n\n\n\n\n\n\n\n\nAlthough bandwidth has a greater impact on density estimation than the kernel type, the choice of kernel can still influence the results by altering how points are weighted within the window. We will explore kernel types a little further when we discuss spatial models in a few weeks time.\n\n\n\nOnce we are satisfied with our KDE visualisation, we can create a proper map by converting the KDE output into raster format.\n\n\n\nR code\n\n# to raster\ntheft_bike_raster &lt;- density.ppp(theft_bike_jitter, sigma = 1000) |&gt;\n    rast()\n\n\nWe now have a standalone raster that we can use with any function in the tmap library. However, one issue is that the resulting raster lacks a Coordinate Reference System (CRS), so we need to manually assign this information to the raster object:\n\n\n\nR code\n\n# set CRS\ncrs(theft_bike_raster) &lt;- \"EPSG:27700\"\n\n\nNow we can map the KDE values.\n\n\n\nR code\n\n# shape\ntm_shape(theft_bike_raster) +\n\n  # map data\n  tm_raster(\n    # map data\n    col = \"lyr.1\",\n    col.scale = tm_scale(\n      values = \"brewer.blues\"\n    ),\n\n    # legend\n    col.legend = tm_legend(\n      title = \"Density\",\n      frame = FALSE,\n    )\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.8, 0.35),\n    legend.text.size = 0.8,\n\n    # canvas\n    frame = FALSE\n  )\n\n\n\n\n\nFigure 5: Kernel Density Estimate of bicycle thefts in London.\n\n\n\n\n\n\n\n\n\n\nThe values of the KDE output are stored in the raster grid as lyr.1.\n\n\n\n\n\n\nKernel Density Estimation is a useful exploratory technique for identifying spatial clusters in point data, but it does not provide precise boundaries for these clusters. To more accurately delineate clusters, we can use an algorithm called DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which takes both distance and density into account. DBSCAN is effective at discovering distinct clusters by grouping together points that are close to one another while marking points that don’t belong to any cluster as noise.\nDBSCAN requires two parameters:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nepsilon\nThe maximum distance for points to be considered in the same cluster.\n\n\nminPts\nThe minimum number of points for a cluster.\n\n\n\nThe algorithm groups nearby points based on these parameters and marks low-density points as outliers. DBSCAN is useful for uncovering patterns that are difficult to detect visually, but it works best when clusters have consistent densities.\nLet us try this with an epsilon of 200 metres and minPts of 20 bicycle thefts:\n\n\n\nR code\n\n# dbscan\nbike_theft_dbscan &lt;- theft_bike |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function accepts a data matrix or dataframe of points, not a spatial dataframe. That is why, in the code above, we use the st_coordinates() function to extract the projected coordinates from the spatial dataframe.\n\n\n\nThe DBSCAN output includes three objects, one of which is a vector detailing the cluster each bike theft observation has been assigned to. To work with this output effectively, we need to add the cluster labels back to the original point dataset. Since DBSCAN does not alter the order of points, we can simply add the cluster output to the theft_bike spatial dataframe.\n\n\n\nR code\n\n# add cluster numbers\ntheft_bike &lt;- theft_bike |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow that each bike theft point in London is associated with a specific cluster, where appropriate, we can generate a polygon representing these clusters. To do this, we will use the st_convex_hull() function from the sf package, which creates a polygon that covers the minimum bounding area of a collection of points. We will apply this function to each cluster using a for loop, which allows us to repeat the process for each group of points and create a polygon representing the geometry of each cluster.\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(theft_bike$dbcluster))\n\n# begin loop\nfor (cluster_index in seq(1, max(theft_bike$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    theft_bike_subset &lt;- theft_bike |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- theft_bike_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[cluster_index] &lt;- (cluster_polygon)\n\n}\n\n# combine the list\ntheft_bike_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\n\n\n\n\n\n\nWhile loops in R should generally be avoided for large datasets due to inefficiency, they remain a useful tool for automating repetitive tasks and reducing the risk of errors. For smaller datasets or tasks that cannot easily be vectorised, loops can still be effective and simplify the code.\n\n\n\nWe now have a spatial dataframe that contains the bike theft clusters in London, as defined by the DBSCAN clustering algorithm. Let’s quickly map these clusters:\n\n\n\nR code\n\n# shape\ntm_shape(outline) +\n\n  # map data\n  tm_polygons(\n    fill = \"#f0f0f0\",\n    col = NA\n  ) +\n\n  # shape\n  tm_shape(theft_bike) +\n\n  # map data\n  tm_symbols(\n    size = 0.10,\n    fill = \"#636363\",\n    col = \"#636363\",\n  ) +\n\n  # shape\n  tm_shape(theft_bike_clusters) +\n\n  # map data\n  tm_polygons(\n    col = \"#fdc086\",\n    fill = \"#fdc086\",\n    fill_alpha = 0.7\n  ) +\n\n  # layout\n  tm_layout(\n    frame = FALSE,\n  )\n\n\n\n\n\nFigure 6: DBSCAN-identified clusters of reported bicycle theft in London."
  },
  {
    "objectID": "03-point-pattern.html#assignment",
    "href": "03-point-pattern.html#assignment",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "Now that we know how to work with point locatoin data, we can again apply a similar analysis to road crashes in London in 2022 that we used last week. This time we will use this dataset to assess whether road crashes cluster in specific areas. Try the following:\n\nCreate a Kernel Density Estimation (KDE) of all road crashes that occurred in London in 2022.\nUsing DBSCAN output, create a cluster map of serious and fatal road crashes in London\n\nIf you no longer have a copy of the 2022 London STATS19 Road Collision dataset, you can download it using the link provided below.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon STATS19 Road Collisions 2022\ncsv\nDownload"
  },
  {
    "objectID": "03-point-pattern.html#before-you-leave",
    "href": "03-point-pattern.html#before-you-leave",
    "title": "1 Point Pattern Analysis",
    "section": "",
    "text": "With access to point event data, geographers aim to identify underlying patterns. This week, we explored several techniques that help us analyse and interpret such data. That is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "01-spatial.html",
    "href": "01-spatial.html",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "This week’s lecture offered a comprehensive introduction to the Geocomputation module, highlighting how and why it differs from a traditional GIScience course. In this week’s tutorial, we will introduce you to using R and RStudio for working with spatial data, focusing specifically on how R can be used to make maps.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nBrunsdon, C. and Comber, A. 2021. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\n\n\n\n\n\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nFranklin, S., Houlden, V., Robinson, C. et al. 2021. Who counts? Gender, Gatekeeping, and Quantitative Human Geography. The Professional Geographer 73(1): 48-61. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]\n\n\n\n\n\nIn RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis.\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0030, and select the location on your computer where you want to save this project by clicking on Browse…. Click on Create Project.\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0030 project. For example, you might use your Geocomputation folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nYou should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0030.\nWith our GEOG0030 project ready to go, in this first tutorial we will look at the distribution of the share of European immigrants across London. The data covers the number of people residing in London that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the Middle Layer Super Output Area (MSOA) level.\n\n\n\n\n\n\nAn MSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 5,000 to 15,000 people and is designed to ensure consistent data reporting. MSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the file in your project folder under data/attributes. Along with this dataset, we also have access to a GeoPackage that contains the MSOA boundaries. Save this file under data/spatial, respectively.\n\n\n\n\n\n\nYou will to have create a folder named data within your RStudio Project directory, inside which you will have to have a folder named attributes and a folder named spatial.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA Census 2021 European Population\ncsv\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w01-european-population-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, the installation of the sf library may fail because additional (non-R) libraries are required which are automatically installed for Windows users. If you encounter installation issues,, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load attribute dataset\nmsoa_eur &lt;- read_csv(\"data/attributes/London-MSOA-European.csv\")\n\nRows: 1002 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): msoa21cd\ndbl (2): eur21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(msoa_eur)\n\n# A tibble: 6 × 3\n  msoa21cd  eur21 pop21\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E02000001  1926  8582\n2 E02000002  1102  8280\n3 E02000003  1930 11542\n4 E02000004   808  6640\n5 E02000005  1541 11082\n6 E02000007  1365 10159\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nThe first thing we want to do when we load spatial data is to plot the data to check whether everything is in order. To do this, we can simply use the base R plot() function\n\n\n\nR code\n\n# plot data\nplot(msoa21, max.plot = 1, main = \"\")\n\n\n\n\n\nFigure 1: Quick plot to inspect the MSOA spatial data.\n\n\n\n\nYou should see your msoa21 plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the attributes of the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 9\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n\nWe can further establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(msoa21)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want.\n\n\n\nNow we have our dataset containing London’s European born population and the MSOA spatial boundaries loaded, we can join these together using an Attribute Join. Before proceeding with the join, we need to verify that a matching unique identifier exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(msoa21)\n\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n# inspect column names\nnames(msoa_eur)\n\n[1] \"msoa21cd\" \"eur21\"    \"pop21\"   \n\n\nThe msoa21cd columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect spatial dataset\nhead(sort(msoa21$msoa21cd))\n\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n# inspect attribute dataset\nhead(sort(msoa_eur$msoa21cd))\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\nmsoa21 &lt;- msoa21 |&gt; \n  left_join(msoa_eur, by = c('msoa21cd' = 'msoa21cd'))\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 11\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid eur21 pop21\n1 {71249043-B176-4306-BA6C-D1A993B1B741}  1926  8582\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E}  1102  8280\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B}  1930 11542\n4 {511181CD-E71F-4C63-81EE-E8E76744A627}   808  6640\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87}  1541 11082\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1}  1365 10159\n                            geom\n1 MULTIPOLYGON (((532135.1 18...\n2 MULTIPOLYGON (((548881.6 19...\n3 MULTIPOLYGON (((549102.4 18...\n4 MULTIPOLYGON (((551550.1 18...\n5 MULTIPOLYGON (((549099.6 18...\n6 MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n [1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n [7] \"long\"      \"globalid\"  \"eur21\"     \"pop21\"     \"geom\"     \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA.\nWe can also compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(msoa21)\n\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$eur21))\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$pop21))\n\n[1] 1002\n\n\nNo missing values. In this case we did not expect any missing values, so this confirms that all our full attribute dataset has been linked to the spatial dataset.\nWe are almost ready to map the data. Only thing that is left is for us to calculate the share of European-born immigrants within each MSOA:\n\n\n\nR code\n\n# calculate proportion\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(prop_eur21 = eur21/pop21)\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and proportional symbol maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the MSOA borders to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\", col = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information — such as polygons for our data. This layer function tells R to “draw my spatial object as X”, where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our msoa21 spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are coloured based on the proportion of European immigrants. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe fill parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red or #fc9272).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our prop_eur21 variable within the fill() parameter and see what we get:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    fill = \"prop_eur21\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfill.scale\nDefines the color scale for polygon fills. Accepts a scale object created by functions such as tm_scale_continuous() and tm_intervals()\n\n\ntm_scale_continuous()\nCreates a continuous scale object for mapping numeric values to colours. You can specify options such as palette (color scheme) and limits (data range).\n\n\ntm_scale_intervals()\nCreates a scale object for mapping data into discrete classes (intervals). Parameters include: n (number of classes) and style (classification method: e.g. quantile, equal, jenks)\n\n\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to specify the n and style parameters as follows:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(n = 5, style = \"jenks\"),\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\n\n\n\n\nStyling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntm_layout()\nCustomises overall map layout, including titles, fonts, legend position, margins, and frame settings.\n\n\ntm_compass()\nAdds a compass or North arrow to the map, with options for size, position, and style.\n\n\ntm_scale_bar()\nAdds a scale bar to indicate distance, with options for units, position, and styling.\n\n\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n      labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # title\n  tm_title(\n    text = \"Share of population born in Europe\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.90, 1.00),\n    legend.title.size = 0.8,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.8,\n\n    # canvas\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    frame = FALSE,\n  ) +\n\n  # North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # scale bar\n  tm_scalebar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(0.85, 0.20),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as separate map layer:\n\n\n\nR code\n\n# map labels\nlab &lt;- msoa21 |&gt;\n  filter(msoa21cd == \"E02000642\" | msoa21cd == \"E02000180\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\nlon_eurpop &lt;-\n  # shape\n  tm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n      labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # shape\n  tm_shape(lab) +\n\n  # centroids\n  tm_symbols(\n    size = 0.4,\n    col = \"#000000\",\n    fill = \"#000000\"\n  ) +\n\n  # labels\n  tm_text(\n    text = \"msoa21nm\",\n    col = \"#000000\",\n    size = 0.8,\n    xmod = 0,\n    ymod = -0.8,\n    bgcol = \"#f0f0f0\",\n    bgcol_alpha = 0.5\n  ) +\n\n  # title\n  tm_title(\n    text = \"Share of population born in Europe\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.90, 1.00),\n    legend.title.size = 0.8,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.8,\n\n    # canvas\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    frame = FALSE,\n  ) +\n\n  # North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # scale bar\n  tm_scalebar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(0.85, 0.20),\n    text.size = 0.4\n  )\n\n# plot\nlon_eurpop\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = lon_eurpop, filename = \"london-european-population.jpg\", width = 15,\n    height = 15, units = c(\"cm\"))\n\n\n\n\n\nNow that we have prepared our dataset and created our initial maps in R, we can also try and map the distribution of the proportion of European immigrants across Wales and experiment with different mapping parameters. Follow these steps:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of people residing in Wales that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the MSOA level.\nA GeoPackage file containing the 2021 MSOA spatial boundaries for England and Wales.\n\nLoad both datasets and merge them together. Make sure to only retain those MSOAs that belong to Wales.\nCreate a map that shows the proportion of the population residing in Wales that is born in Europe.\nExperiment by adjusting various map parameters, such as the colour scheme, map labels, and data classification method.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWales MSOA Census 2021 European Population\ncsv\nDownload\n\n\nEngland and Wales MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\nAnd that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week."
  },
  {
    "objectID": "01-spatial.html#lecture-slides",
    "href": "01-spatial.html#lecture-slides",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "01-spatial.html#reading-list",
    "href": "01-spatial.html#reading-list",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "Brunsdon, C. and Comber, A. 2021. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\n\n\n\n\n\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nFranklin, S., Houlden, V., Robinson, C. et al. 2021. Who counts? Gender, Gatekeeping, and Quantitative Human Geography. The Professional Geographer 73(1): 48-61. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]"
  },
  {
    "objectID": "01-spatial.html#europeans-in-london",
    "href": "01-spatial.html#europeans-in-london",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "In RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis.\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0030, and select the location on your computer where you want to save this project by clicking on Browse…. Click on Create Project.\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0030 project. For example, you might use your Geocomputation folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nYou should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0030.\nWith our GEOG0030 project ready to go, in this first tutorial we will look at the distribution of the share of European immigrants across London. The data covers the number of people residing in London that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the Middle Layer Super Output Area (MSOA) level.\n\n\n\n\n\n\nAn MSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 5,000 to 15,000 people and is designed to ensure consistent data reporting. MSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the file in your project folder under data/attributes. Along with this dataset, we also have access to a GeoPackage that contains the MSOA boundaries. Save this file under data/spatial, respectively.\n\n\n\n\n\n\nYou will to have create a folder named data within your RStudio Project directory, inside which you will have to have a folder named attributes and a folder named spatial.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon MSOA Census 2021 European Population\ncsv\nDownload\n\n\nLondon MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w01-european-population-london.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, the installation of the sf library may fail because additional (non-R) libraries are required which are automatically installed for Windows users. If you encounter installation issues,, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# read spatial dataset\nmsoa21 &lt;- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n\n\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# load attribute dataset\nmsoa_eur &lt;- read_csv(\"data/attributes/London-MSOA-European.csv\")\n\nRows: 1002 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): msoa21cd\ndbl (2): eur21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect\nhead(msoa_eur)\n\n# A tibble: 6 × 3\n  msoa21cd  eur21 pop21\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E02000001  1926  8582\n2 E02000002  1102  8280\n3 E02000003  1930 11542\n4 E02000004   808  6640\n5 E02000005  1541 11082\n6 E02000007  1365 10159\n\n\n\n\n\n\n\n\nYou can further inspect both objects using the View() function.\n\n\n\n\n\nThe first thing we want to do when we load spatial data is to plot the data to check whether everything is in order. To do this, we can simply use the base R plot() function\n\n\n\nR code\n\n# plot data\nplot(msoa21, max.plot = 1, main = \"\")\n\n\n\n\n\nFigure 1: Quick plot to inspect the MSOA spatial data.\n\n\n\n\nYou should see your msoa21 plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the attributes of the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 9\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n\nWe can further establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(msoa21)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want.\n\n\n\nNow we have our dataset containing London’s European born population and the MSOA spatial boundaries loaded, we can join these together using an Attribute Join. Before proceeding with the join, we need to verify that a matching unique identifier exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(msoa21)\n\n\n[1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n[7] \"long\"      \"globalid\"  \"geom\"     \n\n# inspect column names\nnames(msoa_eur)\n\n[1] \"msoa21cd\" \"eur21\"    \"pop21\"   \n\n\nThe msoa21cd columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect spatial dataset\nhead(sort(msoa21$msoa21cd))\n\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n# inspect attribute dataset\nhead(sort(msoa_eur$msoa21cd))\n\n[1] \"E02000001\" \"E02000002\" \"E02000003\" \"E02000004\" \"E02000005\" \"E02000007\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\nmsoa21 &lt;- msoa21 |&gt; \n  left_join(msoa_eur, by = c('msoa21cd' = 'msoa21cd'))\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(msoa21)\n\n\n[1] 11\n\n# inspect rows\nnrow(msoa21)\n\n[1] 1002\n\n# inspect data\nhead(msoa21)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid eur21 pop21\n1 {71249043-B176-4306-BA6C-D1A993B1B741}  1926  8582\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E}  1102  8280\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B}  1930 11542\n4 {511181CD-E71F-4C63-81EE-E8E76744A627}   808  6640\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87}  1541 11082\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1}  1365 10159\n                            geom\n1 MULTIPOLYGON (((532135.1 18...\n2 MULTIPOLYGON (((548881.6 19...\n3 MULTIPOLYGON (((549102.4 18...\n4 MULTIPOLYGON (((551550.1 18...\n5 MULTIPOLYGON (((549099.6 18...\n6 MULTIPOLYGON (((549819.9 18...\n\n# inspect column names\nnames(msoa21)\n\n [1] \"msoa21cd\"  \"msoa21nm\"  \"msoa21nmw\" \"bng_e\"     \"bng_n\"     \"lat\"      \n [7] \"long\"      \"globalid\"  \"eur21\"     \"pop21\"     \"geom\"     \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA.\nWe can also compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(msoa21)\n\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$eur21))\n\n[1] 1002\n\n# check for missing values\nsum(!is.na(msoa21$pop21))\n\n[1] 1002\n\n\nNo missing values. In this case we did not expect any missing values, so this confirms that all our full attribute dataset has been linked to the spatial dataset.\nWe are almost ready to map the data. Only thing that is left is for us to calculate the share of European-born immigrants within each MSOA:\n\n\n\nR code\n\n# calculate proportion\nmsoa21 &lt;- msoa21 |&gt;\n    mutate(prop_eur21 = eur21/pop21)\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and proportional symbol maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the MSOA borders to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(msoa21, fill = \"prop_eur21\", col = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information — such as polygons for our data. This layer function tells R to “draw my spatial object as X”, where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our msoa21 spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are coloured based on the proportion of European immigrants. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe fill parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red or #fc9272).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our prop_eur21 variable within the fill() parameter and see what we get:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    fill = \"prop_eur21\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfill.scale\nDefines the color scale for polygon fills. Accepts a scale object created by functions such as tm_scale_continuous() and tm_intervals()\n\n\ntm_scale_continuous()\nCreates a continuous scale object for mapping numeric values to colours. You can specify options such as palette (color scheme) and limits (data range).\n\n\ntm_scale_intervals()\nCreates a scale object for mapping data into discrete classes (intervals). Parameters include: n (number of classes) and style (classification method: e.g. quantile, equal, jenks)\n\n\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to specify the n and style parameters as follows:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(n = 5, style = \"jenks\"),\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer."
  },
  {
    "objectID": "01-spatial.html#styling-spatial-data",
    "href": "01-spatial.html#styling-spatial-data",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "Styling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntm_layout()\nCustomises overall map layout, including titles, fonts, legend position, margins, and frame settings.\n\n\ntm_compass()\nAdds a compass or North arrow to the map, with options for size, position, and style.\n\n\ntm_scale_bar()\nAdds a scale bar to indicate distance, with options for units, position, and styling.\n\n\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape\ntm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n      labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # title\n  tm_title(\n    text = \"Share of population born in Europe\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.90, 1.00),\n    legend.title.size = 0.8,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.8,\n\n    # canvas\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    frame = FALSE,\n  ) +\n\n  # North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # scale bar\n  tm_scalebar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(0.85, 0.20),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as separate map layer:\n\n\n\nR code\n\n# map labels\nlab &lt;- msoa21 |&gt;\n  filter(msoa21cd == \"E02000642\" | msoa21cd == \"E02000180\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\nlon_eurpop &lt;-\n  # shape\n  tm_shape(msoa21) +\n\n  # map data\n  tm_polygons(\n    # map data\n    fill = \"prop_eur21\",\n    fill.scale = tm_scale_intervals(\n      n = 5, style = \"jenks\",\n      values = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n      labels = c(\"Smallest share\", \"2nd smallest\", \"3rd smallest\", \"4th smallest\", \"Largest share\"),\n    ),\n\n    # legend\n    fill.legend = tm_legend(\n      title = \"Share of population\",\n      na.text = \"No population\",\n      frame = FALSE,\n    ),\n\n    # borders\n    col = \"#ffffff\",\n    col_alpha = 0.3\n  ) +\n\n  # shape\n  tm_shape(lab) +\n\n  # centroids\n  tm_symbols(\n    size = 0.4,\n    col = \"#000000\",\n    fill = \"#000000\"\n  ) +\n\n  # labels\n  tm_text(\n    text = \"msoa21nm\",\n    col = \"#000000\",\n    size = 0.8,\n    xmod = 0,\n    ymod = -0.8,\n    bgcol = \"#f0f0f0\",\n    bgcol_alpha = 0.5\n  ) +\n\n  # title\n  tm_title(\n    text = \"Share of population born in Europe\"\n  ) +\n\n  # layout\n  tm_layout(\n    # legend\n    legend.outside = FALSE,\n    legend.position = c(0.90, 1.00),\n    legend.title.size = 0.8,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.8,\n\n    # canvas\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    frame = FALSE,\n  ) +\n\n  # North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # scale bar\n  tm_scalebar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(0.85, 0.20),\n    text.size = 0.4\n  )\n\n# plot\nlon_eurpop\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = lon_eurpop, filename = \"london-european-population.jpg\", width = 15,\n    height = 15, units = c(\"cm\"))"
  },
  {
    "objectID": "01-spatial.html#assignment",
    "href": "01-spatial.html#assignment",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "Now that we have prepared our dataset and created our initial maps in R, we can also try and map the distribution of the proportion of European immigrants across Wales and experiment with different mapping parameters. Follow these steps:\n\nDownload the two datasets provided below and save them in the appropriate subfolder within your data directory. The datasets include:\n\nA csv file containing the number of people residing in Wales that are born in a European country, as recorded in the 2021 Census for England and Wales, aggregated at the MSOA level.\nA GeoPackage file containing the 2021 MSOA spatial boundaries for England and Wales.\n\nLoad both datasets and merge them together. Make sure to only retain those MSOAs that belong to Wales.\nCreate a map that shows the proportion of the population residing in Wales that is born in Europe.\nExperiment by adjusting various map parameters, such as the colour scheme, map labels, and data classification method.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWales MSOA Census 2021 European Population\ncsv\nDownload\n\n\nEngland and Wales MSOA 2021 Spatial Boundaries\nGeoPackage\nDownload"
  },
  {
    "objectID": "01-spatial.html#before-you-leave",
    "href": "01-spatial.html#before-you-leave",
    "title": "1 Reproducible Spatial Analysis",
    "section": "",
    "text": "And that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week."
  },
  {
    "objectID": "09-maps.html",
    "href": "09-maps.html",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "So far, we have primarily created univariate choropleth maps to visualise data across defined spatial areas, such as LSOAs. This week, we will expand on this by exploring bivariate maps, which illustrate the relationship between two variables within a single visualisation. We will also introduce you to the ggplot2 library and pay some attention to creating functions to automate and repeat analyses for different spatial units, allowing for more efficient and consistent workflows\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 1-32. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2021. Atlas of the invisible: maps and graphics that will change how you see the world. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 19: Functions. [Link]\n\n\n\n\n\nThis week, we will look at the change in unemployment across London between 2011 and 2021. Specifically, we will try to reconcile 2011 Census data with 2021 Census data and present the results on a bivariate map. The data cover all usual residents, as recorded in the 2011 and 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are periodically updated to reflect changes in population and other factors, resulting in occasional boundary adjustments. Consequently, it is essential to use the 2011 LSOA boundaries when mapping 2011 Census data and the 2021 LSOA boundaries for 2021 Census data. To facilitate mapping changes over time, we have access to a csv file containing a best-fit lookup table. This table provides a correspondence between 2011 LSOAs and their equivalent 2021 LSOAs, enabling consistent comparison across census periods.\n\n\n\nYou can download three files below and save them in your project folder under data/attributes. Along with these dataset, we also have access to a GeoPackage that contains the 2021 LSOA boundaries.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2011 Unemployment\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Unemployment\ncsv\nDownload\n\n\nEngland and Wales LSOA 2011-2021 Lookup\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w09-unemployment-change.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(biscale)\nlibrary(cowplot)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load all three files into memory:\n\n\n\nR code\n\n# read 2011 data\nlsoa11 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2011.csv\")\n\n\nRows: 4835 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa11cd, lsoa11nm\ndbl (2): eco_active_unemployed11, pop11\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read 2021 data\nlsoa21 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2021.csv\")\n\nRows: 4994 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, lsoa21nm\ndbl (2): eco_active_unemployed21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read lookup data\nlookup &lt;- read_csv(\"data/attributes/England-Wales-LSOA-2011-2021.csv\")\n\nRows: 35796 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): lsoa11cd, lsoa11nm, lsoa21cd, lsoa21nm, chgind\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa11)\n\n# A tibble: 6 × 4\n  lsoa11cd  lsoa11nm                  eco_active_unemployed11 pop11\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            34  1221\n2 E01000002 City of London 001B                            16  1196\n3 E01000003 City of London 001C                            39  1102\n4 E01000005 City of London 001E                            46   773\n5 E01000006 Barking and Dagenham 016A                      83  1251\n6 E01000007 Barking and Dagenham 015A                      87  1034\n\n# inspect\nhead(lsoa21)\n\n# A tibble: 6 × 4\n  lsoa21cd  lsoa21nm                  eco_active_unemployed21 pop21\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            32  1478\n2 E01000002 City of London 001B                            30  1383\n3 E01000003 City of London 001C                            68  1614\n4 E01000005 City of London 001E                            60  1099\n5 E01000006 Barking and Dagenham 016A                      57  1844\n6 E01000007 Barking and Dagenham 015A                     154  2908\n\n# inspect\nhead(lookup)\n\n# A tibble: 6 × 5\n  lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm                  chgind\n  &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt; \n1 E01000001 City of London 001A       E01000001 City of London 001A       U     \n2 E01000002 City of London 001B       E01000002 City of London 001B       U     \n3 E01000003 City of London 001C       E01000003 City of London 001C       U     \n4 E01000005 City of London 001E       E01000005 City of London 001E       U     \n5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A U     \n6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A U     \n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nTo analyse changes in unemployment over time, we need to combine the 2011 and 2021 unemployment data. Previously, we have joined datasets using a unique identifier found in both, assuming the identifiers match exactly and represent the same geographies. However, when comparing the unique identifiers from (lsoa11cd and lsoa21cd) these datasets, we can see some clear differences:\n\n\n\nR code\n\n# inspect\nlength(unique(lsoa11$lsoa11cd))\n\n\n[1] 4835\n\n# inspect\nlength(unique(lsoa21$lsoa21cd))\n\n[1] 4994\n\n\nThe number of LSOAs increased between the 2011 and 2021 Census due to boundary changes. Specifically, some 2011 LSOAs have been split into multiple 2021 LSOAs, while others have been merged into a single 2021 LSOA polygon. The relationship between 2011 and 2021 LSOAs is captured in the chgind column of the lookup table, which flags the type of change for each case.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nUnchanged: The LSOA boundaries remain the same from 2011 to 2021, allowing direct comparisons between data for these years.\n\n\nS\nSplit: A 2011 LSOA has been divided into two or more 2021 LSOAs. Each split 2021 LSOA will have a corresponding record in the table, enabling comparisons by aggregating the 2021 LSOA data back to the 2011 boundary.\n\n\nM\nMerged: Two or more 2011 LSOAs have been combined into a single 2021 LSOA. Comparisons can be made by aggregating the 2011 LSOA data to match the new 2021 boundary.\n\n\nX\nIrregular/Fragmented: The relationship between 2011 and 2021 LSOAs is complex due to redesigns from local authority boundary changes or efforts to improve social homogeneity. These cases do not allow straightforward comparisons between 2011 and 2021 data.\n\n\n\nAlthough there are different approaches to handling this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across the corresponding 2021 LSOAs.\nCombine the total crimes for 2011 LSOAs that have been merged into a single 2021 LSOA.\n\n\n\n\n\n\n\nThe LSOA boundary changes in London between 2011 and 2021 did not result in any irregular or fragmented boundaries. Therefore, we only need to address the merged and split LSOAs.\n\n\n\nThis means we will apply weightings to the values based on their relationships. We can prepare these weightings as follows:\n\n\n\nR code\n\n# for unchanged LSOAs keep weighting the same\nlsoa_lookup_same &lt;- lookup |&gt;\n    filter(chgind == \"U\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weighting the same\nlsoa_lookup_merge &lt;- lookup |&gt;\n    filter(chgind == \"M\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh proportionally to the number of 2021 LSOAs\nlsoa_lookup_split &lt;- lookup |&gt;\n    filter(chgind == \"S\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 6\n# Groups:   lsoa11cd [34,747]\n   lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm           chgind     n\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;  &lt;dbl&gt;\n 1 E01000001 City of London 001A       E01000001 City of London 00… U          1\n 2 E01000002 City of London 001B       E01000002 City of London 00… U          1\n 3 E01000003 City of London 001C       E01000003 City of London 00… U          1\n 4 E01000005 City of London 001E       E01000005 City of London 00… U          1\n 5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagen… U          1\n 6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagen… U          1\n 7 E01000008 Barking and Dagenham 015B E01000008 Barking and Dagen… U          1\n 8 E01000009 Barking and Dagenham 016B E01000009 Barking and Dagen… U          1\n 9 E01000011 Barking and Dagenham 016C E01000011 Barking and Dagen… U          1\n10 E01000012 Barking and Dagenham 015D E01000012 Barking and Dagen… U          1\n# ℹ 35,776 more rows\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nWe can now join the lookup table on the 2011 LSOA data:\n\n\n\nR code\n\n# join to lsoa data\nlsoa11_21 &lt;- lsoa11 |&gt;\n  select(-lsoa11nm) |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n\nIf we now compare the number of records in our lsoa11_21 dataset with the original 2011 and 2021 LSOA datasets, we notice some differences:\n\n\n\nR code\n\n# lsoa 2011\nnrow(lsoa11)\n\n\n[1] 4835\n\n# lsoa 2021\nnrow(lsoa21)\n\n[1] 4994\n\n# lookup\nnrow(lsoa11_21)\n\n[1] 5016\n\n\nSomehow, the number of our LSOAs seem to have increased. However, this is not an actual increase in LSOAs; rather, the change in the number of LSOAs is due to our one-to-many relationships. A single 2011 LSOA can correspond to multiple 2021 LSOAs, which causes the data for that 2011 LSOA to be duplicated in the join operation. Fortunately, we anticipated this and have already created the necessary weightings. We can now apply these weightings to assign our 2011 population estimates to the 2021 LSOA boundaries as follows:\n\n\n\nR code\n\n# weigh data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(eco_active_unemployed11 = eco_active_unemployed11 * n) |&gt;\n    mutate(pop11 = pop11 * n)\n\n# assign 2011 to 2021\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    group_by(lsoa21cd) |&gt;\n    mutate(eco_active_unemployed11_lsoa21 = sum(eco_active_unemployed11)) |&gt;\n    mutate(pop11_lsoa21 = sum(pop11)) |&gt;\n    distinct(lsoa21cd, eco_active_unemployed11_lsoa21, pop11_lsoa21)\n\n\nWe should now be left with all 2021 LSOAs, each containing the corresponding 2011 values, adjusted according to the merged and split LSOA relationships. We can quickly check this by comparing the original values with the re-assigned values:\n\n\n\nR code\n\n# inspect number\nnrow(lsoa21)\n\n\n[1] 4994\n\n# inspect number\nnrow(lsoa11_21)\n\n[1] 4994\n\n# inspect count original data\nsum(lsoa11$pop11)\n\n[1] 6117482\n\n# inspect count re-assigned data\nsum(lsoa11_21$pop11_lsoa21)\n\n[1] 6117482\n\n\nWe can now join the 2011 and 2021 population data together:\n\n\n\nR code\n\n# join 2011 data with 2021 data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n  left_join(lsoa21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n\n\n\n\nBivariate maps are visualisations that represent two different variables simultaneously on a single map, using combinations of colours, patterns, or symbols to convey relationships between them. They are commonly used to explore spatial correlations or patterns, such as comparing population density with income levels across a region. We will use a bivariate map to illustrate changes in unemployment between 2011 and 2021 in London.\nWe will start by calculating unemployment rates for both years and classifing them into categories using the biscale library:\n\n\n\nR code\n\n# unemployment rates\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(unemp11 = eco_active_unemployed11_lsoa21/pop11_lsoa21) |&gt;\n    mutate(unemp21 = eco_active_unemployed21/pop21) |&gt;\n    select(-lsoa21nm)\n\n# add classes\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# inspect\nhead(lsoa11_21$bi_class)\n\n\n[1] \"1-1\" \"1-1\" \"3-1\" \"3-2\" \"2-3\" \"3-3\"\n\n\n\n\n\n\n\n\nThe dim argument is used to control the extent of the legend. For instance, dim = 2 will produce a two-by-two map where dim = 3 will produce a three-by-three map.\n\n\n\nInstead of using tmap to create our map, we will need to use the ggplot2 library. Like tmap, ggplot2 is based on the grammar of graphics, allowing you to build a graphic step by step by layering components such as data, aesthetics, and geometries. While we will explore ggplot2 in more detail next week, for now, we will use it to create a bivariate map by adding the necessary layers one at a time.\n\n\n\n\n\n\nBivariate maps are not supported in Version 3 of tmap. However, Version 4, which is currently under development, will include functionality for creating bivariate maps. This new version is expected to be released on CRAN soon\n\n\n\nOnce breaks are created, we can use bi_scale_fill() as part of our ggplot() call:\n\n\n\nR code\n\n# load spatial data\nlsoa21_sf &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join unemployment data\nlsoa21_sf &lt;- lsoa21_sf |&gt;\n  left_join(lsoa11_21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# bivariate map using ggplot\nggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n\n\n\nFigure 1: Bivariate map change of unemployment rates in London 2011-2021.\n\n\n\n\nShades closer to grey indicate areas with relative low unemployment rates in both years, while shades closer to blue represent areas with high unemployment rates in both years. Mixed tones suggest areas where unemployment rates have changed between 2011 and 2021, with the specific colour intensity reflecting the degree and direction of this change.\nWe have set show.legend = FALSE to allow us to manually add our own bivariate legend. The palette and dimensions should align with those used in bi_class() for dimensions and bi_scale_fill() for both dimensions and palette to ensure consistency. We can create a legend and combine it with a map object as follows:\n\n\n\nR code\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0, 0, .3, 0.3)\n\n\n\n\n\nFigure 2: Bivariate map of relative changes in unemployment rates in London 2011-2021.\n\n\n\n\n\n\n\n\n\n\nThe values in the draw_plot() function specify the relative location and size of each map object on the canvas. Adjusting these values often requires some trial and error to achieve the desired positioning, as they control the x and y coordinates for placement and the width and height proportions of each object.\n\n\n\nWe have used LSOA data to create a bivariate map illustrating changes in unemployment rates. However, with nearly 5,000 LSOAs in London, this map can be challenging to interpret due to the high level of detail. Let’s zoom in to Lambeth:\n\n\n\nR code\n\n# select lambeth\nlsoa21_lambeth &lt;- lsoa21_sf |&gt;\n  filter(str_detect(lsoa21nm, \"Lambeth\"))\n\n# add classes\nlsoa21_lambeth &lt;- lsoa21_lambeth |&gt;\n  bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_lambeth,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nbivmap &lt;- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n# plot\nbivmap\n\n\n\n\n\nFigure 3: Bivariate map of relative changes in unemployment rates in Lambeth 2011-2021.\n\n\n\n\n\n\n\nSo we have now created a map of Lambeth. But what if we need to create a map for every borough in London? In R, you can create a basic function using the function() keyword, which allows you to encapsulate reusable code. A function can take arguments (inputs), perform operations, and return a result. A simple example of a function that adds two values together:\n\n\n\nR code\n\n# define function to add two numbers\nadd_numbers &lt;- function(a, b) {\n    result &lt;- a + b\n    return(result)\n}\n\n# use function\nadd_numbers(5, 3)\n\n\n[1] 8\n\n\nWe can use the same logic to construct a basic function that takes a spatial dataframe and the name of a borough as input and subsequently creates a bivarate map as output:\n\n\n\nR code\n\n# define function to create bivariate unemployment maps\ncreate_bivariate_map &lt;- function(spatial_df, borough_name) {\n  # select borough\n  spatial_df_filter &lt;- spatial_df |&gt;\n    filter(str_detect(lsoa21nm, borough_name))\n\n  # add classes\n  spatial_df_filter &lt;- spatial_df_filter |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n  # bivariate map object\n  map &lt;- ggplot() +\n    geom_sf(\n      data = spatial_df_filter,\n      mapping = aes(fill = bi_class),\n      color = NA,\n      show.legend = FALSE\n    ) +\n    bi_scale_fill(\n      pal = \"DkBlue2\",\n      dim = 3\n    ) +\n    bi_theme()\n\n  # legend object\n  legend &lt;- bi_legend(\n    pal = \"DkBlue2\",\n    dim = 3,\n    xlab = \"Higher Unemployment 2021\",\n    ylab = \"Higher Unemployment 2011\",\n    size = 6\n  )\n\n  # combine\n  bivariate_map &lt;- ggdraw() +\n    draw_plot(map, 0, 0, 1, 1) +\n    draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n  # return value\n  return(bivariate_map)\n}\n\n\nWe can now use this function to quickly recreate maps for individual boroughs. Let’s try it for the London Borough of Hammersmith:\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Hammersmith\")\n\n\n\n\n\nFigure 4: Bivariate map of relative changes in unemployment rates in Hammersmith 2011-2021.\n\n\n\n\nWhat about Kensington and Chelsea? Or Wandsworth?\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Kensington and Chelsea\")\n\n\n\n\n\nFigure 5: Bivariate map of relative changes in unemployment rates in Kensington and Chelsea 2011-2021.\n\n\n\n\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Wandsworth\")\n\n\n\n\n\nFigure 6: Bivariate map of relative changes in unemployment rates in Wandsworth 2011-2021.\n\n\n\n\n\n\n\n\nWhen the same action, such as mapping a particular variable, needs to be repeated across different datasets or regions, a function ensures that the process is consistent and can be applied easily without rewriting code. This not only saves time but also reduces the risk of errors, as you can simply call the function with different inputs, ensuring the same analysis steps are followed each time.\nHaving created a function to generate bivariate maps, we can now create a function for univariate maps. Using the dataset from the tutorial, try to:\n\nWrite a function with two parameters that uses the standard tmap library to map unemployment rates at the LSOA-level for a specified borough.\nAdd a third parameter that specifies which variable should be mapped (e.g., unemployment rates in 2011 or 2021).\nAdd a fourth parameter to define the colour palette to be used for the map.\n\n\n\n\n\n\n\nIf you would like a more comprehensive introduction to writing your own functions in R, refer to Chapter 19: Functions in R for Data Science. This chapter provides a detailed explanation of how to create and use functions, along with best practices for making your code more efficient and reusable.\n\n\n\n\n\n\nThat is it for today. You should now be able to use lookup tables, create bivariate maps with the ggplot2 library, and build basic reproducible functions. Next week, we will dive deeper into the ggplot2 library, but for now that is this week’s Geocompuation done!"
  },
  {
    "objectID": "09-maps.html#lecture-slides",
    "href": "09-maps.html#lecture-slides",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "09-maps.html#reading-list",
    "href": "09-maps.html#reading-list",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 1-32. [Link]\n\n\n\n\n\nCheshire, J. and Uberti, O. 2021. Atlas of the invisible: maps and graphics that will change how you see the world. London: Particular Books.\nWickham, H., Çetinkaya-Rundel, M., and Grolemund, G. R for Data Science. 2nd edition. Chapter 19: Functions. [Link]"
  },
  {
    "objectID": "09-maps.html#unemployment-in-london",
    "href": "09-maps.html#unemployment-in-london",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "This week, we will look at the change in unemployment across London between 2011 and 2021. Specifically, we will try to reconcile 2011 Census data with 2021 Census data and present the results on a bivariate map. The data cover all usual residents, as recorded in the 2011 and 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are periodically updated to reflect changes in population and other factors, resulting in occasional boundary adjustments. Consequently, it is essential to use the 2011 LSOA boundaries when mapping 2011 Census data and the 2021 LSOA boundaries for 2021 Census data. To facilitate mapping changes over time, we have access to a csv file containing a best-fit lookup table. This table provides a correspondence between 2011 LSOAs and their equivalent 2021 LSOAs, enabling consistent comparison across census periods.\n\n\n\nYou can download three files below and save them in your project folder under data/attributes. Along with these dataset, we also have access to a GeoPackage that contains the 2021 LSOA boundaries.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2011 Unemployment\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Unemployment\ncsv\nDownload\n\n\nEngland and Wales LSOA 2011-2021 Lookup\ncsv\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\nOpen a new script within your GEOG0030 project and save this as w09-unemployment-change.r.\nBegin by loading the necessary libraries:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(biscale)\nlibrary(cowplot)\n\n\n\n\n\n\n\n\nYou may have to install some of these libraries if you have not used these before.\n\n\n\nOnce downloaded, we can load all three files into memory:\n\n\n\nR code\n\n# read 2011 data\nlsoa11 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2011.csv\")\n\n\nRows: 4835 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa11cd, lsoa11nm\ndbl (2): eco_active_unemployed11, pop11\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read 2021 data\nlsoa21 &lt;- read_csv(\"data/attributes/London-LSOA-Unemployment-2021.csv\")\n\nRows: 4994 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, lsoa21nm\ndbl (2): eco_active_unemployed21, pop21\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read lookup data\nlookup &lt;- read_csv(\"data/attributes/England-Wales-LSOA-2011-2021.csv\")\n\nRows: 35796 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): lsoa11cd, lsoa11nm, lsoa21cd, lsoa21nm, chgind\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(lsoa11)\n\n# A tibble: 6 × 4\n  lsoa11cd  lsoa11nm                  eco_active_unemployed11 pop11\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            34  1221\n2 E01000002 City of London 001B                            16  1196\n3 E01000003 City of London 001C                            39  1102\n4 E01000005 City of London 001E                            46   773\n5 E01000006 Barking and Dagenham 016A                      83  1251\n6 E01000007 Barking and Dagenham 015A                      87  1034\n\n# inspect\nhead(lsoa21)\n\n# A tibble: 6 × 4\n  lsoa21cd  lsoa21nm                  eco_active_unemployed21 pop21\n  &lt;chr&gt;     &lt;chr&gt;                                       &lt;dbl&gt; &lt;dbl&gt;\n1 E01000001 City of London 001A                            32  1478\n2 E01000002 City of London 001B                            30  1383\n3 E01000003 City of London 001C                            68  1614\n4 E01000005 City of London 001E                            60  1099\n5 E01000006 Barking and Dagenham 016A                      57  1844\n6 E01000007 Barking and Dagenham 015A                     154  2908\n\n# inspect\nhead(lookup)\n\n# A tibble: 6 × 5\n  lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm                  chgind\n  &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt; \n1 E01000001 City of London 001A       E01000001 City of London 001A       U     \n2 E01000002 City of London 001B       E01000002 City of London 001B       U     \n3 E01000003 City of London 001C       E01000003 City of London 001C       U     \n4 E01000005 City of London 001E       E01000005 City of London 001E       U     \n5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A U     \n6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A U     \n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\nTo analyse changes in unemployment over time, we need to combine the 2011 and 2021 unemployment data. Previously, we have joined datasets using a unique identifier found in both, assuming the identifiers match exactly and represent the same geographies. However, when comparing the unique identifiers from (lsoa11cd and lsoa21cd) these datasets, we can see some clear differences:\n\n\n\nR code\n\n# inspect\nlength(unique(lsoa11$lsoa11cd))\n\n\n[1] 4835\n\n# inspect\nlength(unique(lsoa21$lsoa21cd))\n\n[1] 4994\n\n\nThe number of LSOAs increased between the 2011 and 2021 Census due to boundary changes. Specifically, some 2011 LSOAs have been split into multiple 2021 LSOAs, while others have been merged into a single 2021 LSOA polygon. The relationship between 2011 and 2021 LSOAs is captured in the chgind column of the lookup table, which flags the type of change for each case.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nUnchanged: The LSOA boundaries remain the same from 2011 to 2021, allowing direct comparisons between data for these years.\n\n\nS\nSplit: A 2011 LSOA has been divided into two or more 2021 LSOAs. Each split 2021 LSOA will have a corresponding record in the table, enabling comparisons by aggregating the 2021 LSOA data back to the 2011 boundary.\n\n\nM\nMerged: Two or more 2011 LSOAs have been combined into a single 2021 LSOA. Comparisons can be made by aggregating the 2011 LSOA data to match the new 2021 boundary.\n\n\nX\nIrregular/Fragmented: The relationship between 2011 and 2021 LSOAs is complex due to redesigns from local authority boundary changes or efforts to improve social homogeneity. These cases do not allow straightforward comparisons between 2011 and 2021 data.\n\n\n\nAlthough there are different approaches to handling this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across the corresponding 2021 LSOAs.\nCombine the total crimes for 2011 LSOAs that have been merged into a single 2021 LSOA.\n\n\n\n\n\n\n\nThe LSOA boundary changes in London between 2011 and 2021 did not result in any irregular or fragmented boundaries. Therefore, we only need to address the merged and split LSOAs.\n\n\n\nThis means we will apply weightings to the values based on their relationships. We can prepare these weightings as follows:\n\n\n\nR code\n\n# for unchanged LSOAs keep weighting the same\nlsoa_lookup_same &lt;- lookup |&gt;\n    filter(chgind == \"U\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weighting the same\nlsoa_lookup_merge &lt;- lookup |&gt;\n    filter(chgind == \"M\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh proportionally to the number of 2021 LSOAs\nlsoa_lookup_split &lt;- lookup |&gt;\n    filter(chgind == \"S\") |&gt;\n    group_by(lsoa11cd) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 6\n# Groups:   lsoa11cd [34,747]\n   lsoa11cd  lsoa11nm                  lsoa21cd  lsoa21nm           chgind     n\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;  &lt;dbl&gt;\n 1 E01000001 City of London 001A       E01000001 City of London 00… U          1\n 2 E01000002 City of London 001B       E01000002 City of London 00… U          1\n 3 E01000003 City of London 001C       E01000003 City of London 00… U          1\n 4 E01000005 City of London 001E       E01000005 City of London 00… U          1\n 5 E01000006 Barking and Dagenham 016A E01000006 Barking and Dagen… U          1\n 6 E01000007 Barking and Dagenham 015A E01000007 Barking and Dagen… U          1\n 7 E01000008 Barking and Dagenham 015B E01000008 Barking and Dagen… U          1\n 8 E01000009 Barking and Dagenham 016B E01000009 Barking and Dagen… U          1\n 9 E01000011 Barking and Dagenham 016C E01000011 Barking and Dagen… U          1\n10 E01000012 Barking and Dagenham 015D E01000012 Barking and Dagen… U          1\n# ℹ 35,776 more rows\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\nWe can now join the lookup table on the 2011 LSOA data:\n\n\n\nR code\n\n# join to lsoa data\nlsoa11_21 &lt;- lsoa11 |&gt;\n  select(-lsoa11nm) |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa11cd\" = \"lsoa11cd\"))\n\n\nIf we now compare the number of records in our lsoa11_21 dataset with the original 2011 and 2021 LSOA datasets, we notice some differences:\n\n\n\nR code\n\n# lsoa 2011\nnrow(lsoa11)\n\n\n[1] 4835\n\n# lsoa 2021\nnrow(lsoa21)\n\n[1] 4994\n\n# lookup\nnrow(lsoa11_21)\n\n[1] 5016\n\n\nSomehow, the number of our LSOAs seem to have increased. However, this is not an actual increase in LSOAs; rather, the change in the number of LSOAs is due to our one-to-many relationships. A single 2011 LSOA can correspond to multiple 2021 LSOAs, which causes the data for that 2011 LSOA to be duplicated in the join operation. Fortunately, we anticipated this and have already created the necessary weightings. We can now apply these weightings to assign our 2011 population estimates to the 2021 LSOA boundaries as follows:\n\n\n\nR code\n\n# weigh data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(eco_active_unemployed11 = eco_active_unemployed11 * n) |&gt;\n    mutate(pop11 = pop11 * n)\n\n# assign 2011 to 2021\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    group_by(lsoa21cd) |&gt;\n    mutate(eco_active_unemployed11_lsoa21 = sum(eco_active_unemployed11)) |&gt;\n    mutate(pop11_lsoa21 = sum(pop11)) |&gt;\n    distinct(lsoa21cd, eco_active_unemployed11_lsoa21, pop11_lsoa21)\n\n\nWe should now be left with all 2021 LSOAs, each containing the corresponding 2011 values, adjusted according to the merged and split LSOA relationships. We can quickly check this by comparing the original values with the re-assigned values:\n\n\n\nR code\n\n# inspect number\nnrow(lsoa21)\n\n\n[1] 4994\n\n# inspect number\nnrow(lsoa11_21)\n\n[1] 4994\n\n# inspect count original data\nsum(lsoa11$pop11)\n\n[1] 6117482\n\n# inspect count re-assigned data\nsum(lsoa11_21$pop11_lsoa21)\n\n[1] 6117482\n\n\nWe can now join the 2011 and 2021 population data together:\n\n\n\nR code\n\n# join 2011 data with 2021 data\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n  left_join(lsoa21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n\n\n\n\nBivariate maps are visualisations that represent two different variables simultaneously on a single map, using combinations of colours, patterns, or symbols to convey relationships between them. They are commonly used to explore spatial correlations or patterns, such as comparing population density with income levels across a region. We will use a bivariate map to illustrate changes in unemployment between 2011 and 2021 in London.\nWe will start by calculating unemployment rates for both years and classifing them into categories using the biscale library:\n\n\n\nR code\n\n# unemployment rates\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    mutate(unemp11 = eco_active_unemployed11_lsoa21/pop11_lsoa21) |&gt;\n    mutate(unemp21 = eco_active_unemployed21/pop21) |&gt;\n    select(-lsoa21nm)\n\n# add classes\nlsoa11_21 &lt;- lsoa11_21 |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# inspect\nhead(lsoa11_21$bi_class)\n\n\n[1] \"1-1\" \"1-1\" \"3-1\" \"3-2\" \"2-3\" \"3-3\"\n\n\n\n\n\n\n\n\nThe dim argument is used to control the extent of the legend. For instance, dim = 2 will produce a two-by-two map where dim = 3 will produce a three-by-three map.\n\n\n\nInstead of using tmap to create our map, we will need to use the ggplot2 library. Like tmap, ggplot2 is based on the grammar of graphics, allowing you to build a graphic step by step by layering components such as data, aesthetics, and geometries. While we will explore ggplot2 in more detail next week, for now, we will use it to create a bivariate map by adding the necessary layers one at a time.\n\n\n\n\n\n\nBivariate maps are not supported in Version 3 of tmap. However, Version 4, which is currently under development, will include functionality for creating bivariate maps. This new version is expected to be released on CRAN soon\n\n\n\nOnce breaks are created, we can use bi_scale_fill() as part of our ggplot() call:\n\n\n\nR code\n\n# load spatial data\nlsoa21_sf &lt;- st_read(\"data/spatial/London-LSOA-2021.gpkg\")\n\n\nReading layer `London-LSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-LSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4994 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# join unemployment data\nlsoa21_sf &lt;- lsoa21_sf |&gt;\n  left_join(lsoa11_21, by = c(\"lsoa21cd\" = \"lsoa21cd\"))\n\n# bivariate map using ggplot\nggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n\n\n\nFigure 1: Bivariate map change of unemployment rates in London 2011-2021.\n\n\n\n\nShades closer to grey indicate areas with relative low unemployment rates in both years, while shades closer to blue represent areas with high unemployment rates in both years. Mixed tones suggest areas where unemployment rates have changed between 2011 and 2021, with the specific colour intensity reflecting the degree and direction of this change.\nWe have set show.legend = FALSE to allow us to manually add our own bivariate legend. The palette and dimensions should align with those used in bi_class() for dimensions and bi_scale_fill() for both dimensions and palette to ensure consistency. We can create a legend and combine it with a map object as follows:\n\n\n\nR code\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_sf,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0, 0, .3, 0.3)\n\n\n\n\n\nFigure 2: Bivariate map of relative changes in unemployment rates in London 2011-2021.\n\n\n\n\n\n\n\n\n\n\nThe values in the draw_plot() function specify the relative location and size of each map object on the canvas. Adjusting these values often requires some trial and error to achieve the desired positioning, as they control the x and y coordinates for placement and the width and height proportions of each object.\n\n\n\nWe have used LSOA data to create a bivariate map illustrating changes in unemployment rates. However, with nearly 5,000 LSOAs in London, this map can be challenging to interpret due to the high level of detail. Let’s zoom in to Lambeth:\n\n\n\nR code\n\n# select lambeth\nlsoa21_lambeth &lt;- lsoa21_sf |&gt;\n  filter(str_detect(lsoa21nm, \"Lambeth\"))\n\n# add classes\nlsoa21_lambeth &lt;- lsoa21_lambeth |&gt;\n  bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n# bivariate map object\nmap &lt;- ggplot() +\n  geom_sf(\n    data = lsoa21_lambeth,\n    mapping = aes(fill = bi_class),\n    color = NA,\n    show.legend = FALSE\n  ) +\n  bi_scale_fill(\n    pal = \"DkBlue2\",\n    dim = 3\n  ) +\n  bi_theme()\n\n# legend object\nlegend &lt;- bi_legend(\n  pal = \"DkBlue2\",\n  dim = 3,\n  xlab = \"Higher Unemployment 2021\",\n  ylab = \"Higher Unemployment 2011\",\n  size = 6\n)\n\n# combine, draw\nbivmap &lt;- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n# plot\nbivmap\n\n\n\n\n\nFigure 3: Bivariate map of relative changes in unemployment rates in Lambeth 2011-2021.\n\n\n\n\n\n\n\nSo we have now created a map of Lambeth. But what if we need to create a map for every borough in London? In R, you can create a basic function using the function() keyword, which allows you to encapsulate reusable code. A function can take arguments (inputs), perform operations, and return a result. A simple example of a function that adds two values together:\n\n\n\nR code\n\n# define function to add two numbers\nadd_numbers &lt;- function(a, b) {\n    result &lt;- a + b\n    return(result)\n}\n\n# use function\nadd_numbers(5, 3)\n\n\n[1] 8\n\n\nWe can use the same logic to construct a basic function that takes a spatial dataframe and the name of a borough as input and subsequently creates a bivarate map as output:\n\n\n\nR code\n\n# define function to create bivariate unemployment maps\ncreate_bivariate_map &lt;- function(spatial_df, borough_name) {\n  # select borough\n  spatial_df_filter &lt;- spatial_df |&gt;\n    filter(str_detect(lsoa21nm, borough_name))\n\n  # add classes\n  spatial_df_filter &lt;- spatial_df_filter |&gt;\n    bi_class(x = unemp21, y = unemp11, style = \"quantile\", dim = 3)\n\n  # bivariate map object\n  map &lt;- ggplot() +\n    geom_sf(\n      data = spatial_df_filter,\n      mapping = aes(fill = bi_class),\n      color = NA,\n      show.legend = FALSE\n    ) +\n    bi_scale_fill(\n      pal = \"DkBlue2\",\n      dim = 3\n    ) +\n    bi_theme()\n\n  # legend object\n  legend &lt;- bi_legend(\n    pal = \"DkBlue2\",\n    dim = 3,\n    xlab = \"Higher Unemployment 2021\",\n    ylab = \"Higher Unemployment 2011\",\n    size = 6\n  )\n\n  # combine\n  bivariate_map &lt;- ggdraw() +\n    draw_plot(map, 0, 0, 1, 1) +\n    draw_plot(legend, 0.1, 0.1, 0.3, 0.3)\n\n  # return value\n  return(bivariate_map)\n}\n\n\nWe can now use this function to quickly recreate maps for individual boroughs. Let’s try it for the London Borough of Hammersmith:\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Hammersmith\")\n\n\n\n\n\nFigure 4: Bivariate map of relative changes in unemployment rates in Hammersmith 2011-2021.\n\n\n\n\nWhat about Kensington and Chelsea? Or Wandsworth?\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Kensington and Chelsea\")\n\n\n\n\n\nFigure 5: Bivariate map of relative changes in unemployment rates in Kensington and Chelsea 2011-2021.\n\n\n\n\n\n\n\nR code\n\n# run function\ncreate_bivariate_map(lsoa21_sf, \"Wandsworth\")\n\n\n\n\n\nFigure 6: Bivariate map of relative changes in unemployment rates in Wandsworth 2011-2021."
  },
  {
    "objectID": "09-maps.html#assignment",
    "href": "09-maps.html#assignment",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "When the same action, such as mapping a particular variable, needs to be repeated across different datasets or regions, a function ensures that the process is consistent and can be applied easily without rewriting code. This not only saves time but also reduces the risk of errors, as you can simply call the function with different inputs, ensuring the same analysis steps are followed each time.\nHaving created a function to generate bivariate maps, we can now create a function for univariate maps. Using the dataset from the tutorial, try to:\n\nWrite a function with two parameters that uses the standard tmap library to map unemployment rates at the LSOA-level for a specified borough.\nAdd a third parameter that specifies which variable should be mapped (e.g., unemployment rates in 2011 or 2021).\nAdd a fourth parameter to define the colour palette to be used for the map.\n\n\n\n\n\n\n\nIf you would like a more comprehensive introduction to writing your own functions in R, refer to Chapter 19: Functions in R for Data Science. This chapter provides a detailed explanation of how to create and use functions, along with best practices for making your code more efficient and reusable."
  },
  {
    "objectID": "09-maps.html#before-you-leave",
    "href": "09-maps.html#before-you-leave",
    "title": "1 Beyond the Choropleth",
    "section": "",
    "text": "That is it for today. You should now be able to use lookup tables, create bivariate maps with the ggplot2 library, and build basic reproducible functions. Next week, we will dive deeper into the ggplot2 library, but for now that is this week’s Geocompuation done!"
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis and data visualisation while providing a thorough introduction to reproducible research. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, cleaning, analysing and presenting spatial, demographic, and socioeconomic datasets.\n\n\n\nPlease be aware that for this module you are expected to have access to a working R v4.5 installation and have a basic level of proficiency in programming with R. This includes skills such as installing libraries, loading data, calculating variables, and reshaping data.\n\n\n\n\n\n\nFor installation instructions and a refresher, please refer to the Getting started and R for Data Analysis tutorials in the GEOG0018: Methods in Human Geography workbook.\n\n\n\n\n\n\nMoodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial.\n\n\n\nThe topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nReproducible Spatial Analysis\n\n\n2\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n3\nCore Spatial Analysis\nPoint Pattern Analysis\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nBeyond the Choropleth\n\n\n10\nData Visualisation\nComplex Visualisations\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2025-2026 academic year. The content for 2024-2025 has been archived and can be found here: [Link]\n\n\n\n\n\n\nFor specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook into the Academic Support and Feedback hours.\n\n\n\n\n\n\n\n\n\n\nThis year’s module material features the following major updates:\n\nAll code has been revised to ensure compatibility with tmap v4.\nPackage management using renv has been removed due to issues where sf installed within the package manager would not bind correctly to GDAL.\n\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "Geocomputation",
    "section": "",
    "text": "Welcome to Geocomputation. This module offers a deep dive into the principles of spatial analysis and data visualisation while providing a thorough introduction to reproducible research. Over the next ten weeks, you will explore the theory, methods, and tools of spatial analysis through engaging case studies. You will gain hands-on experience in sourcing, managing, cleaning, analysing and presenting spatial, demographic, and socioeconomic datasets."
  },
  {
    "objectID": "00-index.html#prerequisites",
    "href": "00-index.html#prerequisites",
    "title": "Geocomputation",
    "section": "",
    "text": "Please be aware that for this module you are expected to have access to a working R v4.5 installation and have a basic level of proficiency in programming with R. This includes skills such as installing libraries, loading data, calculating variables, and reshaping data.\n\n\n\n\n\n\nFor installation instructions and a refresher, please refer to the Getting started and R for Data Analysis tutorials in the GEOG0018: Methods in Human Geography workbook."
  },
  {
    "objectID": "00-index.html#moodle",
    "href": "00-index.html#moodle",
    "title": "Geocomputation",
    "section": "",
    "text": "Moodle serves as the central hub for GEOG0030, where you will find all essential module information, including key details about assessments. This workbook provides links to all required reading materials and contains the content for each computer tutorial."
  },
  {
    "objectID": "00-index.html#module-overview",
    "href": "00-index.html#module-overview",
    "title": "Geocomputation",
    "section": "",
    "text": "The topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nCore Spatial Analysis\nReproducible Spatial Analysis\n\n\n2\nCore Spatial Analysis\nSpatial Queries and Geometric Operations\n\n\n3\nCore Spatial Analysis\nPoint Pattern Analysis\n\n\n4\nCore Spatial Analysis\nSpatial Autocorrelation\n\n\n5\nCore Spatial Analysis\nSpatial Models\n\n\n\nReading week\nReading week\n\n\n6\nApplied Spatial Analysis\nRaster Data Analysis\n\n\n7\nApplied Spatial Analysis\nGeodemographic Classification\n\n\n8\nApplied Spatial Analysis\nAccessibility Analysis\n\n\n9\nData Visualisation\nBeyond the Choropleth\n\n\n10\nData Visualisation\nComplex Visualisations\n\n\n\n\n\n\n\n\n\nThis GitHub resource has been updated for the 2025-2026 academic year. The content for 2024-2025 has been archived and can be found here: [Link]"
  },
  {
    "objectID": "00-index.html#troubleshooting",
    "href": "00-index.html#troubleshooting",
    "title": "Geocomputation",
    "section": "",
    "text": "For specific assistance with this module, you can:\n\nRefer to the Moodle assessment tab for queries about module assessments.\nAsk a question at the end of lectures or during the computer practicals.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook into the Academic Support and Feedback hours."
  },
  {
    "objectID": "00-index.html#major-updates",
    "href": "00-index.html#major-updates",
    "title": "Geocomputation",
    "section": "",
    "text": "This year’s module material features the following major updates:\n\nAll code has been revised to ensure compatibility with tmap v4.\nPackage management using renv has been removed due to issues where sf installed within the package manager would not bind correctly to GDAL."
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "Geocomputation",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. Elements of this workbook are partially based on and modified from:\n\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  }
]