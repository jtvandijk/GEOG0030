{
  "hash": "f43a2c2326ca30e69e2231c1755ba3ff",
  "result": {
    "markdown": "# Raster Data Analysis\nSo far, we have exclusively focused on the use of vector and tabular data. However, depending on the nature of your research problem, you may also encounter *raster data*. This week's content introduces you to raster data, map algebra, and interpolation. \n\n## Lecture slides\nYou can download the slides of this week's lecture here: [[Link]]({{< var slides.week06 >}}).\n\n## Reading list\n#### Essential readings {.unnumbered}\n- Gimond, M. 2021. Intro to GIS and spatial analysis. **Chapter 14**: *Spatial Interpolation*. [[Link]](https://mgimond.github.io/Spatial/spatial-interpolation.html)\n- Heris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. *Scientific Data* 7: 207. [[Link]](https://doi.org/10.1038/s41597-020-0542-3)\n- Thomson, D., Leasure, D., Bird, T. *et al*. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. *Plos ONE* 17:7: e0271504. [[Link]](https://doi.org/10.1371/journal.pone.0271504)\n\n#### Suggested readings {.unnumbered}\n- Mellander, C., Lobo, J., Stolarick, K. *et al*. 2015. Night-time light data: A good proxy measure for economic activity? *PLoS ONE* 10(10): e0139779. [[Link]](https://doi.org/10.1371/journal.pone.0139779)\n- Park, G. and Franklin, R. 2023. The changing demography of hurricane at-risk areas in the United States (1970â€“2018). *Population, Space and Place* 29(6): e2683. [[Link]](https://doi.org/10.1002/psp.2685)\n\n## Population change in London\nFor the first part of this week's practical material we will be using raster datasets from [WorldPop](https://hub.worldpop.org/). These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These datasets can be used to explore, for example, changes in the demographic profiles or area deprivation at small spatial scales.\n\n::: {.callout-note}\nThe key difference between vector and raster models lies in their structure. Vectors are made up of points, lines, and polygons. In contrast, raster data consists of pixels (or grid cells), similar to an image. Each cell holds a single value representing a geographic phenomenon, such as population density at that location. Common raster data types include remote sensing imagery, such as satellite or LIDAR data.\n::: \n\n1. Navigate to the WorldPop Hub: [[Link]](https://hub.worldpop.org/)\n2. Go to **Population Count** -> **Unconstrained individual countries 2000-2020 (1km resolution)**.\n3. Type *United Kingdom* in the search bar.\n4. Download the [GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF) files for **2010** and **2020**: `gbr_ppp_2010_1km_Aggregated` and `gbr_ppp_2020_1km_Aggregated`.\n5. Save the files to your computer in your `data` folder.\n\n::: {.callout-note}\nA GeoTIFF is a type of raster file format that embeds geographic information, enabling the image to be georeferenced to specific real-world coordinates. It includes metadata like projection, coordinate system, and geographic extent, making it compatible with GIS software for spatial analysis.\n:::\n\nTo focus the analysis on London, we need to clip our dataset to the boundaries of the city. For this, we will use the London Borough boundaries, which can be downloaded from the link below. Be sure to save the files in the data folder within your `data` directory.\n\n| File                                        | Type   | Link |\n| :------                                     | :------| :------ |\n| London Borough Spatial Boundaries           | `GeoPackage` | [Download](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/spatial/London-Boroughs.gpkg) |\n\nOpen a new script within your `GEOG0030` project and save this as `w06-raster-data-analysis.r`. \n\nBegin by loading the necessary libraries:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load libraries\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\nlibrary(sf)\nlibrary(tmap)\n```\n:::\n\n\n::: {.callout-warning}\nYou may have to install some of these libraries if you have not used these before.\n:::\n\n### Map algebra\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. We can load the individual `GeoTiff` files that we downloaded into R and reproject them into British National Grid using the `terra` library.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load data\npop2010 <- rast(\"data/spatial/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 <- rast(\"data/spatial/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 <- pop2010 |>\n    project(\"EPSG:27700\")\npop2020 <- pop2020 |>\n    project(\"EPSG:27700\")\n```\n:::\n\n\nCarefully examine each dataframe to understand its structure and the information it contains:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# inspect 2010 data\nhead(pop2010)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  gbr_ppp_2010_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n```\n:::\n\n```{.r .cell-code}\n# inspect 2020 data\nhead(pop2020)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  gbr_ppp_2020_1km_Aggregated\n1                          NA\n2                          NA\n3                          NA\n4                          NA\n5                          NA\n6                          NA\n```\n:::\n:::\n\n::: {.callout-note}\nA raster file is always rectangular, with areas lacking data stored as `NA`. For our population data, this means any pixels outside the land borders of Great Britain will have by definition an `NA` value.\n:::\n\n::: {.callout-tip}\nYou can further inspect the results using the `View()` function. \n:::\n\nWe can also plot the raster files for visual inspection:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# plot 2010\nplot(pop2010)\n```\n\n::: {.cell-output-display}\n![WorldPop 2010 population estimates for the United Kingdom.](06-raster_files/figure-html/fig-06-load-raster-data-2010-1.png){#fig-06-load-raster-data-2010 width=672}\n:::\n:::\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# plot 2020\nplot(pop2020)\n```\n\n::: {.cell-output-display}\n![WorldPop 2020 population estimates for the United Kingdom.](06-raster_files/figure-html/fig-06-load-raster-data-2020-1.png){#fig-06-load-raster-data-2020 width=672}\n:::\n:::\n\n\nYou will notice that while the maps appear similar, the legend indicates a significant increase in values over the decade from 2010 to 2021, with the maximum rising from approximately 12,000 people per cell to over 14,000. \n\nNow that we have our raster data loaded, we will focus on reducing it to display only the extent of London. We will use the London borough `GeoPackage`\n\n::: {.callout-tip}\nThe `terra` package does not accept `sf` objects, so after loading the London borough boundaries, we need to convert the file into a `SpatRaster` or `SpatVector`.\n:::\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load data, to spatvector\nborough <- st_read(\"data/spatial/London-Boroughs.gpkg\") |>\n    vect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n```\n:::\n\n```{.r .cell-code}\n# crop to extent\npop2010_london <- crop(pop2010, borough)\npop2020_london <- crop(pop2020, borough)\n\n# mask to boundaries\npop2010_london <- mask(pop2010_london, borough)\npop2020_london <- mask(pop2020_london, borough)\n```\n:::\n\n\nWe should now have the raster cells that fall within the boundaries of London:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# inspect\nplot(pop2010_london)\n```\n\n::: {.cell-output-display}\n![WorldPop 2010 population estimates for London.](06-raster_files/figure-html/fig-06-load-raster-data-2010-lon-1.png){#fig-06-load-raster-data-2010-lon width=672}\n:::\n:::\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# inspect\nplot(pop2020_london)\n```\n\n::: {.cell-output-display}\n![WorldPop 2020 population estimates for London.](06-raster_files/figure-html/fig-06-load-raster-data-2020-lon-1.png){#fig-06-load-raster-data-2020-lon width=672}\n:::\n:::\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n  \n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# subtract\nlonpop_change <- pop2020_london - pop2010_london\n\n# inspect\nplot(lonpop_change)\n```\n\n::: {.cell-output-display}\n![Population change in London 2010-2020.](06-raster_files/figure-html/fig-07-subtract-london-1.png){#fig-07-subtract-london width=672}\n:::\n:::\n\n\n### Zonal statistics\nTo further analyse our population change raster, we can create a smoothed version of the `lonpop_change` raster using the `focal()` function. This function generates a raster that calculates the average (mean) value of the nearest neighbours for each cell.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# smooth\nlonpop_smooth <- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# inspect\nplot(lonpop_change)\n```\n\n::: {.cell-output-display}\n![Smoothed version of population change in London 2010-2020.](06-raster_files/figure-html/fig-06-focus-on-the-hood-1.png){#fig-06-focus-on-the-hood width=672}\n:::\n:::\n\n\nThe differences may not be immediately apparent, but if you subtract the smoothed raster from the original raster, you will clearly see that changes have occurred.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# substract\nlonpop_chang_smooth <- lonpop_change - lonpop_smooth\n\n# inspect\nplot(lonpop_chang_smooth)\n```\n\n::: {.cell-output-display}\n![Difference smoothed population change with original population change raster.](06-raster_files/figure-html/fig--6-focus-on-the-smooth-1.png){#fig--6-focus-on-the-smooth width=672}\n:::\n:::\n\n\nWe can also use zonal functions to better represent population change by aggregating the data to coarser resolutions. For example, resizing the raster's spatial resolution to contain larger grid cells simplifies the data, making broader trends more visible. However,it may also end up obfuscating more local patterns. \n\n::: {.callout-tip}\nWe can resize a raster using the `aggregate() function`, setting the `factor` parameter to the scale of resampling desired (e.g. doubling both the width and height of a cell). The `function` parameter determines how to aggregate the data.\n:::\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# aggregate\nlonpop_agg <- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# inspect\nplot(lonpop_agg)\n```\n\n::: {.cell-output-display}\n![Aggregated cell values.](06-raster_files/figure-html/fig-06-aggregate-the-raster-1.png){#fig-06-aggregate-the-raster width=672}\n:::\n:::\n\n\nWe can also aggregate raster cells to vector geographies. For example, we can aggregate the WorldPop gridded population estimates to the London borough boundaries:\n\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n# aggregate\nlondon_borough_pop <- extract(lonpop_change, borough, fun = sum)\n\n# bind to spatial boundaries\nborough <- borough |>\n  st_as_sf() |>\n  mutate(pop_change = london_borough_pop$gbr_ppp_2020_1km_Aggregated)\n\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"pop_change\",\n    palette = c(\"#f1eef6\", \"#bdc9e1\", \"#74a9cf\", \"#0570b0\"),\n    title = \"\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Absolute population change in London boroughs 2010-2020.](06-raster_files/figure-html/fig-06-aggregate-the-raster-to-vector-1.png){#fig-06-aggregate-the-raster-to-vector width=672}\n:::\n:::\n\n::: {.callout-tip}\nYou can further inspect the results using the `View()` function. \n:::\n\nWe now have a vector dataset, which allows us to perform many of the analyses we have explored in previous weeks. \n\n::: {.callout-tip}\nCalculating population change, particularly over decades as we have done, can be challenging due to changes in administrative boundaries. Using raster data offers a helpful workaround, provided the rasters are of consistent size and extent.\n:::\n\n## Air pollution in London\nIn the second part of this week's practical, we will explore various methods of spatial data interpolation, focusing on air pollution in London using data from [Londonair](https://www.londonair.org.uk/). We will specifically look at Nitrogen Dioxide (NO~2~) measurements.\n\n::: {.callout-note}\nLondonair is the website of the London Air Quality Network (LAQN), which provides air pollution data for London and southeast England through the [Environmental Research Group](https://www.imperial.ac.uk/school-public-health/environmental-research-group/) at Imperial College This data is publicly available and can be accessed directly using the `openair` R package, without needing to download files.\n:::\n\n::: {.callout-note}\nSpatial interpolation predicts a phenomenon at unmeasured locations. It is often used when we want to estimate a variable across space, particularly in areas with sparse or no data.\n::: \n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# get list of all measurement sites\nsite_meta <- importMeta(source = \"kcl\", all = TRUE, year = 2023:2023)\n\n# download all data pertaining to these sites\npollution <- importKCL(site = c(site_meta$code), year = 2023:2023, pollutant = \"no2\",\n    meta = TRUE)\n```\n:::\n\n\n::: {.callout-tip}\nNot all measurements sites collect data on NO~2~ so it is normal to get some `404 Not Found` warnings.\n:::\n\n::: {.callout-warning}\nThis code may take some time to run, as it will attempt to download data from all air measurement sites for an entire year, with many measurements taken hourly. If you experience too many errors or if it is taking too long, you can download a copy of the data here: [[Download]](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/attributes/London-NO2-2023.zip). Once downloaded, place the `zip` file in your `data` folder. The file is large, so you can leave it unzipped.\n:::\n\nLet us start by loading and inspecting the data:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load from zip if downloaded through the link\npollution <- read_csv(\"data/attributes/London-NO2-2023.zip\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nMultiple files in zip: reading 'London-Pollution-2023.csv'\nRows: 1615976 Columns: 8\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\n# inspect\nhead(pollution)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 8\n  date                  no2 site       code  source latitude longitude site_type\n  <dttm>              <dbl> <chr>      <chr> <chr>     <dbl>     <dbl> <chr>    \n1 2023-01-01 00:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n2 2023-01-01 01:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n3 2023-01-01 02:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n4 2023-01-01 03:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n5 2023-01-01 04:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n6 2023-01-01 05:00:00    NA City of Lâ€¦ CTA   kcl        51.5   -0.0921 Roadside \n```\n:::\n:::\n\n\nIn the first five rows, we can see data from the same site, with the date field showing an observation for every hour. Given there are 24 hours in a day, 365 days in a year, and data from hundreds of sites, it is no surprise that the dataset is so large. To make the dataset more manageable, let us summarise the values by site.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# mean site values\npollution_avg <- pollution |>\n    filter(!is.na(latitude) & !is.na(longitude) & !is.na(no2)) |>\n    group_by(code, latitude, longitude) |>\n    summarise(no2 = mean(no2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\n# inspect\nhead(pollution_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  <chr>    <dbl>     <dbl> <dbl>\n1 AH0       49.8    -7.56  15.7 \n2 BG1       51.6     0.178 16.4 \n3 BG2       51.5     0.133 18.4 \n4 BH0       50.8    -0.148 10.8 \n5 BK0       53.8    -3.01   4.80\n6 BL0       51.5    -0.126 24.0 \n```\n:::\n:::\n\n\nWe now have 177 measurement sites with their corresponding latitudes, longitudes, and average NO~2~ values. Let us have a look at the spatial distribution of these measurement sites.\n\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n# load boroughs for background\nborough <- st_read(\"data/spatial/London-Boroughs.gpkg\") |>\n  st_union()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `london_boroughs' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-Boroughs.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n```\n:::\n\n```{.r .cell-code}\n# create a point spatial dataframe\nmeasurement_sites <- pollution_avg |>\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |>\n  st_transform(27700)\n\n# clip measurement sites to london boundaries\nmeasurement_sites <- measurement_sites |>\n  st_intersection(borough)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n:::\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(borough) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify colours\n  tm_symbols(\n    col = \"#fc9272\",\n    size = 0.3,\n  ) +\n\n  # set legend\n  tm_add_legend(\n    type = \"symbol\",\n    labels = \"Measurement site\",\n    col = \"#fc9272\",\n    size = 0.5\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![KCL NO~2~ measurement sites in London.](06-raster_files/figure-html/fig-06-air-quality-measurement-sites-1.png){#fig-06-air-quality-measurement-sites width=672}\n:::\n:::\n\n\nWe can also use proportional symbols to visualise the values, helping us observe how measurements vary across London.\n\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(borough) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(measurement_sites) +\n\n  # specify column\n  tm_bubbles(\n    size = \"no2\",\n    title.size = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.text.size = 1,\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Proportional symbol map of average KCL NO~2~ measurement in London.](06-raster_files/figure-html/fig-06-air-quality-measurement-sites-proportional-symbol-1.png){#fig-06-air-quality-measurement-sites-proportional-symbol width=672}\n:::\n:::\n\n\n@fig-06-air-quality-measurement-sites-proportional-symbol shows heterogeneity in average NO~2~ measurements across London, both in terms of coverage and NO~2~ levels. To make reasonable assumptions about NO~2~ levels in areas without measurements, we can interpolate the missing values.\n\n### Voronoi tessellation\nA straightforward method for interpolating values across space is to create a Voronoi tessellation polygons. These polygons define the boundaries of areas closest to each unique point, meaning that each point in the dataset has a corresponding polygon.\n\n::: {.callout-note}\nIn addition to Voronoi tessellation, you may encounter the term Thiessen polygons. These terms are often used interchangeably to describe the geometry created from point data.\n:::\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# function\nst_voronoi_points <- function(points) {\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to voronoi\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# voronoi tessellation\nmeasurement_sites_voronoi <- st_voronoi_points(measurement_sites)\n\n# replace point geometry with polygon geometry\nmeasurement_sites_tesselation <- measurement_sites |>\n    st_set_geometry(measurement_sites_voronoi) |>\n    st_intersection(borough)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n:::\n\n```{.r .cell-code}\n# inspect\nmeasurement_sites_tesselation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple feature collection with 98 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 98 Ã— 3\n   code    no2                                                          geometry\n * <chr> <dbl>                                                    <GEOMETRY [m]>\n 1 BG1    16.4 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,â€¦\n 2 BG2    18.4 POLYGON ((545264.9 181815.5, 545402.2 184801.2, 547703.4 186697.â€¦\n 3 BL0    24.0 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5â€¦\n 4 BQ7    13.9 POLYGON ((546306.3 181181, 549837.9 181565.4, 548349.8 175998.9,â€¦\n 5 BT4    38.9 POLYGON ((519912.6 183741.2, 516082 187365.6, 520958.9 189405.3,â€¦\n 6 BT5    24.9 POLYGON ((523877.3 190896.9, 524273.2 190424, 524781.6 189259.6,â€¦\n 7 BT6    24.6 POLYGON ((521236.1 184358.2, 522982.8 184472, 522330 181976.4, 5â€¦\n 8 BT8    23.3 POLYGON ((522982.8 184472, 524217.5 185711.1, 525595.2 182818.7,â€¦\n 9 BX1    15.7 MULTIPOLYGON (((550125.4 169173.1, 550132.2 169161.6, 550144.3 1â€¦\n10 BX2    15.4 POLYGON ((548349.8 175998.9, 549837.9 181565.4, 550403.4 181822.â€¦\n# â„¹ 88 more rows\n```\n:::\n:::\n\n\n::: {.callout-warning}\nDo not worry about fully understanding the code behind the function; just know that it takes a point spatial data frame as input and produces a tessellated spatial data frame as output.\n:::\n\n::: {.callout-tip}\nYou can further inspect the results using the `View()` function. \n:::\n\nWe can now visualise the results of the interpolation:\n\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(measurement_sites_tesselation) +\n\n  # specify column, classes\n  tm_polygons(\n    col = \"no2\",\n    palette = c(\"#ffffcc\", \"#c2e699\", \"#78c679\", \"#0570b0\"),\n    title = \"Average reading\",\n  ) +\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Interpolation of average NO~2~ measurements in London using a Voronoi tessellation.](06-raster_files/figure-html/fig-06-air-quality-london-thiessen-1.png){#fig-06-air-quality-london-thiessen width=672}\n:::\n:::\n\n\n### Inverse Distance Weighting\nA more sophisticated method for interpolating point data is Inverse Distance Weighting (IDW). IDW converts numerical point data into a continuous surface, allowing for visualisation of how the data is distributed across space. This technique estimates values at each location by calculating a weighted average from nearby points, with the weights inversely related to their distances.\n\n::: {.callout-note}\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface. \n:::\n\nWe will start by generating an empty grid to store the predicted values before running the IDW.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# create regular output grid\noutput_grid <- borough |>\n    st_make_grid(cellsize = c(1000, 1000))\n\n# execute\nmeasurement_sites_idw <- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    idp = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[inverse distance weighted interpolation]\n```\n:::\n\n```{.r .cell-code}\n# clip\nmeasurement_sites_idw <- measurement_sites_idw |>\n    st_intersection(borough)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n:::\n:::\n\n\n::: {.callout-warning}\nThe IDW interpolation may take some time to run because it involves calculating the weighted average of nearby points for each location on the grid. In this case, `idp = 2` specifies a quadratic decay, meaning the influence of a point decreases with the square of the distance.\n:::\n\nAgain, we can map the results for visual inspection.\n\n::: {.callout-tip}\nThe values of the IDW output are stored in the raster grid as `var1.pred`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(measurement_sites_idw) +\n\n  # specify column, classes\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"cont\",\n    palette = \"Oranges\",\n    title = \"Average reading\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Interpolation of average NO~2~ measurements in London using Inverse Distance Weighting.](06-raster_files/figure-html/fig-06-air-quality-idw-1.png){#fig-06-air-quality-idw width=672}\n:::\n:::\n\n\n::: {.callout-note}\nWe have set the output cell size to 1000x1000 metres. While a smaller cell size can yield a smoother IDW output, it may introduce uncertainty due to the limited number of data points available for interpolation. Moreover, reducing the cell size will exponentially increase processing time.\n:::\n\n## Assignment \nHaving run through all the steps during the tutorial, we can conduct some more granular analysis of the NO~2~ measurements. For example, instead of examining the annual average measurements, we could compare data across different months. Please try the following tasks:\n\n1. Create monthly averages for the pollution data.\n2. For both *June* and *December*, generate a dataframe containing the London monitoring sites along with their average NOâ‚‚ readings for these months.\n3. Perform Inverse Distance Weighting (IDW) interpolation for the data from both months.\n4. Combine the results to assess the differences between these months.\n\n## Before you leave \nThis week, we have explored raster datasets and how to manage and process them using the `terra` library. While you will typically encounter vector data, particularly in relation to government statistics and administrative boundaries, there are also many use cases where raster data may be encountered. With that being said: [that is it for this week](https://www.youtube.com/watch?v=8iwBM_YB1sE)! \n",
    "supporting": [
      "06-raster_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}