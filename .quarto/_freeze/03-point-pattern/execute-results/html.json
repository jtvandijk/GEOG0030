{
  "hash": "b93075b9a092cb5319d69305dc5b9abe",
  "result": {
    "markdown": "# Point Pattern Analysis\nThis week, we will be focusing on point pattern analysis (PPA), which aims to detect clusters or patterns within a set of points. Through this analysis, we can measure density, dispersion, and homogeneity in point structures. Various methods exist for calculating and identifying these clusters, and today we will explore several of these techniques using our [bike theft dataset from last week](02-operations.html#crime-data).\n\n## Lecture slides\nThe slides for this week's lecture can be downloaded here: [[Link]]({{< var slides.week03 >}})\n\n## Reading list\n#### Essential readings {.unnumbered}\n- Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. *Journal of Urban Economics* 125: 103217. [[Link]](https://doi.org/10.1016/j.jue.2019.103217)\n- Cheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. *International Journal of Geographical Information Science* 26(2), pp.309-325. [[Link]](https://doi.org/10.1080/13658816.2011.591291)\n- Longley, P. *et al.* 2015. *Geographic Information Science & systems*, **Chapter 12**: *Geovisualization*. [[Link]](https://ucl.rl.talis.com/link?url=https%3A%2F%2Fapp.knovel.com%2Fhotlink%2Ftoc%2Fid%3AkpGISSE001%2Fgeographic-information-science%3Fkpromoter%3Dmarc&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a)\n\n#### Suggested readings {.unnumbered}\n- Van Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. *Journal of Maps* 16, pp.58-76. [[Link]](https://doi.org/10.1080/17445647.2020.1746418)\n- Yin, P. 2020. *Kernels and density estimation*. The Geographic Information Science & Technology Body of Knowledge. [[Link]](https://doi.org/10.22224/gistbok/2020.1.12)\n\n## Bike theft in London II\nThis week, we will revisit bicycle theft in London, focusing specifically on identifying patterns and clusters of theft incidents. To do this, we will use the [bicycle theft dataset](02-operations.html#crime-data) that we prepared last week, along with the 2021 MSOA boundaries for London. If you no longer have a copy of these files on your computer, you can download them using the links provided below.\n\n| File                                        | Type   | Link |\n| :------                                     | :------| :------ |\n| London Bicycle Theft 2023                   | `GeoPackage` | [Download](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/spatial/London-BicycleTheft-2023.gpkg)|\n| London MSOA 2021 Spatial Boundaries         | `GeoPackage` | [Download](https://github.com/jtvandijk/GEOG0030/raw/refs/heads/main/data/spatial/London-MSOA-2021.gpkg) |\n\nTo get started, let us create our first script. **File** -> **New File** -> **R Script**. Save your script as `w03-bike-theft.r`. \n\nWe will start by loading the libraries that we will need:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n```\n:::\n\n\n::: {.callout-warning}\nYou may have to install some of these libraries if you have not used these before.\n:::\n\nAs always, we will start by loading loading our files into memory:\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# load msoa dataset\nmsoa21 <- st_read(\"data/spatial/London-MSOA-2021.gpkg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `London-MSOA-2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-MSOA-2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n```\n:::\n\n```{.r .cell-code}\n# load bicycle theft dataset\ntheft_bike <- st_read(\"data/spatial/London-BicycleTheft-2023.gpkg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `London-BicycleTheft-2023' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/spatial/London-BicycleTheft-2023.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16019 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 384035.6 ymin: 101574.6 xmax: 612801.1 ymax: 398089\nProjected CRS: OSGB36 / British National Grid\n```\n:::\n\n```{.r .cell-code}\n# inspect\nhead(msoa21)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180512.6 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   msoa21cd                 msoa21nm msoa21nmw  bng_e  bng_n      lat      long\n1 E02000001       City of London 001           532384 181355 51.51562 -0.093490\n2 E02000002 Barking and Dagenham 001           548267 189685 51.58652  0.138756\n3 E02000003 Barking and Dagenham 002           548259 188520 51.57606  0.138149\n4 E02000004 Barking and Dagenham 003           551004 186412 51.55639  0.176828\n5 E02000005 Barking and Dagenham 004           548733 186824 51.56069  0.144267\n6 E02000007 Barking and Dagenham 006           549698 186609 51.55851  0.158087\n                                globalid                           geom\n1 {71249043-B176-4306-BA6C-D1A993B1B741} MULTIPOLYGON (((532135.1 18...\n2 {997A80A8-0EBE-461C-91EB-3E4122571A6E} MULTIPOLYGON (((548881.6 19...\n3 {62DED9D9-F53A-454D-AF35-04404D9DBE9B} MULTIPOLYGON (((549102.4 18...\n4 {511181CD-E71F-4C63-81EE-E8E76744A627} MULTIPOLYGON (((551550.1 18...\n5 {B0C823EB-69E0-4AE7-9E1C-37715CF3FE87} MULTIPOLYGON (((549099.6 18...\n6 {A33C6ADD-D70A-4737-ADE5-3460D7016CA1} MULTIPOLYGON (((549819.9 18...\n```\n:::\n\n```{.r .cell-code}\n# inspect\nhead(theft_bike)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 531274.9 ymin: 180967.9 xmax: 533333.7 ymax: 181781.7\nProjected CRS: OSGB36 / British National Grid\n                                                          crime_id   month\n1 62b0f525fc471c062463ec87469c71e133cdd1c39a09e2bc5ec50c0cbbdd650a 2023-01\n2 9a078d630cf67c37c9e47b5904149d553697931d920eec993f358a637fbf6186 2023-01\n3 f175a32ef7f90c67a5cb83c268ad793e4ce7e0ae19e52b57d213805e65651bdd 2023-01\n4 137ec1201fd64b57898d3558fe3b8000182442e957efc28246797ea61065c86b 2023-01\n5 4c3b467755a98afa3d3c82b526307750ddad9ec13598aaa68130b76863e112cb 2023-01\n6 13b5eb5ca0aef09a222221fbab8da799d55b5a65716f1bbb7cc96e36aebdc816 2023-01\n            reported_by          falls_within                   location\n1 City of London Police City of London Police                 On or near\n2 City of London Police City of London Police                 On or near\n3 City of London Police City of London Police  On or near Lombard Street\n4 City of London Police City of London Police    On or near Mitre Street\n5 City of London Police City of London Police   On or near Temple Avenue\n6 City of London Police City of London Police On or near Montague Street\n  lsoa_code           lsoa_name    crime_type\n1 E01000002 City of London 001B Bicycle theft\n2 E01032739 City of London 001F Bicycle theft\n3 E01032739 City of London 001F Bicycle theft\n4 E01032739 City of London 001F Bicycle theft\n5 E01032740 City of London 001G Bicycle theft\n6 E01032740 City of London 001G Bicycle theft\n                          last_outcome_category context\n1 Investigation complete; no suspect identified      NA\n2 Investigation complete; no suspect identified      NA\n3 Investigation complete; no suspect identified      NA\n4 Investigation complete; no suspect identified      NA\n5 Investigation complete; no suspect identified      NA\n6 Investigation complete; no suspect identified      NA\n                       geom\n1 POINT (532390.8 181781.7)\n2 POINT (532157.8 181196.8)\n3 POINT (532720.7 181087.8)\n4 POINT (533333.7 181219.8)\n5 POINT (531274.9 180967.9)\n6 POINT (531956.8 181624.8)\n```\n:::\n:::\n\n\n::: {.callout-note}\nYou can further inspect both objects using the `View()` function. \n:::\n\n### Point aggregation\nOne key advantage of point data is that it is scale-free, allowing aggregation to any geographic level for analysis. Before diving into PPA, we will aggregate the bicycle thefts to the MSOA level to map their distribution by using a point-in-polygon approach.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# point in polygon\nmsoa21 <- msoa21 |>\n    mutate(theft_bike_n = lengths(st_intersects(msoa21, theft_bike, sparse = TRUE)))\n```\n:::\n\n\n::: {.callout-tip}\nTo create a point-in-polygon count within `sf`, we use the `st_intersects()` function and keep its default `sparse = TRUE` output, which produces a list of intersecting points by index for each polygon (e.g. MSOA). We then apply the `lengths()` function to count the number of points intersecting each polygon, giving us the total number of bike thefts per MSOA.\n:::\n\nWe can now calculate the area of each MSOA and, combined with the total number of bicycle thefts, determine the number of thefts per square kilometre. This involves calculating the size of each MSOA in square kilometres and then dividing the total number of thefts by this area to get a theft density measure.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# msoa area size\nmsoa21 <- msoa21 |>\n    mutate(km_sq = as.numeric(st_area(msoa21))/1e+06)\n\n# theft density\nmsoa21 <- msoa21 |>\n    mutate(thef_km_sq = theft_bike_n/km_sq)\n```\n:::\n\n\nLet's put this onto a map:\n\n\n::: {.cell filename='R code'}\n\n```{.r .cell-code}\n# shape, polygons\ntm_shape(msoa21) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"thef_km_sq\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"),\n    title = \"Thefts / Square kilometre\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n  )\n```\n\n::: {.cell-output-display}\n![Number of reported bicycle thefts by square kilometre.](03-point-pattern_files/figure-html/fig-03-choro-3-1.png){#fig-03-choro-3 width=672}\n:::\n:::\n\n\n### Point pattern analysis\n@fig-03-choro-3 shows that the number of bicycle thefts is clearly concentrated in parts of Central London. While this map may provide helpful insights, its representation depends on the classification and aggregation of the underlying data. Alternatively, we can directly analyse the point events themselves. For this, we will use the `spatstat` library, the primary library for point pattern analysis in R. To use `spatstat`, we need to convert our data into a `ppp` object. \n\n::: {.callout-note}\nThe `ppp` format is specific to `spatstat` but is also used in some other spatial analysis libraries. A `ppp` object represents a two-dimensional point dataset within a defined area, called the *window of observation* (`owin` in *spatstat*). We can either create a `ppp` object directly from a list of coordinates (with a specified window of observation) or convert it from another data type.\n:::\n\nWe can turn our `theft_bike` dataframe into a `ppp` object as follows:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# london outline\noutline <- msoa21 |>\n    st_union()\n\n# clip\ntheft_bike <- theft_bike |>\n    st_intersection(outline)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n:::\n\n```{.r .cell-code}\n# sf to ppp\nwindow = as.owin(msoa21)\ntheft_bike_ppp <- ppp(st_coordinates(theft_bike)[, 1], st_coordinates(theft_bike)[,\n    2], window = window)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: data contain duplicated points\n```\n:::\n\n```{.r .cell-code}\n# inspect\npar(mar = c(1, 1, 1, 1))\nplot(theft_bike_ppp, main = \"\")\n```\n\n::: {.cell-output-display}\n![Bike theft in London represented as `ppp` object.](03-point-pattern_files/figure-html/fig-03-bike-theft-to-ppp-1.png){#fig-03-bike-theft-to-ppp width=672}\n:::\n:::\n\n\nSome statistical procedures require point events to be unique. In our bicycle theft data, duplicates are likely due to the police snapping points to protect anonymity and privacy. This can pose an issue for spatial point pattern analysis, where each theft and its location must be distinct. We can check whether we have any duplicated points as follows:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# check for duplicates\nanyDuplicated(theft_bike_ppp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# count number of duplicated points\nsum(multiplicity(theft_bike_ppp) > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10003\n```\n:::\n:::\n\n\nTo address this, we have three options:\n\n1. Remove duplicates if the number of duplicated points is small or the exact location is less important than the overall distribution.\n2. Assign weights to points, where each has an attribute indicating the number of events at that location rather than being recorded as separate event. \n3. Add *jitter* by slightly offsetting the points randomly, which can be useful if precise location is not crucial for the analysis.\n\nEach approach has its own trade-offs, depending on the analysis. In our case, we will use the *jitter* approach to retain all bike theft events. Since the locations are already approximated, adding a small offset (~5 metre) will not impact the analysis.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# add ajitter\ntheft_bike_jitter <- rjitter(theft_bike_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for duplicates\nanyDuplicated(theft_bike_jitter)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\n# count number of duplicated points\nsum(multiplicity(theft_bike_jitter) > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nThis seemed to have worked, so we can move forward.\n\n#### Kernel density estimation\nInstead of visualising the distribution of bike thefts at a specific geographical level, we can use Kernel Density Estimation (KDE) to display the distribution of these incidents. KDE is a statistical method that creates a smooth, continuous distribution to represent the density of the underlying pattern between data points.\n\n::: {.callout-note}\nKernel Density Estimation (KDE) generates a raster surface that shows the estimated density of event points across space. Each cell represents the local density, highlighting areas of high or low concentration. KDE uses overlapping moving windows (defined by a kernel) and a bandwidth parameter, which controls the size of the window, influencing the smoothness of the resulting density surface. The kernel function can assign equal or weighted values to points, producing a grid of density values based on these local calculations.\n:::\n\nLet's go ahead and create a simple KDE of bike theft with our bandwidth set to 500 metres:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 500), main = \"\")\n```\n\n::: {.cell-output-display}\n![Kernel density estimation - bandwidth 500m.](03-point-pattern_files/figure-html/fig-03-bike-theft-kde500-1.png){#fig-03-bike-theft-kde500 width=672}\n:::\n:::\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around Central London. We can go ahead and increase the bandwidth to to see how that affects the density estimate:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# kernel density estimation\npar(mar = c(1, 1, 1, 1))\nplot(density.ppp(theft_bike_jitter, sigma = 1000), main = \"\")\n```\n\n::: {.cell-output-display}\n![Kernel density estimation - bandwidth 1000m.](03-point-pattern_files/figure-html/fig-03-bike-theft-kde1000-1.png){#fig-03-bike-theft-kde1000 width=672}\n:::\n:::\n\n\nBy increasing the bandwidth, our clusters appear larger and brighter than with the 500-metre bandwidth. A larger bandwidth considers more points, resulting in a smoother surface. However, this can lead to oversmoothing, where clusters become less defined, potentially overestimating areas of high bike theft. Smaller bandwidths offer more precision and sharper clusters but risk undersmoothing, which can cause irregularities.\n\n::: {.callout-tip} \nWhile automated methods (e.g. maximum-likelihood estimation) can assist in selecting an optimal bandwidth, the choice is subjective and depends on the specific characteristics of your dataset. \n:::\n\n::: {.callout-note}\nAlthough bandwidth has a greater impact on density estimation than the kernel type, the choice of kernel can still influence the results by altering how points are weighted within the window. We will explore kernel types a little further when we [discuss spatial models in a few weeks time](05-models.html).\n:::\n\nOnce we are satisfied with our KDE visualisation, we can create a proper map by converting the KDE output into raster format.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# to raster\ntheft_bike_raster <- density.ppp(theft_bike_jitter, sigma = 1000) |>\n    rast()\n```\n:::\n\n\nWe now have a standalone raster that we can use with any function in the `tmap` library. However, one issue is that the resulting raster lacks a Coordinate Reference System (CRS), so we need to manually assign this information to the raster object:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# set CRS\ncrs(theft_bike_raster) <- \"epsg:27700\"\n```\n:::\n\n\nNow we can map the KDE values.\n\n\n::: {.cell filename='R code'}\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(theft_bike_raster) +\n\n  # specify column, colours\n  tm_raster(\n    col = \"lyr.1\",\n    palette = \"Blues\",\n    title = \"Density\"\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Kernel Density Estimate of bicycle thefts in London.](03-point-pattern_files/figure-html/fig-kde-raster-map-1.png){#fig-kde-raster-map width=672}\n:::\n:::\n\n\n::: {.callout-tip}\nThe values of the KDE output are stored in the raster grid as `lyr.1`.\n:::\n\n#### DBSCAN\nKernel Density Estimation is a useful exploratory technique for identifying spatial clusters in point data, but it does not provide precise boundaries for these clusters. To more accurately delineate clusters, we can use an algorithm called DBSCAN [(Density-Based Spatial Clustering of Applications with Noise)](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf), which takes both distance and density into account. DBSCAN is effective at discovering distinct clusters by grouping together points that are close to one another while marking points that don't belong to any cluster as noise.\n\nDBSCAN requires two parameters:\n\n| Parameter  | Description |\n| :-         | :------ |\n| `epsilon`  | The maximum distance for points to be considered in the same cluster. |\n| `minPts`   | The minimum number of points for a cluster. |\n\nThe algorithm groups nearby points based on these parameters and marks low-density points as outliers. DBSCAN is useful for uncovering patterns that are difficult to detect visually, but it works best when clusters have consistent densities.\n\nLet us try this with an `epsilon` of  **200** metres and `minPts` of **20** bicycle thefts:\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# dbscan\nbike_theft_dbscan <- theft_bike |>\n    st_coordinates() |>\n    dbscan(eps = 200, minPts = 20)\n```\n:::\n\n\n::: {.callout-note}\nThe `dbscan()` function accepts a data matrix or dataframe of points, not a spatial dataframe. That is why, in the code above, we use the `st_coordinates()` function to extract the projected coordinates from the spatial dataframe.\n:::\n\nThe DBSCAN output includes three objects, one of which is a vector detailing the cluster each bike theft observation has been assigned to. To work with this output effectively, we need to add the cluster labels back to the original point dataset. Since DBSCAN does not alter the order of points, we can simply add the cluster output to the `theft_bike` spatial dataframe.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# add cluster numbers\ntheft_bike <- theft_bike |>\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n```\n:::\n\n\nNow that each bike theft point in London is associated with a specific cluster, where appropriate, we can generate a polygon representing these clusters. To do this, we will use the `st_convex_hull()` function from the `sf` package, which creates a polygon that covers the [minimum bounding area](https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/minimum-bounding-geometry.htm) of a collection of points. We will apply this function to each cluster using a `for` loop, which allows us to repeat the process for each group of points and create a polygon representing the geometry of each cluster.\n\n\n::: {.cell .styled-output filename='R code'}\n\n```{.r .cell-code}\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list <- vector(mode = \"list\", length = max(theft_bike$dbcluster))\n\n# begin loop\nfor (cluster_index in seq(1, max(theft_bike$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    theft_bike_subset <- theft_bike |>\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon <- theft_bike_subset |>\n        st_union() |>\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[cluster_index] <- (cluster_polygon)\n\n}\n\n# combine the list\ntheft_bike_clusters <- st_sfc(geometry_list, crs = 27700)\n```\n:::\n\n\n::: {.callout-tip}\nWhile loops in R should generally be avoided for large datasets due to inefficiency, they remain a useful tool for automating repetitive tasks and reducing the risk of errors. For smaller datasets or tasks that cannot easily be vectorised, loops can still be effective and simplify the code.\n:::\n\nWe now have a spatial dataframe that contains the bike theft clusters in London, as defined by the DBSCAN clustering algorithm. Let's quickly map these clusters:\n\n\n::: {.cell filename='R code'}\n\n```{.r .cell-code}\n# shape, polygon\ntm_shape(outline) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#f0f0f0\",\n  ) +\n\n  # shape, points\n  tm_shape(theft_bike) +\n\n  # specify colours\n  tm_dots(\n    col = \"#636363\",\n    size = 0.01,\n  ) +\n\n  # shape, polygon\n  tm_shape(theft_bike_clusters) +\n\n  # specify colours\n  tm_polygons(\n    col = \"#fdc086\",\n  ) +\n\n  # set layout\n  tm_layout(\n    frame = FALSE,\n  )\n```\n\n::: {.cell-output-display}\n![DBSCAN-identified clusters of reported bicycle theft in London.](03-point-pattern_files/figure-html/fig-03-map-clusters-1.png){#fig-03-map-clusters width=672}\n:::\n:::\n\n\n## Assignment\nNow that we know how to work with point locatoin data, we can again apply a similar analysis to road crashes in London in 2022 [that we used last week](02-operations.html#assignment). This time we will use this dataset to assess whether road crashes cluster in specific areas. Try the following:\n\n1. Create a Kernel Density Estimation (KDE) of *all* road crashes that occurred in London in 2022.\n2. Using DBSCAN output, create a cluster map of *serious* and *fatal* road crashes in London\n\nIf you no longer have a copy of the 2022 London STATS19 Road Collision dataset, you can download it using the link provided below.\n\n| File                                        | Type   | Link |\n| :------                                     | :------| :------ |\n| London STATS19 Road Collisions 2022         | `csv` | [Download](https://github.com/jtvandijk/GEOG0030/tree/master/data/attributes/London-Collisions-2022.csv) |\n\n## Before you leave\nWith access to point event data, geographers aim to identify underlying patterns. This week, we explored several techniques that help us analyse and interpret such data. That is [us done for this week](https://www.youtube.com/watch?v=-zxtbwGogyY). Reading list anyone?",
    "supporting": [
      "03-point-pattern_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}